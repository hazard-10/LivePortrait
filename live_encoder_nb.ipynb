{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile\n",
    "and initialize args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile complete\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import contextlib\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "import tyro\n",
    "import subprocess\n",
    "from rich.progress import track\n",
    "\n",
    "from src.config.argument_config import ArgumentConfig\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.config.crop_config import CropConfig\n",
    "\n",
    "def partial_fields(target_class, kwargs):\n",
    "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
    "\n",
    "args = ArgumentConfig()\n",
    "inference_cfg = partial_fields(InferenceConfig, args.__dict__)\n",
    "crop_cfg = partial_fields(CropConfig, args.__dict__)\n",
    "# print(\"inference_cfg: \", inference_cfg)\n",
    "# print(\"crop_cfg: \", crop_cfg)\n",
    "device = 'cuda'\n",
    "print(\"Compile complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize util functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize motion extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helper import load_model, concat_feat\n",
    "from src.utils.camera import headpose_pred_to_degree, get_rotation_matrix\n",
    "from src.utils.retargeting_utils import calc_eye_close_ratio, calc_lip_close_ratio\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.utils.cropper import Cropper\n",
    "from src.utils.camera import get_rotation_matrix\n",
    "from src.utils.video import images2video, concat_frames, get_fps, add_audio_to_video, has_audio_stream\n",
    "from src.utils.crop import _transform_img, prepare_paste_back, paste_back\n",
    "from src.utils.io import load_image_rgb, load_video, resize_to_limit, dump, load\n",
    "from src.utils.helper import mkdir, basename, dct2device, is_video, is_template, remove_suffix, is_image\n",
    "from src.utils.filter import smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[22:55:12] </span>LandmarkRunner warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.</span>124s                                                 <a href=\"file:///mnt/e/wsl_projects/LivePortrait/src/utils/landmark_runner.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">landmark_runner.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/e/wsl_projects/LivePortrait/src/utils/landmark_runner.py#95\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">95</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[22:55:12]\u001b[0m\u001b[2;36m \u001b[0mLandmarkRunner warmup time: \u001b[1;36m6.\u001b[0m124s                                                 \u001b]8;id=986579;file:///mnt/e/wsl_projects/LivePortrait/src/utils/landmark_runner.py\u001b\\\u001b[2mlandmark_runner.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=131630;file:///mnt/e/wsl_projects/LivePortrait/src/utils/landmark_runner.py#95\u001b\\\u001b[2m95\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[22:55:14] </span>FaceAnalysisDIY warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.</span>079s                                              <a href=\"file:///mnt/e/wsl_projects/LivePortrait/src/utils/face_analysis_diy.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">face_analysis_diy.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/e/wsl_projects/LivePortrait/src/utils/face_analysis_diy.py#79\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">79</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[22:55:14]\u001b[0m\u001b[2;36m \u001b[0mFaceAnalysisDIY warmup time: \u001b[1;36m2.\u001b[0m079s                                              \u001b]8;id=257677;file:///mnt/e/wsl_projects/LivePortrait/src/utils/face_analysis_diy.py\u001b\\\u001b[2mface_analysis_diy.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=54778;file:///mnt/e/wsl_projects/LivePortrait/src/utils/face_analysis_diy.py#79\u001b\\\u001b[2m79\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_config = yaml.load(open(inference_cfg.models_config, 'r'), Loader=yaml.SafeLoader)\n",
    "# init F\n",
    "appearance_feature_extractor = load_model(inference_cfg.checkpoint_F, model_config, device, 'appearance_feature_extractor')\n",
    "# init M\n",
    "motion_extractor = load_model(inference_cfg.checkpoint_M, model_config, device, 'motion_extractor')\n",
    "# init W\n",
    "warping_module = load_model(inference_cfg.checkpoint_W, model_config, device, 'warping_module')\n",
    "# init G\n",
    "spade_generator = load_model(inference_cfg.checkpoint_G, model_config, device, 'spade_generator')\n",
    "# init S and R\n",
    "if inference_cfg.checkpoint_S is not None and os.path.exists(inference_cfg.checkpoint_S):\n",
    "    stitching_retargeting_module = load_model(inference_cfg.checkpoint_S, model_config, device, 'stitching_retargeting_module')\n",
    "else:\n",
    "    stitching_retargeting_module = None\n",
    "\n",
    "cropper = Cropper(crop_cfg=crop_cfg, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_distance_ratio(lmk: np.ndarray, idx1: int, idx2: int, idx3: int, idx4: int, eps: float = 1e-6) -> np.ndarray:\n",
    "    return (np.linalg.norm(lmk[:, idx1] - lmk[:, idx2], axis=1, keepdims=True) /\n",
    "            (np.linalg.norm(lmk[:, idx3] - lmk[:, idx4], axis=1, keepdims=True) + eps))\n",
    "\n",
    "\n",
    "def calc_eye_close_ratio(lmk: np.ndarray, target_eye_ratio: np.ndarray = None) -> np.ndarray:\n",
    "    lefteye_close_ratio = calculate_distance_ratio(lmk, 6, 18, 0, 12)\n",
    "    righteye_close_ratio = calculate_distance_ratio(lmk, 30, 42, 24, 36)\n",
    "    if target_eye_ratio is not None:\n",
    "        return np.concatenate([lefteye_close_ratio, righteye_close_ratio, target_eye_ratio], axis=1)\n",
    "    else:\n",
    "        return np.concatenate([lefteye_close_ratio, righteye_close_ratio], axis=1)\n",
    "\n",
    "\n",
    "def calc_lip_close_ratio(lmk: np.ndarray) -> np.ndarray:\n",
    "    return calculate_distance_ratio(lmk, 90, 102, 48, 66)\n",
    "\n",
    "def calc_ratio(lmk_lst):\n",
    "    input_eye_ratio_lst = []\n",
    "    input_lip_ratio_lst = []\n",
    "    for lmk in lmk_lst:\n",
    "        # for eyes retargeting\n",
    "        input_eye_ratio_lst.append(calc_eye_close_ratio(lmk[None]))\n",
    "        # for lip retargeting\n",
    "        input_lip_ratio_lst.append(calc_lip_close_ratio(lmk[None]))\n",
    "    return input_eye_ratio_lst, input_lip_ratio_lst\n",
    "\n",
    "def prepare_videos(imgs, device) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    imgs: NxBxHxWx3, uint8\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, list):\n",
    "        _imgs = np.array(imgs)[..., np.newaxis]  # TxHxWx3x1\n",
    "    elif isinstance(imgs, np.ndarray):\n",
    "        _imgs = imgs\n",
    "    else:\n",
    "        raise ValueError(f'imgs type error: {type(imgs)}')\n",
    "\n",
    "    y = _imgs.astype(np.float32) / 255.\n",
    "    y = np.clip(y, 0, 1)  # clip to 0~1\n",
    "    y = torch.from_numpy(y).permute(0, 4, 3, 1, 2)  # TxHxWx3x1 -> Tx1x3xHxW\n",
    "    y = y.to(device)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Video loading ( SLOW load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_path = '/mnt/c/Users/mjh/Downloads/live_in/a_1.mp4'\n",
    "is_video(vid_path)\n",
    "driving_rgb_lst = load_video(vid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# driving_rgb_lst = load_video(vid_path)\n",
    "len(driving_rgb_lst), driving_rgb_lst[0].shape, type(driving_rgb_lst[0])\n",
    "driving_lmk_crop_lst = cropper.calc_lmks_from_cropped_video(driving_rgb_lst)\n",
    "driving_rgb_crop_256x256_lst = [cv2.resize(_, (256, 256)) for _ in driving_rgb_lst]\n",
    "\n",
    "c_d_eyes_lst, c_d_lip_lst = calc_ratio(driving_lmk_crop_lst)\n",
    "I_d_lst = prepare_videos(driving_rgb_crop_256x256_lst, device)\n",
    "# I_d_lst = I_d_lst.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder of videos load ( fast )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import cv2\n",
    "import threading\n",
    "import queue\n",
    "import torchvision.transforms as transforms\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_video_frames(video_path, frame_queue):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "#     frames = []\n",
    "#     for _ in range(frame_count):\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "#     cap.release()\n",
    "#     frame_queue.put((video_path, frames))\n",
    "\n",
    "# def read_multiple_videos(video_paths, num_threads=4):\n",
    "#     frame_queue = queue.Queue()\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "#         futures = [executor.submit(read_video_frames, path, frame_queue) for path in video_paths]\n",
    "#         for future in as_completed(futures):\n",
    "#             future.result()\n",
    "\n",
    "#     frame_queue.put(None)  # Signal that all videos have been read\n",
    "#     return frame_queue\n",
    "\n",
    "# # Usage\n",
    "# video_dir = '/mnt/e/data/vox2/0_500_512_video/id00062/ImB2zCgOuyk'\n",
    "# video_paths = glob.glob(os.path.join(video_dir, '*.mp4'))\n",
    "\n",
    "# print(f\"Found {len(video_paths)} video files.\")\n",
    "\n",
    "# frame_queue = read_multiple_videos(video_paths)\n",
    "\n",
    "# all_frames = []\n",
    "# total_frames = 0\n",
    "\n",
    "# while True:\n",
    "#     item = frame_queue.get()\n",
    "#     if item is None:\n",
    "#         break\n",
    "#     video_path, frames = item\n",
    "#     all_frames.extend(frames)\n",
    "#     total_frames += len(frames)\n",
    "#     print(f\"Processed video: {video_path}, frames: {len(frames)}\")\n",
    "\n",
    "# print(f\"\\nTotal frames across all videos: {total_frames}\")\n",
    "\n",
    "# # Convert to numpy array\n",
    "# all_frames = np.array(all_frames)\n",
    "\n",
    "# print(f\"Shape of concatenated array: {all_frames.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driving_rgb_lst = all_frames\n",
    "driving_rgb_crop_256x256_lst = [cv2.resize(_, (256, 256)) for _ in driving_rgb_lst]\n",
    "I_d_lst = prepare_videos(driving_rgb_crop_256x256_lst, device)\n",
    "# I_d_lst = I_d_lst.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([405, 1, 3, 256, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_d_lst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motion Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kp_info(x: torch.Tensor, **kwargs) -> dict:\n",
    "    \"\"\" get the implicit keypoint information\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    flag_refine_info: whether to trandform the pose to degrees and the dimention of the reshape\n",
    "    return: A dict contains keys: 'pitch', 'yaw', 'roll', 't', 'exp', 'scale', 'kp'\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        kp_info = motion_extractor(x)\n",
    "\n",
    "        if inference_cfg.flag_use_half_precision:\n",
    "            # float the dict\n",
    "            for k, v in kp_info.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    kp_info[k] = v.float()\n",
    "\n",
    "    flag_refine_info: bool = kwargs.get('flag_refine_info', True)\n",
    "    if flag_refine_info:\n",
    "        bs = kp_info['kp'].shape[0]\n",
    "        kp_info['pitch'] = headpose_pred_to_degree(kp_info['pitch'])[:, None]  # Bx1\n",
    "        kp_info['yaw'] = headpose_pred_to_degree(kp_info['yaw'])[:, None]  # Bx1\n",
    "        kp_info['roll'] = headpose_pred_to_degree(kp_info['roll'])[:, None]  # Bx1\n",
    "        kp_info['kp'] = kp_info['kp'].reshape(bs, -1, 3)  # BxNx3\n",
    "        kp_info['exp'] = kp_info['exp'].reshape(bs, -1, 3)  # BxNx3\n",
    "\n",
    "    return kp_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 1, 3, 256, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_d_lst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bz = 250\n",
    "# total_count = I_d_lst.shape[0]\n",
    "# for i in range(0, total_count, bz):\n",
    "#     I_d_part = I_d_lst[i:i+bz]\n",
    "#     x_i_info = get_kp_info(I_d_part)\n",
    "#     print(x_i_info['kp'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_i_info = get_kp_info(I_d_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 0 done\n",
      "frame 1 done\n",
      "frame 2 done\n",
      "frame 3 done\n",
      "frame 4 done\n",
      "frame 5 done\n",
      "frame 6 done\n",
      "frame 7 done\n",
      "frame 8 done\n",
      "frame 9 done\n",
      "frame 10 done\n",
      "frame 11 done\n",
      "frame 12 done\n",
      "frame 13 done\n",
      "frame 14 done\n",
      "frame 15 done\n",
      "frame 16 done\n",
      "frame 17 done\n",
      "frame 18 done\n",
      "frame 19 done\n",
      "frame 20 done\n",
      "frame 21 done\n",
      "frame 22 done\n",
      "frame 23 done\n",
      "frame 24 done\n",
      "frame 25 done\n",
      "frame 26 done\n",
      "frame 27 done\n",
      "frame 28 done\n",
      "frame 29 done\n",
      "frame 30 done\n",
      "frame 31 done\n",
      "frame 32 done\n",
      "frame 33 done\n",
      "frame 34 done\n",
      "frame 35 done\n",
      "frame 36 done\n",
      "frame 37 done\n",
      "frame 38 done\n",
      "frame 39 done\n",
      "frame 40 done\n",
      "frame 41 done\n",
      "frame 42 done\n",
      "frame 43 done\n",
      "frame 44 done\n",
      "frame 45 done\n",
      "frame 46 done\n",
      "frame 47 done\n",
      "frame 48 done\n",
      "frame 49 done\n",
      "frame 50 done\n",
      "frame 51 done\n",
      "frame 52 done\n",
      "frame 53 done\n",
      "frame 54 done\n",
      "frame 55 done\n",
      "frame 56 done\n",
      "frame 57 done\n",
      "frame 58 done\n",
      "frame 59 done\n",
      "frame 60 done\n",
      "frame 61 done\n",
      "frame 62 done\n",
      "frame 63 done\n",
      "frame 64 done\n",
      "frame 65 done\n",
      "frame 66 done\n",
      "frame 67 done\n",
      "frame 68 done\n",
      "frame 69 done\n",
      "frame 70 done\n",
      "frame 71 done\n",
      "frame 72 done\n",
      "frame 73 done\n",
      "frame 74 done\n",
      "frame 75 done\n",
      "frame 76 done\n",
      "frame 77 done\n",
      "frame 78 done\n",
      "frame 79 done\n",
      "frame 80 done\n",
      "frame 81 done\n",
      "frame 82 done\n",
      "frame 83 done\n",
      "frame 84 done\n",
      "frame 85 done\n",
      "frame 86 done\n",
      "frame 87 done\n",
      "frame 88 done\n",
      "frame 89 done\n",
      "frame 90 done\n",
      "frame 91 done\n",
      "frame 92 done\n",
      "frame 93 done\n",
      "frame 94 done\n",
      "frame 95 done\n",
      "frame 96 done\n",
      "frame 97 done\n",
      "frame 98 done\n",
      "frame 99 done\n",
      "frame 100 done\n",
      "frame 101 done\n",
      "frame 102 done\n",
      "frame 103 done\n",
      "frame 104 done\n",
      "frame 105 done\n",
      "frame 106 done\n",
      "frame 107 done\n",
      "frame 108 done\n",
      "frame 109 done\n",
      "frame 110 done\n",
      "frame 111 done\n",
      "frame 112 done\n",
      "frame 113 done\n",
      "frame 114 done\n",
      "frame 115 done\n",
      "frame 116 done\n",
      "frame 117 done\n",
      "frame 118 done\n",
      "frame 119 done\n",
      "frame 120 done\n",
      "frame 121 done\n",
      "frame 122 done\n",
      "frame 123 done\n",
      "frame 124 done\n",
      "frame 125 done\n",
      "frame 126 done\n",
      "frame 127 done\n",
      "frame 128 done\n",
      "frame 129 done\n",
      "frame 130 done\n",
      "frame 131 done\n",
      "frame 132 done\n",
      "frame 133 done\n",
      "frame 134 done\n",
      "frame 135 done\n",
      "frame 136 done\n",
      "frame 137 done\n",
      "frame 138 done\n",
      "frame 139 done\n",
      "frame 140 done\n",
      "frame 141 done\n",
      "frame 142 done\n",
      "frame 143 done\n",
      "frame 144 done\n",
      "frame 145 done\n",
      "frame 146 done\n",
      "frame 147 done\n",
      "frame 148 done\n",
      "frame 149 done\n",
      "frame 150 done\n",
      "frame 151 done\n",
      "frame 152 done\n",
      "frame 153 done\n",
      "frame 154 done\n",
      "frame 155 done\n",
      "frame 156 done\n",
      "frame 157 done\n",
      "frame 158 done\n",
      "frame 159 done\n",
      "frame 160 done\n",
      "frame 161 done\n",
      "frame 162 done\n",
      "frame 163 done\n",
      "frame 164 done\n",
      "frame 165 done\n",
      "frame 166 done\n",
      "frame 167 done\n",
      "frame 168 done\n",
      "frame 169 done\n",
      "frame 170 done\n",
      "frame 171 done\n",
      "frame 172 done\n",
      "frame 173 done\n",
      "frame 174 done\n",
      "frame 175 done\n",
      "frame 176 done\n",
      "frame 177 done\n",
      "frame 178 done\n",
      "frame 179 done\n",
      "frame 180 done\n",
      "frame 181 done\n",
      "frame 182 done\n",
      "frame 183 done\n",
      "frame 184 done\n",
      "frame 185 done\n",
      "frame 186 done\n",
      "frame 187 done\n",
      "frame 188 done\n",
      "frame 189 done\n",
      "frame 190 done\n",
      "frame 191 done\n",
      "frame 192 done\n",
      "frame 193 done\n",
      "frame 194 done\n",
      "frame 195 done\n",
      "frame 196 done\n",
      "frame 197 done\n",
      "frame 198 done\n",
      "frame 199 done\n",
      "frame 200 done\n",
      "frame 201 done\n",
      "frame 202 done\n",
      "frame 203 done\n",
      "frame 204 done\n",
      "frame 205 done\n",
      "frame 206 done\n",
      "frame 207 done\n",
      "frame 208 done\n",
      "frame 209 done\n",
      "frame 210 done\n",
      "frame 211 done\n",
      "frame 212 done\n",
      "frame 213 done\n",
      "frame 214 done\n",
      "frame 215 done\n",
      "frame 216 done\n",
      "frame 217 done\n",
      "frame 218 done\n",
      "frame 219 done\n",
      "frame 220 done\n",
      "frame 221 done\n",
      "frame 222 done\n",
      "frame 223 done\n",
      "frame 224 done\n",
      "frame 225 done\n",
      "frame 226 done\n",
      "frame 227 done\n",
      "frame 228 done\n",
      "frame 229 done\n",
      "frame 230 done\n",
      "frame 231 done\n",
      "frame 232 done\n",
      "frame 233 done\n",
      "frame 234 done\n",
      "frame 235 done\n",
      "frame 236 done\n",
      "frame 237 done\n",
      "frame 238 done\n",
      "frame 239 done\n",
      "frame 240 done\n",
      "frame 241 done\n",
      "frame 242 done\n",
      "frame 243 done\n",
      "frame 244 done\n",
      "frame 245 done\n",
      "frame 246 done\n",
      "frame 247 done\n",
      "frame 248 done\n",
      "frame 249 done\n",
      "frame 250 done\n",
      "frame 251 done\n",
      "frame 252 done\n",
      "frame 253 done\n",
      "frame 254 done\n",
      "frame 255 done\n",
      "frame 256 done\n",
      "frame 257 done\n",
      "frame 258 done\n",
      "frame 259 done\n",
      "frame 260 done\n",
      "frame 261 done\n",
      "frame 262 done\n",
      "frame 263 done\n",
      "frame 264 done\n",
      "frame 265 done\n",
      "frame 266 done\n",
      "frame 267 done\n",
      "frame 268 done\n",
      "frame 269 done\n",
      "frame 270 done\n",
      "frame 271 done\n",
      "frame 272 done\n",
      "frame 273 done\n",
      "frame 274 done\n",
      "frame 275 done\n",
      "frame 276 done\n",
      "frame 277 done\n",
      "frame 278 done\n",
      "frame 279 done\n",
      "frame 280 done\n",
      "frame 281 done\n",
      "frame 282 done\n",
      "frame 283 done\n",
      "frame 284 done\n",
      "frame 285 done\n",
      "frame 286 done\n",
      "frame 287 done\n",
      "frame 288 done\n",
      "frame 289 done\n",
      "frame 290 done\n",
      "frame 291 done\n",
      "frame 292 done\n",
      "frame 293 done\n",
      "frame 294 done\n",
      "frame 295 done\n",
      "frame 296 done\n",
      "frame 297 done\n",
      "frame 298 done\n",
      "frame 299 done\n",
      "frame 300 done\n",
      "frame 301 done\n",
      "frame 302 done\n",
      "frame 303 done\n",
      "frame 304 done\n",
      "frame 305 done\n",
      "frame 306 done\n",
      "frame 307 done\n",
      "frame 308 done\n",
      "frame 309 done\n",
      "frame 310 done\n",
      "frame 311 done\n",
      "frame 312 done\n",
      "frame 313 done\n",
      "frame 314 done\n",
      "frame 315 done\n",
      "frame 316 done\n",
      "frame 317 done\n",
      "frame 318 done\n",
      "frame 319 done\n",
      "frame 320 done\n",
      "frame 321 done\n",
      "frame 322 done\n",
      "frame 323 done\n",
      "frame 324 done\n",
      "frame 325 done\n",
      "frame 326 done\n",
      "frame 327 done\n",
      "frame 328 done\n",
      "frame 329 done\n",
      "frame 330 done\n",
      "frame 331 done\n",
      "frame 332 done\n",
      "frame 333 done\n",
      "frame 334 done\n",
      "frame 335 done\n",
      "frame 336 done\n",
      "frame 337 done\n",
      "frame 338 done\n",
      "frame 339 done\n",
      "frame 340 done\n",
      "frame 341 done\n",
      "frame 342 done\n",
      "frame 343 done\n",
      "frame 344 done\n",
      "frame 345 done\n",
      "frame 346 done\n",
      "frame 347 done\n",
      "frame 348 done\n",
      "frame 349 done\n",
      "frame 350 done\n",
      "frame 351 done\n",
      "frame 352 done\n",
      "frame 353 done\n",
      "frame 354 done\n",
      "frame 355 done\n",
      "frame 356 done\n",
      "frame 357 done\n",
      "frame 358 done\n",
      "frame 359 done\n",
      "frame 360 done\n",
      "frame 361 done\n",
      "frame 362 done\n",
      "frame 363 done\n",
      "frame 364 done\n",
      "frame 365 done\n",
      "frame 366 done\n",
      "frame 367 done\n",
      "frame 368 done\n",
      "frame 369 done\n",
      "frame 370 done\n",
      "frame 371 done\n",
      "frame 372 done\n",
      "frame 373 done\n",
      "frame 374 done\n",
      "frame 375 done\n",
      "frame 376 done\n",
      "frame 377 done\n",
      "frame 378 done\n",
      "frame 379 done\n",
      "frame 380 done\n",
      "frame 381 done\n",
      "frame 382 done\n",
      "frame 383 done\n",
      "frame 384 done\n",
      "frame 385 done\n",
      "frame 386 done\n",
      "frame 387 done\n",
      "frame 388 done\n",
      "frame 389 done\n",
      "frame 390 done\n",
      "frame 391 done\n",
      "frame 392 done\n",
      "frame 393 done\n",
      "frame 394 done\n",
      "frame 395 done\n",
      "frame 396 done\n",
      "frame 397 done\n",
      "frame 398 done\n",
      "frame 399 done\n",
      "frame 400 done\n",
      "frame 401 done\n",
      "frame 402 done\n",
      "frame 403 done\n",
      "frame 404 done\n"
     ]
    }
   ],
   "source": [
    "n_frames = I_d_lst.shape[0]\n",
    "template_dct = {\n",
    "    'n_frames': n_frames,\n",
    "    'output_fps': 25,\n",
    "    'motion': [],\n",
    "    'c_d_eyes_lst': [],\n",
    "    'c_d_lip_lst': [],\n",
    "    'x_i_info_lst': [],\n",
    "}\n",
    "\n",
    "for i in (range(n_frames)):\n",
    "    # collect s, R, δ and t for inference\n",
    "    I_i = I_d_lst[i]\n",
    "    x_i_info = get_kp_info(I_i)\n",
    "    R_i = get_rotation_matrix(x_i_info['pitch'], x_i_info['yaw'], x_i_info['roll'])\n",
    "\n",
    "    item_dct = {\n",
    "        'scale': x_i_info['scale'].cpu().numpy().astype(np.float32),\n",
    "        'R': R_i.cpu().numpy().astype(np.float32),\n",
    "        'exp': x_i_info['exp'].cpu().numpy().astype(np.float32),\n",
    "        't': x_i_info['t'].cpu().numpy().astype(np.float32),\n",
    "    }\n",
    "\n",
    "    template_dct['motion'].append(item_dct)\n",
    "\n",
    "    # c_eyes = c_d_eyes_lst[i].astype(np.float32)\n",
    "    # template_dct['c_d_eyes_lst'].append(c_eyes)\n",
    "\n",
    "    # c_lip = c_d_lip_lst[i].astype(np.float32)\n",
    "    # template_dct['c_d_lip_lst'].append(c_lip)\n",
    "\n",
    "    template_dct['x_i_info_lst'].append(x_i_info)\n",
    "    print(f'frame {i} done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frontalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3) (1, 21, 3) (1, 3) (1, 1)\n",
      "(9,) (63,) (3,) (1,)\n",
      "-0.3030878 0.9846125 -0.017364502 0.06213379 -0.06512451 0.0 1.46875 1.46875\n"
     ]
    }
   ],
   "source": [
    "R = template_dct['motion'][0]['R']\n",
    "exp = template_dct['motion'][0]['exp']\n",
    "t = template_dct['motion'][0]['t']\n",
    "scale = template_dct['motion'][0]['scale']\n",
    "# print dims\n",
    "print(R.shape, exp.shape, t.shape, scale.shape)\n",
    "# print flatten dims\n",
    "print(R.flatten().shape, exp.flatten().shape, t.flatten().shape, scale.flatten().shape)\n",
    "# print range\n",
    "print(R.min(), R.max(), exp.min(), exp.max(), t.min(), t.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_frames):\n",
    "    R = template_dct['motion'][i]['R']\n",
    "    exp = template_dct['motion'][i]['exp']\n",
    "    t = template_dct['motion'][i]['t']\n",
    "    scale = template_dct['motion'][i]['scale']\n",
    "    info = template_dct['x_i_info_lst'][i]\n",
    "    roll, pitch, yaw = info['roll'], info['pitch'], info['yaw']\n",
    "\n",
    "    new_R = get_rotation_matrix(pitch, yaw, roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant pose (roll, pitch, yaw): [ 2.2473297 13.937256  23.98526  ]\n",
      "Median t: [-0.0970459   0.01846313  0.        ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def angular_distance(pose1, pose2):\n",
    "    diff = torch.abs(pose1 - pose2)\n",
    "    diff = torch.min(diff, 2*torch.pi - diff)\n",
    "    return torch.norm(diff)\n",
    "\n",
    "def find_dominant_pose(poses):\n",
    "    N = poses.shape[0]\n",
    "    total_distances = torch.zeros(N, device=poses.device)\n",
    "    for i in range(N):\n",
    "        distances = angular_distance(poses[i].unsqueeze(0), poses)\n",
    "        total_distances[i] = torch.sum(distances)\n",
    "    min_distance_index = torch.argmin(total_distances)\n",
    "    return poses[min_distance_index], min_distance_index\n",
    "\n",
    "# Prepare data\n",
    "n_frames = len(template_dct['motion'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Collect all poses and t values\n",
    "all_poses = torch.zeros(n_frames, 3, device=device)\n",
    "all_t = torch.zeros(n_frames, 3, device=device)\n",
    "\n",
    "for i in range(n_frames):\n",
    "    info = template_dct['x_i_info_lst'][i]\n",
    "    roll, pitch, yaw = info['roll'], info['pitch'], info['yaw']\n",
    "    all_poses[i] = torch.tensor([roll, pitch, yaw], device=device).squeeze()\n",
    "    all_t[i] = torch.tensor(template_dct['motion'][i]['t'], device=device)\n",
    "\n",
    "# Find dominant pose\n",
    "dominant_pose, _ = find_dominant_pose(all_poses)\n",
    "\n",
    "# Find median t\n",
    "median_t = torch.median(all_t, dim=0).values\n",
    "\n",
    "# Subtract dominant pose and median t from the sequence\n",
    "for i in range(n_frames):\n",
    "    # Update pose\n",
    "    template_dct['x_i_info_lst'][i]['roll'] = (all_poses[i, 0]  - 2 * dominant_pose[0]).unsqueeze(0)\n",
    "    template_dct['x_i_info_lst'][i]['pitch'] = (all_poses[i, 1] - 2 * dominant_pose[1]).unsqueeze(0)\n",
    "    template_dct['x_i_info_lst'][i]['yaw'] = (all_poses[i, 2]   - 2 * dominant_pose[2]).unsqueeze(0)\n",
    "\n",
    "    # Update t\n",
    "    template_dct['motion'][i]['t'] = (all_t[i] - median_t).cpu().numpy()\n",
    "\n",
    "    # Recalculate R with the updated pose\n",
    "    new_R = get_rotation_matrix(\n",
    "        template_dct['x_i_info_lst'][i]['pitch'],\n",
    "        template_dct['x_i_info_lst'][i]['yaw'],\n",
    "        template_dct['x_i_info_lst'][i]['roll']\n",
    "    )\n",
    "    template_dct['motion'][i]['R'] = new_R.cpu().numpy()\n",
    "\n",
    "print(f\"Dominant pose (roll, pitch, yaw): {dominant_pose.cpu().numpy()}\")\n",
    "print(f\"Median t: {median_t.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Speed test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_source(img: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    img: HxWx3, uint8, 256x256\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    x = img.copy()\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        x = x[np.newaxis].astype(np.float32) / 255.  # HxWx3 -> 1xHxWx3, normalized to 0~1\n",
    "    elif x.ndim == 4:\n",
    "        x = x.astype(np.float32) / 255.  # BxHxWx3, normalized to 0~1\n",
    "    else:\n",
    "        raise ValueError(f'img ndim should be 3 or 4: {x.ndim}')\n",
    "    x = np.clip(x, 0, 1)  # clip to 0~1\n",
    "    x = torch.from_numpy(x).permute(0, 3, 1, 2)  # 1xHxWx3 -> 1x3xHxW\n",
    "    x = x.to(device)\n",
    "    return x\n",
    "\n",
    "def warp_decode(feature_3d: torch.Tensor, kp_source: torch.Tensor, kp_driving: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the image after the warping of the implicit keypoints\n",
    "    feature_3d: Bx32x16x64x64, feature volume\n",
    "    kp_source: BxNx3\n",
    "    kp_driving: BxNx3\n",
    "    \"\"\"\n",
    "    # The line 18 in Algorithm 1: D(W(f_s; x_s, x′_d,i)）\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        # get decoder input\n",
    "        ret_dct = warping_module(feature_3d, kp_source=kp_source, kp_driving=kp_driving)\n",
    "        # decode\n",
    "        ret_dct['out'] = spade_generator(feature=ret_dct['out'])\n",
    "\n",
    "    return ret_dct\n",
    "\n",
    "def extract_feature_3d( x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the appearance feature of the image by F\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        feature_3d = appearance_feature_extractor(x)\n",
    "\n",
    "    return feature_3d.float()\n",
    "\n",
    "def transform_keypoint(kp_info: dict):\n",
    "    \"\"\"\n",
    "    transform the implicit keypoints with the pose, shift, and expression deformation\n",
    "    kp: BxNx3\n",
    "    \"\"\"\n",
    "    kp = kp_info['kp']    # (bs, k, 3)\n",
    "    pitch, yaw, roll = kp_info['pitch'], kp_info['yaw'], kp_info['roll']\n",
    "\n",
    "    t, exp = kp_info['t'], kp_info['exp']\n",
    "    scale = kp_info['scale']\n",
    "\n",
    "    pitch = headpose_pred_to_degree(pitch)\n",
    "    yaw = headpose_pred_to_degree(yaw)\n",
    "    roll = headpose_pred_to_degree(roll)\n",
    "\n",
    "    bs = kp.shape[0]\n",
    "    if kp.ndim == 2:\n",
    "        num_kp = kp.shape[1] // 3  # Bx(num_kpx3)\n",
    "    else:\n",
    "        num_kp = kp.shape[1]  # Bxnum_kpx3\n",
    "\n",
    "    rot_mat = get_rotation_matrix(pitch, yaw, roll)    # (bs, 3, 3)\n",
    "\n",
    "    # Eqn.2: s * (R * x_c,s + exp) + t\n",
    "    kp_transformed = kp.view(bs, num_kp, 3) @ rot_mat + exp.view(bs, num_kp, 3)\n",
    "    kp_transformed *= scale[..., None]  # (bs, k, 3) * (bs, 1, 1) = (bs, k, 3)\n",
    "    kp_transformed[:, :, 0:2] += t[:, None, 0:2]  # remove z, only apply tx ty\n",
    "\n",
    "    return kp_transformed\n",
    "\n",
    "def parse_output(out: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\" construct the output as standard\n",
    "    return: 1xHxWx3, uint8\n",
    "    \"\"\"\n",
    "    out = np.transpose(out.data.cpu().numpy(), [0, 2, 3, 1])  # 1x3xHxW -> 1xHxWx3\n",
    "    out = np.clip(out, 0, 1)  # clip to 0~1\n",
    "    out = np.clip(out * 255, 0, 255).astype(np.uint8)  # 0~1 -> 0~255\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize source image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/mnt/c/Users/mjh/Downloads/live_in/t4.jpg'\n",
    "img_rgb = load_image_rgb(input_path)\n",
    "source_rgb_lst = [img_rgb]\n",
    "\n",
    "source_lmk = cropper.calc_lmk_from_cropped_image(source_rgb_lst[0])\n",
    "img_crop_256x256 = cv2.resize(source_rgb_lst[0], (256, 256))  # force to resize to 256x256\n",
    "\n",
    "I_s = prepare_source(img_crop_256x256)\n",
    "x_s_info = get_kp_info(I_s)\n",
    "x_c_s = x_s_info['kp']\n",
    "x_s = transform_keypoint(x_s_info)\n",
    "f_s = extract_feature_3d(I_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single frame retarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1]) torch.Size([1, 3, 3]) torch.Size([1, 21, 3]) torch.Size([1, 3])\n",
      "warp_decode time: 0.033080101013183594\n"
     ]
    }
   ],
   "source": [
    "R = template_dct['motion'][0]['R']\n",
    "exp = template_dct['motion'][0]['exp']\n",
    "t = template_dct['motion'][0]['t']\n",
    "scale = template_dct['motion'][0]['scale']\n",
    "\n",
    "scale_tensor = torch.tensor(scale, device=device)\n",
    "R_tensor = torch.tensor(R, device=device)\n",
    "exp_tensor = torch.tensor(exp, device=device)\n",
    "t_tensor = torch.tensor(t, device=device)\n",
    "print(scale_tensor.shape, R_tensor.shape, exp_tensor.shape, t_tensor.shape)\n",
    "\n",
    "start = time.time()\n",
    "x_d_i_new = scale_tensor * (x_c_s @ R_tensor + exp_tensor) + t_tensor\n",
    "\n",
    "# x_d_i_new = scale * (x_c_s @ R + exp) + t\n",
    "out = warp_decode(f_s, x_s, x_d_i_new)\n",
    "# print(out)\n",
    "# I_p_i = parse_output(out['out'])[0]\n",
    "end_time = time.time() - start\n",
    "print(f'warp_decode time: {end_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large chunk of frames generator. Performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "# Initialize variables\n",
    "frame_index = 0\n",
    "total_frames = len(template_dct['motion'])\n",
    "\n",
    "# Create a window for display\n",
    "# cv2.namedWindow('Processed Frame', cv2.WINDOW_NORMAL)\n",
    "# cv2.resizeWindow('Processed Frame', 512, 512)  # Adjust size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frame 1/405\n",
      "Processed frame 2/405\n",
      "Processed frame 3/405\n",
      "Processed frame 4/405\n",
      "Processed frame 5/405\n",
      "Processed frame 6/405\n",
      "Processed frame 7/405\n",
      "Processed frame 8/405\n",
      "Processed frame 9/405\n",
      "Processed frame 10/405\n",
      "Processed frame 11/405\n",
      "Processed frame 12/405\n",
      "Processed frame 13/405\n",
      "Processed frame 14/405\n",
      "Processed frame 15/405\n",
      "Processed frame 16/405\n",
      "Processed frame 17/405\n",
      "Processed frame 18/405\n",
      "Processed frame 19/405\n",
      "Processed frame 20/405\n",
      "Processed frame 21/405\n",
      "Processed frame 22/405\n",
      "Processed frame 23/405\n",
      "Processed frame 24/405\n",
      "Processed frame 25/405\n",
      "Processed frame 26/405\n",
      "Processed frame 27/405\n",
      "Processed frame 28/405\n",
      "Processed frame 29/405\n",
      "Processed frame 30/405\n",
      "Processed frame 31/405\n",
      "Processed frame 32/405\n",
      "Processed frame 33/405\n",
      "Processed frame 34/405\n",
      "Processed frame 35/405\n",
      "Processed frame 36/405\n",
      "Processed frame 37/405\n",
      "Processed frame 38/405\n",
      "Processed frame 39/405\n",
      "Processed frame 40/405\n",
      "Processed frame 41/405\n",
      "Processed frame 42/405\n",
      "Processed frame 43/405\n",
      "Processed frame 44/405\n",
      "Processed frame 45/405\n",
      "Processed frame 46/405\n",
      "Processed frame 47/405\n",
      "Processed frame 48/405\n",
      "Processed frame 49/405\n",
      "Processed frame 50/405\n",
      "Processed frame 51/405\n",
      "Processed frame 52/405\n",
      "Processed frame 53/405\n",
      "Processed frame 54/405\n",
      "Processed frame 55/405\n",
      "Processed frame 56/405\n",
      "Processed frame 57/405\n",
      "Processed frame 58/405\n",
      "Processed frame 59/405\n",
      "Processed frame 60/405\n",
      "Processed frame 61/405\n",
      "Processed frame 62/405\n",
      "Processed frame 63/405\n",
      "Processed frame 64/405\n",
      "Processed frame 65/405\n",
      "Processed frame 66/405\n",
      "Processed frame 67/405\n",
      "Processed frame 68/405\n",
      "Processed frame 69/405\n",
      "Processed frame 70/405\n",
      "Processed frame 71/405\n",
      "Processed frame 72/405\n",
      "Processed frame 73/405\n",
      "Processed frame 74/405\n",
      "Processed frame 75/405\n",
      "Processed frame 76/405\n",
      "Processed frame 77/405\n",
      "Processed frame 78/405\n",
      "Processed frame 79/405\n",
      "Processed frame 80/405\n",
      "Processed frame 81/405\n",
      "Processed frame 82/405\n",
      "Processed frame 83/405\n",
      "Processed frame 84/405\n",
      "Processed frame 85/405\n",
      "Processed frame 86/405\n",
      "Processed frame 87/405\n",
      "Processed frame 88/405\n",
      "Processed frame 89/405\n",
      "Processed frame 90/405\n",
      "Processed frame 91/405\n",
      "Processed frame 92/405\n",
      "Processed frame 93/405\n",
      "Processed frame 94/405\n",
      "Processed frame 95/405\n",
      "Processed frame 96/405\n",
      "Processed frame 97/405\n",
      "Processed frame 98/405\n",
      "Processed frame 99/405\n",
      "Processed frame 100/405\n",
      "Processed frame 101/405\n",
      "Processed frame 102/405\n",
      "Processed frame 103/405\n",
      "Processed frame 104/405\n",
      "Processed frame 105/405\n",
      "Processed frame 106/405\n",
      "Processed frame 107/405\n",
      "Processed frame 108/405\n",
      "Processed frame 109/405\n",
      "Processed frame 110/405\n",
      "Processed frame 111/405\n",
      "Processed frame 112/405\n",
      "Processed frame 113/405\n",
      "Processed frame 114/405\n",
      "Processed frame 115/405\n",
      "Processed frame 116/405\n",
      "Processed frame 117/405\n",
      "Processed frame 118/405\n",
      "Processed frame 119/405\n",
      "Processed frame 120/405\n",
      "Processed frame 121/405\n",
      "Processed frame 122/405\n",
      "Processed frame 123/405\n",
      "Processed frame 124/405\n",
      "Processed frame 125/405\n",
      "Processed frame 126/405\n",
      "Processed frame 127/405\n",
      "Processed frame 128/405\n",
      "Processed frame 129/405\n",
      "Processed frame 130/405\n",
      "Processed frame 131/405\n",
      "Processed frame 132/405\n",
      "Processed frame 133/405\n",
      "Processed frame 134/405\n",
      "Processed frame 135/405\n",
      "Processed frame 136/405\n",
      "Processed frame 137/405\n",
      "Processed frame 138/405\n",
      "Processed frame 139/405\n",
      "Processed frame 140/405\n",
      "Processed frame 141/405\n",
      "Processed frame 142/405\n",
      "Processed frame 143/405\n",
      "Processed frame 144/405\n",
      "Processed frame 145/405\n",
      "Processed frame 146/405\n",
      "Processed frame 147/405\n",
      "Processed frame 148/405\n",
      "Processed frame 149/405\n",
      "Processed frame 150/405\n",
      "Processed frame 151/405\n",
      "Processed frame 152/405\n",
      "Processed frame 153/405\n",
      "Processed frame 154/405\n",
      "Processed frame 155/405\n",
      "Processed frame 156/405\n",
      "Processed frame 157/405\n",
      "Processed frame 158/405\n",
      "Processed frame 159/405\n",
      "Processed frame 160/405\n",
      "Processed frame 161/405\n",
      "Processed frame 162/405\n",
      "Processed frame 163/405\n",
      "Processed frame 164/405\n",
      "Processed frame 165/405\n",
      "Processed frame 166/405\n",
      "Processed frame 167/405\n",
      "Processed frame 168/405\n",
      "Processed frame 169/405\n",
      "Processed frame 170/405\n",
      "Processed frame 171/405\n",
      "Processed frame 172/405\n",
      "Processed frame 173/405\n",
      "Processed frame 174/405\n",
      "Processed frame 175/405\n",
      "Processed frame 176/405\n",
      "Processed frame 177/405\n",
      "Processed frame 178/405\n",
      "Processed frame 179/405\n",
      "Processed frame 180/405\n",
      "Processed frame 181/405\n",
      "Processed frame 182/405\n",
      "Processed frame 183/405\n",
      "Processed frame 184/405\n",
      "Processed frame 185/405\n",
      "Processed frame 186/405\n",
      "Processed frame 187/405\n",
      "Processed frame 188/405\n",
      "Processed frame 189/405\n",
      "Processed frame 190/405\n",
      "Processed frame 191/405\n",
      "Processed frame 192/405\n",
      "Processed frame 193/405\n",
      "Processed frame 194/405\n",
      "Processed frame 195/405\n",
      "Processed frame 196/405\n",
      "Processed frame 197/405\n",
      "Processed frame 198/405\n",
      "Processed frame 199/405\n",
      "Processed frame 200/405\n",
      "Processed frame 201/405\n",
      "Processed frame 202/405\n",
      "Processed frame 203/405\n",
      "Processed frame 204/405\n",
      "Processed frame 205/405\n",
      "Processed frame 206/405\n",
      "Processed frame 207/405\n",
      "Processed frame 208/405\n",
      "Processed frame 209/405\n",
      "Processed frame 210/405\n",
      "Processed frame 211/405\n",
      "Processed frame 212/405\n",
      "Processed frame 213/405\n",
      "Processed frame 214/405\n",
      "Processed frame 215/405\n",
      "Processed frame 216/405\n",
      "Processed frame 217/405\n",
      "Processed frame 218/405\n",
      "Processed frame 219/405\n",
      "Processed frame 220/405\n",
      "Processed frame 221/405\n",
      "Processed frame 222/405\n",
      "Processed frame 223/405\n",
      "Processed frame 224/405\n",
      "Processed frame 225/405\n",
      "Processed frame 226/405\n",
      "Processed frame 227/405\n",
      "Processed frame 228/405\n",
      "Processed frame 229/405\n",
      "Processed frame 230/405\n",
      "Processed frame 231/405\n",
      "Processed frame 232/405\n",
      "Processed frame 233/405\n",
      "Processed frame 234/405\n",
      "Processed frame 235/405\n",
      "Processed frame 236/405\n",
      "Processed frame 237/405\n",
      "Processed frame 238/405\n",
      "Processed frame 239/405\n",
      "Processed frame 240/405\n",
      "Processed frame 241/405\n",
      "Processed frame 242/405\n",
      "Processed frame 243/405\n",
      "Processed frame 244/405\n",
      "Processed frame 245/405\n",
      "Processed frame 246/405\n",
      "Processed frame 247/405\n",
      "Processed frame 248/405\n",
      "Processed frame 249/405\n",
      "Processed frame 250/405\n",
      "Processed frame 251/405\n",
      "Processed frame 252/405\n",
      "Processed frame 253/405\n",
      "Processed frame 254/405\n",
      "Processed frame 255/405\n",
      "Processed frame 256/405\n",
      "Processed frame 257/405\n",
      "Processed frame 258/405\n",
      "Processed frame 259/405\n",
      "Processed frame 260/405\n",
      "Processed frame 261/405\n",
      "Processed frame 262/405\n",
      "Processed frame 263/405\n",
      "Processed frame 264/405\n",
      "Processed frame 265/405\n",
      "Processed frame 266/405\n",
      "Processed frame 267/405\n",
      "Processed frame 268/405\n",
      "Processed frame 269/405\n",
      "Processed frame 270/405\n",
      "Processed frame 271/405\n",
      "Processed frame 272/405\n",
      "Processed frame 273/405\n",
      "Processed frame 274/405\n",
      "Processed frame 275/405\n",
      "Processed frame 276/405\n",
      "Processed frame 277/405\n",
      "Processed frame 278/405\n",
      "Processed frame 279/405\n",
      "Processed frame 280/405\n",
      "Processed frame 281/405\n",
      "Processed frame 282/405\n",
      "Processed frame 283/405\n",
      "Processed frame 284/405\n",
      "Processed frame 285/405\n",
      "Processed frame 286/405\n",
      "Processed frame 287/405\n",
      "Processed frame 288/405\n",
      "Processed frame 289/405\n",
      "Processed frame 290/405\n",
      "Processed frame 291/405\n",
      "Processed frame 292/405\n",
      "Processed frame 293/405\n",
      "Processed frame 294/405\n",
      "Processed frame 295/405\n",
      "Processed frame 296/405\n",
      "Processed frame 297/405\n",
      "Processed frame 298/405\n",
      "Processed frame 299/405\n",
      "Processed frame 300/405\n",
      "Processed frame 301/405\n",
      "Processed frame 302/405\n",
      "Processed frame 303/405\n",
      "Processed frame 304/405\n",
      "Processed frame 305/405\n",
      "Processed frame 306/405\n",
      "Processed frame 307/405\n",
      "Processed frame 308/405\n",
      "Processed frame 309/405\n",
      "Processed frame 310/405\n",
      "Processed frame 311/405\n",
      "Processed frame 312/405\n",
      "Processed frame 313/405\n",
      "Processed frame 314/405\n",
      "Processed frame 315/405\n",
      "Processed frame 316/405\n",
      "Processed frame 317/405\n",
      "Processed frame 318/405\n",
      "Processed frame 319/405\n",
      "Processed frame 320/405\n",
      "Processed frame 321/405\n",
      "Processed frame 322/405\n",
      "Processed frame 323/405\n",
      "Processed frame 324/405\n",
      "Processed frame 325/405\n",
      "Processed frame 326/405\n",
      "Processed frame 327/405\n",
      "Processed frame 328/405\n",
      "Processed frame 329/405\n",
      "Processed frame 330/405\n",
      "Processed frame 331/405\n",
      "Processed frame 332/405\n",
      "Processed frame 333/405\n",
      "Processed frame 334/405\n",
      "Processed frame 335/405\n",
      "Processed frame 336/405\n",
      "Processed frame 337/405\n",
      "Processed frame 338/405\n",
      "Processed frame 339/405\n",
      "Processed frame 340/405\n",
      "Processed frame 341/405\n",
      "Processed frame 342/405\n",
      "Processed frame 343/405\n",
      "Processed frame 344/405\n",
      "Processed frame 345/405\n",
      "Processed frame 346/405\n",
      "Processed frame 347/405\n",
      "Processed frame 348/405\n",
      "Processed frame 349/405\n",
      "Processed frame 350/405\n",
      "Processed frame 351/405\n",
      "Processed frame 352/405\n",
      "Processed frame 353/405\n",
      "Processed frame 354/405\n",
      "Processed frame 355/405\n",
      "Processed frame 356/405\n",
      "Processed frame 357/405\n",
      "Processed frame 358/405\n",
      "Processed frame 359/405\n",
      "Processed frame 360/405\n",
      "Processed frame 361/405\n",
      "Processed frame 362/405\n",
      "Processed frame 363/405\n",
      "Processed frame 364/405\n",
      "Processed frame 365/405\n",
      "Processed frame 366/405\n",
      "Processed frame 367/405\n",
      "Processed frame 368/405\n",
      "Processed frame 369/405\n",
      "Processed frame 370/405\n",
      "Processed frame 371/405\n",
      "Processed frame 372/405\n",
      "Processed frame 373/405\n",
      "Processed frame 374/405\n",
      "Processed frame 375/405\n",
      "Processed frame 376/405\n",
      "Processed frame 377/405\n",
      "Processed frame 378/405\n",
      "Processed frame 379/405\n",
      "Processed frame 380/405\n",
      "Processed frame 381/405\n",
      "Processed frame 382/405\n",
      "Processed frame 383/405\n",
      "Processed frame 384/405\n",
      "Processed frame 385/405\n",
      "Processed frame 386/405\n",
      "Processed frame 387/405\n",
      "Processed frame 388/405\n",
      "Processed frame 389/405\n",
      "Processed frame 390/405\n",
      "Processed frame 391/405\n",
      "Processed frame 392/405\n",
      "Processed frame 393/405\n",
      "Processed frame 394/405\n",
      "Processed frame 395/405\n",
      "Processed frame 396/405\n",
      "Processed frame 397/405\n",
      "Processed frame 398/405\n",
      "Processed frame 399/405\n",
      "Processed frame 400/405\n",
      "Processed frame 401/405\n",
      "Processed frame 402/405\n",
      "Processed frame 403/405\n",
      "Processed frame 404/405\n",
      "Processed frame 405/405\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "# Initialize variables\n",
    "frame_index = 0\n",
    "total_frames = len(template_dct['motion'])\n",
    "\n",
    "# Create a window for display\n",
    "cv2.namedWindow('Processed Frame', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Processed Frame', 512, 512)  # Adjust size as needed\n",
    "\n",
    "while frame_index < total_frames:\n",
    "    # Get motion data for the current frame\n",
    "    R = template_dct['motion'][frame_index]['R']\n",
    "    exp = template_dct['motion'][frame_index]['exp']\n",
    "    t = template_dct['motion'][frame_index]['t']\n",
    "    scale = template_dct['motion'][frame_index]['scale']\n",
    "\n",
    "    # Convert to tensors\n",
    "    scale_tensor = torch.tensor(scale, device=device)\n",
    "    R_tensor = torch.tensor(R, device=device)\n",
    "    exp_tensor = torch.tensor(exp, device=device)\n",
    "    t_tensor = torch.tensor(t, device=device)\n",
    "\n",
    "    # Process the frame\n",
    "    x_d_i_new = scale_tensor * (x_c_s @ R_tensor + exp_tensor) + t_tensor\n",
    "    out = warp_decode(f_s, x_s, x_d_i_new)\n",
    "\n",
    "    # Convert tensor to numpy array and rescale to 0-255 range\n",
    "    img_np = (out['out'][0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    # Convert from RGB to BGR for cv2\n",
    "    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Processed Frame', img_bgr)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Processed frame {frame_index+1}/{total_frames}\")\n",
    "\n",
    "    # Wait for a short time and check for 'q' key to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_index += 1\n",
    "\n",
    "    # Optional: add a small delay to make the display more visible\n",
    "    time.sleep(0.03)  # Adjust as needed\n",
    "\n",
    "# Clean up\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "live_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
