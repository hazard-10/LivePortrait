{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile\n",
    "and initialize args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile complete\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import contextlib\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "import tyro\n",
    "import subprocess\n",
    "from rich.progress import track\n",
    "\n",
    "from src.config.argument_config import ArgumentConfig\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.config.crop_config import CropConfig\n",
    "\n",
    "def partial_fields(target_class, kwargs):\n",
    "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
    "\n",
    "args = ArgumentConfig()\n",
    "inference_cfg = partial_fields(InferenceConfig, args.__dict__)\n",
    "crop_cfg = partial_fields(CropConfig, args.__dict__)\n",
    "# print(\"inference_cfg: \", inference_cfg)\n",
    "# print(\"crop_cfg: \", crop_cfg)\n",
    "device = 'cuda'\n",
    "print(\"Compile complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize util functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize motion extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helper import load_model, concat_feat\n",
    "from src.utils.camera import headpose_pred_to_degree, get_rotation_matrix\n",
    "from src.utils.retargeting_utils import calc_eye_close_ratio, calc_lip_close_ratio\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.utils.cropper import Cropper\n",
    "from src.utils.camera import get_rotation_matrix\n",
    "from src.utils.video import images2video, concat_frames, get_fps, add_audio_to_video, has_audio_stream\n",
    "from src.utils.crop import _transform_img, prepare_paste_back, paste_back\n",
    "from src.utils.io import load_image_rgb, load_video, resize_to_limit, dump, load\n",
    "from src.utils.helper import mkdir, basename, dct2device, is_video, is_template, remove_suffix, is_image\n",
    "from src.utils.filter import smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:48:42] </span>LandmarkRunner warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>194s                                                 <a href=\"file:///mnt/e/wsl_projects/LivePortrait/src/utils/landmark_runner.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">landmark_runner.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/e/wsl_projects/LivePortrait/src/utils/landmark_runner.py#95\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">95</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[15:48:42]\u001b[0m\u001b[2;36m \u001b[0mLandmarkRunner warmup time: \u001b[1;36m1.\u001b[0m194s                                                 \u001b]8;id=412785;file:///mnt/e/wsl_projects/LivePortrait/src/utils/landmark_runner.py\u001b\\\u001b[2mlandmark_runner.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=28753;file:///mnt/e/wsl_projects/LivePortrait/src/utils/landmark_runner.py#95\u001b\\\u001b[2m95\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:48:43] </span>FaceAnalysisDIY warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>031s                                              <a href=\"file:///mnt/e/wsl_projects/LivePortrait/src/utils/face_analysis_diy.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">face_analysis_diy.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/e/wsl_projects/LivePortrait/src/utils/face_analysis_diy.py#79\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">79</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[15:48:43]\u001b[0m\u001b[2;36m \u001b[0mFaceAnalysisDIY warmup time: \u001b[1;36m1.\u001b[0m031s                                              \u001b]8;id=775425;file:///mnt/e/wsl_projects/LivePortrait/src/utils/face_analysis_diy.py\u001b\\\u001b[2mface_analysis_diy.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=425399;file:///mnt/e/wsl_projects/LivePortrait/src/utils/face_analysis_diy.py#79\u001b\\\u001b[2m79\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_config = yaml.load(open(inference_cfg.models_config, 'r'), Loader=yaml.SafeLoader)\n",
    "# init F\n",
    "appearance_feature_extractor = load_model(inference_cfg.checkpoint_F, model_config, device, 'appearance_feature_extractor')\n",
    "# init M\n",
    "motion_extractor = load_model(inference_cfg.checkpoint_M, model_config, device, 'motion_extractor')\n",
    "# init W\n",
    "warping_module = load_model(inference_cfg.checkpoint_W, model_config, device, 'warping_module')\n",
    "# init G\n",
    "spade_generator = load_model(inference_cfg.checkpoint_G, model_config, device, 'spade_generator')\n",
    "# init S and R\n",
    "if inference_cfg.checkpoint_S is not None and os.path.exists(inference_cfg.checkpoint_S):\n",
    "    stitching_retargeting_module = load_model(inference_cfg.checkpoint_S, model_config, device, 'stitching_retargeting_module')\n",
    "else:\n",
    "    stitching_retargeting_module = None\n",
    "\n",
    "cropper = Cropper(crop_cfg=crop_cfg, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_distance_ratio(lmk: np.ndarray, idx1: int, idx2: int, idx3: int, idx4: int, eps: float = 1e-6) -> np.ndarray:\n",
    "    return (np.linalg.norm(lmk[:, idx1] - lmk[:, idx2], axis=1, keepdims=True) /\n",
    "            (np.linalg.norm(lmk[:, idx3] - lmk[:, idx4], axis=1, keepdims=True) + eps))\n",
    "\n",
    "\n",
    "def calc_eye_close_ratio(lmk: np.ndarray, target_eye_ratio: np.ndarray = None) -> np.ndarray:\n",
    "    lefteye_close_ratio = calculate_distance_ratio(lmk, 6, 18, 0, 12)\n",
    "    righteye_close_ratio = calculate_distance_ratio(lmk, 30, 42, 24, 36)\n",
    "    if target_eye_ratio is not None:\n",
    "        return np.concatenate([lefteye_close_ratio, righteye_close_ratio, target_eye_ratio], axis=1)\n",
    "    else:\n",
    "        return np.concatenate([lefteye_close_ratio, righteye_close_ratio], axis=1)\n",
    "\n",
    "\n",
    "def calc_lip_close_ratio(lmk: np.ndarray) -> np.ndarray:\n",
    "    return calculate_distance_ratio(lmk, 90, 102, 48, 66)\n",
    "\n",
    "def calc_ratio(lmk_lst):\n",
    "    input_eye_ratio_lst = []\n",
    "    input_lip_ratio_lst = []\n",
    "    for lmk in lmk_lst:\n",
    "        # for eyes retargeting\n",
    "        input_eye_ratio_lst.append(calc_eye_close_ratio(lmk[None]))\n",
    "        # for lip retargeting\n",
    "        input_lip_ratio_lst.append(calc_lip_close_ratio(lmk[None]))\n",
    "    return input_eye_ratio_lst, input_lip_ratio_lst\n",
    "\n",
    "def prepare_videos(imgs, device) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    imgs: NxBxHxWx3, uint8\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, list):\n",
    "        _imgs = np.array(imgs)[..., np.newaxis]  # TxHxWx3x1\n",
    "    elif isinstance(imgs, np.ndarray):\n",
    "        _imgs = imgs\n",
    "    else:\n",
    "        raise ValueError(f'imgs type error: {type(imgs)}')\n",
    "\n",
    "    y = _imgs.astype(np.float32) / 255.\n",
    "    y = np.clip(y, 0, 1)  # clip to 0~1\n",
    "    y = torch.from_numpy(y).permute(0, 4, 3, 1, 2)  # TxHxWx3x1 -> Tx1x3xHxW\n",
    "    y = y.to(device)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Video loading ( SLOW load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_path = '/mnt/e/data/diffposetalk_data/TFHP_raw/crop/TH_00212/000.mp4'\n",
    "is_video(vid_path)\n",
    "driving_rgb_lst = load_video(vid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# driving_rgb_lst = load_video(vid_path)\n",
    "len(driving_rgb_lst), driving_rgb_lst[0].shape, type(driving_rgb_lst[0])\n",
    "driving_lmk_crop_lst = cropper.calc_lmks_from_cropped_video(driving_rgb_lst)\n",
    "driving_rgb_crop_256x256_lst = [cv2.resize(_, (256, 256)) for _ in driving_rgb_lst]\n",
    "\n",
    "c_d_eyes_lst, c_d_lip_lst = calc_ratio(driving_lmk_crop_lst)\n",
    "I_d_lst = prepare_videos(driving_rgb_crop_256x256_lst, device)\n",
    "# I_d_lst = I_d_lst.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder of videos load ( fast )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import cv2\n",
    "import threading\n",
    "import queue\n",
    "import torchvision.transforms as transforms\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import imageio\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for videos: 2it [00:00, 130.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 video files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def read_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (256, 256))  # Resize to 256x256\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return video_path, frames\n",
    "\n",
    "def read_multiple_videos(video_paths, num_threads=16):\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        results = list(executor.map(read_video_frames, video_paths))\n",
    "    return results\n",
    "\n",
    "def get_video_paths(directory, max_videos=100):\n",
    "    video_paths = []\n",
    "    for root, dirs, files in tqdm(os.walk(directory), desc=\"Searching for videos\"):\n",
    "        for file in files:\n",
    "            if file.endswith(('.mp4', '.avi', '.mov')):  # Add more video extensions if needed\n",
    "                video_paths.append(os.path.join(root, file))\n",
    "                if len(video_paths) >= max_videos:\n",
    "                    return sorted(video_paths)\n",
    "    return sorted(video_paths)\n",
    "\n",
    "# Usage\n",
    "# video_dir = '/mnt/e/data/vox2/0_500_512_video/id00062/ImB2zCgOuyk\n",
    "video_dir = '/mnt/e/data/vox2/videos/512'\n",
    "video_paths = get_video_paths(video_dir, max_videos=5)  # Limit to 100 videos\n",
    "\n",
    "print(f\"Found {len(video_paths)} video files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video: /mnt/e/data/vox2/videos/512/id00012/aE4Om0EEiuk/00116.mp4, frames: 187\n",
      "Processed video: /mnt/e/data/vox2/videos/512/id00012/aE4Om0EEiuk/00117.mp4, frames: 172\n",
      "Processed video: /mnt/e/data/vox2/videos/512/id00012/aE4Om0EEiuk/00120.mp4, frames: 412\n",
      "Processed video: /mnt/e/data/vox2/videos/512/id00012/aE4Om0EEiuk/00122.mp4, frames: 300\n",
      "Processed video: /mnt/e/data/vox2/videos/512/id00012/aE4Om0EEiuk/00123.mp4, frames: 252\n",
      "\n",
      "Total frames across all videos: 1323\n",
      "Video lengths: [187, 172, 412, 300, 252]\n",
      "Shape of concatenated array: (1323, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "video_frames = read_multiple_videos(video_paths)\n",
    "all_frames = []\n",
    "total_frames = 0\n",
    "video_lengths = []\n",
    "\n",
    "for video_path, frames in video_frames:\n",
    "    all_frames.extend(frames)\n",
    "    frame_count = len(frames)\n",
    "    total_frames += frame_count\n",
    "    video_lengths.append(frame_count)\n",
    "    print(f\"Processed video: {video_path}, frames: {frame_count}\")\n",
    "\n",
    "print(f\"\\nTotal frames across all videos: {total_frames}\")\n",
    "print(f\"Video lengths: {video_lengths}\")\n",
    "\n",
    "# Convert to numpy array\n",
    "all_frames = np.array(all_frames)\n",
    "\n",
    "print(f\"Shape of concatenated array: {all_frames.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import time\n",
    "\n",
    "# # Create a window\n",
    "# cv2.namedWindow('Video Frames', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# for i in range(208):\n",
    "#     # Get the current frame\n",
    "#     frame = all_frames[i]\n",
    "\n",
    "#     # Display the frame number\n",
    "#     cv2.putText(frame, f'Frame: {i+1}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "#     # Display the image\n",
    "#     cv2.imshow('Video Frames', cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "#     # Wait for a key press or 33 ms (approx. 30 fps)\n",
    "#     key = cv2.waitKey(33) & 0xFF\n",
    "\n",
    "#     # If 'q' is pressed, break the loop\n",
    "#     if key == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Close all OpenCV windows\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_videos_(imgs, device):\n",
    "    \"\"\" construct the input as standard\n",
    "    imgs: NxHxWx3, uint8\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, list):\n",
    "        _imgs = np.array(imgs)\n",
    "    elif isinstance(imgs, np.ndarray):\n",
    "        _imgs = imgs\n",
    "    else:\n",
    "        raise ValueError(f'imgs type error: {type(imgs)}')\n",
    "\n",
    "    # y = _imgs.astype(np.float32) / 255.\n",
    "    y = _imgs\n",
    "    y = torch.from_numpy(y).permute(0, 3, 1, 2)  # NxHxWx3 -> Nx3xHxW\n",
    "    y = y.to(device)\n",
    "    y = y / 255.\n",
    "    y = torch.clamp(y, 0, 1)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driving_rgb_lst = I_d_lst\n",
    "I_d_lst = prepare_videos_(driving_rgb_lst, device)\n",
    "# I_d_lst = I_d_lst.unsqueeze(1)\n",
    "\n",
    "print(f\"Shape of driving video: {I_d_lst.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motion Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kp_info(x: torch.Tensor, **kwargs) -> dict:\n",
    "    \"\"\" get the implicit keypoint information\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    flag_refine_info: whether to trandform the pose to degrees and the dimention of the reshape\n",
    "    return: A dict contains keys: 'pitch', 'yaw', 'roll', 't', 'exp', 'scale', 'kp'\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        kp_info = motion_extractor(x)\n",
    "\n",
    "        if inference_cfg.flag_use_half_precision:\n",
    "            # float the dict\n",
    "            for k, v in kp_info.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    kp_info[k] = v.float()\n",
    "\n",
    "    flag_refine_info: bool = kwargs.get('flag_refine_info', True)\n",
    "    if flag_refine_info:\n",
    "        bs = kp_info['kp'].shape[0]\n",
    "        kp_info['pitch'] = headpose_pred_to_degree(kp_info['pitch'])[:, None]  # Bx1\n",
    "        kp_info['yaw'] = headpose_pred_to_degree(kp_info['yaw'])[:, None]  # Bx1\n",
    "        kp_info['roll'] = headpose_pred_to_degree(kp_info['roll'])[:, None]  # Bx1\n",
    "        # kp_info['kp'] = kp_info['kp'].reshape(bs, -1, 3)  # BxNx3\n",
    "        # kp_info['exp'] = kp_info['exp'].reshape(bs, -1, 3)  # BxNx3\n",
    "\n",
    "    return kp_info\n",
    "\n",
    "def transform_keypoint(kp_info: dict):\n",
    "    \"\"\"\n",
    "    transform the implicit keypoints with the pose, shift, and expression deformation\n",
    "    kp: BxNx3\n",
    "    \"\"\"\n",
    "    kp = kp_info['kp']    # (bs, k, 3)\n",
    "    pitch, yaw, roll = kp_info['pitch'], kp_info['yaw'], kp_info['roll']\n",
    "\n",
    "    t, exp = kp_info['t'], kp_info['exp']\n",
    "    scale = kp_info['scale']\n",
    "\n",
    "    pitch = headpose_pred_to_degree(pitch)\n",
    "    yaw = headpose_pred_to_degree(yaw)\n",
    "    roll = headpose_pred_to_degree(roll)\n",
    "\n",
    "    bs = kp.shape[0]\n",
    "    if kp.ndim == 2:\n",
    "        num_kp = kp.shape[1] // 3  # Bx(num_kpx3)\n",
    "    else:\n",
    "        num_kp = kp.shape[1]  # Bxnum_kpx3\n",
    "\n",
    "    rot_mat = get_rotation_matrix(pitch, yaw, roll)    # (bs, 3, 3)\n",
    "\n",
    "    # Eqn.2: s * (R * x_c,s + exp) + t\n",
    "    kp_transformed = kp.view(bs, num_kp, 3) @ rot_mat + exp.view(bs, num_kp, 3)\n",
    "    kp_transformed *= scale[..., None]  # (bs, k, 3) * (bs, 1, 1) = (bs, k, 3)\n",
    "    kp_transformed[:, :, 0:2] += t[:, None, 0:2]  # remove z, only apply tx ty\n",
    "\n",
    "    return kp_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003, 1, 3, 256, 256])\n",
      "torch.Size([1003, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(I_d_lst.shape)\n",
    "if len(I_d_lst.shape) == 5:\n",
    "    I_d_lst = I_d_lst.squeeze(1)\n",
    "    print(I_d_lst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_d_list: 1003 torch.Size([1003, 133])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "total_frames = I_d_lst.shape[0]\n",
    "x_d_list = []\n",
    "x_i_info_list = []\n",
    "\n",
    "for i in range(0, total_frames, batch_size):\n",
    "    batch = I_d_lst[i:i+batch_size]\n",
    "    x_i_info = get_kp_info(batch)\n",
    "\n",
    "    concat_tensor = torch.cat([\n",
    "        x_i_info['kp'], # 63\n",
    "        x_i_info['exp'], # 63, .reshape(mini_batch_end - mini_batch_start, -1),\n",
    "        x_i_info['t'], # 3\n",
    "        x_i_info['pitch'], # 1\n",
    "        x_i_info['yaw'], # 1\n",
    "        x_i_info['roll'], # 1\n",
    "        x_i_info['scale'], # 1\n",
    "    ], dim=1)\n",
    "\n",
    "    x_i_info_list.append(concat_tensor)\n",
    "x_i_info_list = torch.cat(x_i_info_list, dim=0)\n",
    "\n",
    "print(f'x_d_list: {len(x_i_info_list)}', x_i_info_list.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert x_i_info_list to numpy array\n",
    "x_d_np = x_i_info_list.cpu().numpy()\n",
    "\n",
    "# Define the path to save the numpy array\n",
    "save_path = 'ls /mnt/c/Users/mjh/Downloads/x_d_list.npy'  # Replace with your desired path\n",
    "\n",
    "# Save the numpy array\n",
    "np.save(save_path, x_d_np)\n",
    "\n",
    "print(f\"Saved x_d_list to {save_path}\")\n",
    "print(f\"Shape of saved array: {x_d_np.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Flatten the last two dimensions of x_d\n",
    "x_d_flattened = x_d.reshape(x_d.shape[0], -1)\n",
    "\n",
    "# Get the number of parameters\n",
    "num_params = x_d_flattened.shape[1]\n",
    "\n",
    "# Calculate grid dimensions\n",
    "grid_rows = int(np.ceil(np.sqrt(num_params)))\n",
    "grid_cols = int(np.ceil(num_params / grid_rows))\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for i in range(num_params):\n",
    "    # Extract the current parameter\n",
    "    param_values = x_d_flattened[:, i].cpu().numpy()\n",
    "\n",
    "    # Create a subplot for each parameter\n",
    "    plt.subplot(grid_rows, grid_cols, i + 1)\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.hist(param_values, bins=50, edgecolor='black', range=(-1, 1))\n",
    "\n",
    "    # Set title and labels\n",
    "    plt.title(f'Parameter {i + 1}')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Set fixed x-axis range\n",
    "    plt.xlim(-1, 1)\n",
    "\n",
    "    # Add some statistics to the plot\n",
    "    plt.text(0.05, 0.95, f'Mean: {np.mean(param_values):.4f}\\nStd: {np.std(param_values):.4f}',\n",
    "             transform=plt.gca().transAxes, verticalalignment='top', fontsize=8)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.suptitle('Distribution of Parameters Across Batches', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "for i in range(num_params):\n",
    "    param_values = x_d_flattened[:, i].cpu().numpy()\n",
    "    print(f\"Parameter {i+1}:\")\n",
    "    print(f\"  Mean: {np.mean(param_values):.4f}\")\n",
    "    print(f\"  Std Dev: {np.std(param_values):.4f}\")\n",
    "    print(f\"  Min: {np.min(param_values):.4f}\")\n",
    "    print(f\"  Max: {np.max(param_values):.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bz = 250\n",
    "# total_count = I_d_lst.shape[0]\n",
    "# for i in range(0, total_count, bz):\n",
    "#     I_d_part = I_d_lst[i:i+bz]\n",
    "#     x_i_info = get_kp_info(I_d_part)\n",
    "#     print(x_i_info['kp'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_i_info = get_kp_info(I_d_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_d_lst[0]\n",
    "x_i_info_0 = get_kp_info(I_d_lst[0])\n",
    "\n",
    "x_i_info_0['latent'].shape\n",
    "x_i_info_0_latent_np = x_i_info_0['latent'].squeeze(0).cpu().numpy()\n",
    "x_i_info_0_latent_np.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = I_d_lst.shape[0]\n",
    "template_dct = {\n",
    "    'n_frames': n_frames,\n",
    "    'output_fps': 25,\n",
    "    'motion': [],\n",
    "    'c_d_eyes_lst': [],\n",
    "    'c_d_lip_lst': [],\n",
    "    'x_i_info_lst': [],\n",
    "    'latent_lst': []  # New list to store latent vectors\n",
    "}\n",
    "\n",
    "for i in range(n_frames):\n",
    "    # collect s, R, δ and t for inference\n",
    "    I_i = I_d_lst[i]\n",
    "    x_i_info = get_kp_info(I_i)\n",
    "    R_i = get_rotation_matrix(x_i_info['pitch'], x_i_info['yaw'], x_i_info['roll'])\n",
    "\n",
    "    item_dct = {\n",
    "        'scale': x_i_info['scale'].cpu().numpy().astype(np.float32),\n",
    "        'R': R_i.cpu().numpy().astype(np.float32),\n",
    "        'exp': x_i_info['exp'].cpu().numpy().astype(np.float32),\n",
    "        't': x_i_info['t'].cpu().numpy().astype(np.float32),\n",
    "    }\n",
    "\n",
    "    template_dct['motion'].append(item_dct)\n",
    "    template_dct['x_i_info_lst'].append(x_i_info)\n",
    "\n",
    "    # Extract and store the latent vector\n",
    "    latent = x_i_info['latent'].squeeze(0).cpu().numpy()\n",
    "    template_dct['latent_lst'].append(latent)\n",
    "\n",
    "    print(f'frame {i} done')\n",
    "\n",
    "# After the loop, you can verify the shape of the stored latent vectors\n",
    "print(f\"Number of latent vectors: {len(template_dct['latent_lst'])}\")\n",
    "print(f\"Shape of each latent vector: {template_dct['latent_lst'][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lat_tensor = torch.tensor(template_dct['latent_lst'])\n",
    "\n",
    "# Analyze latent vectors\n",
    "latents_analysis = lat_tensor[:, :]  # Analyze first 100 dimensions\n",
    "latent_dim = latents_analysis.shape[-1]\n",
    "\n",
    "# Flatten the batch dimension\n",
    "flattened_motion = latents_analysis.view(-1, latent_dim)\n",
    "\n",
    "# Calculate basic statistics\n",
    "mean = flattened_motion.mean(dim=0)\n",
    "std = flattened_motion.std(dim=0)\n",
    "min_vals = flattened_motion.min(dim=0).values\n",
    "max_vals = flattened_motion.max(dim=0).values\n",
    "\n",
    "# Calculate absolute magnitude statistics\n",
    "abs_flattened_motion = flattened_motion.abs()\n",
    "abs_mean = abs_flattened_motion.mean(dim=0)\n",
    "abs_std = abs_flattened_motion.std(dim=0)\n",
    "abs_min_vals = abs_flattened_motion.min(dim=0).values\n",
    "abs_max_vals = abs_flattened_motion.max(dim=0).values\n",
    "\n",
    "# Create a summary dataframe\n",
    "summary = pd.DataFrame({\n",
    "    'Mean': mean.numpy(),\n",
    "    'Std': std.numpy(),\n",
    "    'Min': min_vals.numpy(),\n",
    "    'Max': max_vals.numpy(),\n",
    "    'Abs Mean': abs_mean.numpy(),\n",
    "    'Abs Std': abs_std.numpy(),\n",
    "    'Abs Min': abs_min_vals.numpy(),\n",
    "    'Abs Max': abs_max_vals.numpy()\n",
    "})\n",
    "\n",
    "# Set display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Print the full summary\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming abs_std is already defined\n",
    "# abs_std = torch.randn(70)  # Example tensor, replace with your actual tensor\n",
    "\n",
    "# Define the bins in logarithmic scale from 1e-1 to 1e-8\n",
    "bins = np.logspace(-1, -8, num=8)\n",
    "\n",
    "# Digitize the abs_std values into the defined bins\n",
    "bin_indices = np.digitize(abs_mean.numpy(), bins)\n",
    "\n",
    "# Create a list to store indices for each bin\n",
    "bin_index_lists = [[] for _ in range(len(bins) + 1)]\n",
    "\n",
    "# Populate the bin_index_lists\n",
    "for idx, bin_idx in enumerate(bin_indices):\n",
    "    bin_index_lists[bin_idx].append(idx)\n",
    "\n",
    "# Print the bin counts and indices\n",
    "bin_counts = np.bincount(bin_indices, minlength=len(bins) + 1)\n",
    "for i, (count, indices) in enumerate(zip(bin_counts, bin_index_lists)):\n",
    "    if i == 0:\n",
    "        print(f\"< {bins[0]:.1e}: {count}\")\n",
    "    elif i == len(bins):\n",
    "        print(f\">= {bins[-1]:.1e}: {count}\")\n",
    "    else:\n",
    "        print(f\"{bins[i-1]:.1e} - {bins[i]:.1e}: {count}\")\n",
    "    print(f\"Indices: {indices}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot histograms for each feature\n",
    "# fig, axes = plt.subplots( 10, latent_dim // 10 + int(latent_dim % 10!=0), figsize=(20, 14))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# for i in tqdm(range(latent_dim), desc=\"Plotting histograms\"):\n",
    "#     sns.histplot(flattened_motion[:, i].numpy(), ax=axes[i], kde=True)\n",
    "#     axes[i].set_title(f'Feature {i}')\n",
    "#     axes[i].set_xlabel('')\n",
    "#     axes[i].set_ylabel('')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Plot box plots for each feature\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(data=flattened_motion[:, :latent_dim].numpy())\n",
    "plt.title('Box Plot of Motion Features')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = I_d_lst.shape[0]\n",
    "for i in (range(n_frames)):\n",
    "    # collect s, R, δ and t for inference\n",
    "    I_i = I_d_lst[i]\n",
    "    x_i_info = get_kp_info(I_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = I_d_lst.shape[0]\n",
    "template_dct = {\n",
    "    'n_frames': n_frames,\n",
    "    'output_fps': 25,\n",
    "    'motion': [],\n",
    "    'c_d_eyes_lst': [],\n",
    "    'c_d_lip_lst': [],\n",
    "    'x_i_info_lst': [],\n",
    "}\n",
    "\n",
    "for i in (range(n_frames)):\n",
    "    # collect s, R, δ and t for inference\n",
    "    I_i = I_d_lst[i]\n",
    "    x_i_info = get_kp_info(I_i)\n",
    "    R_i = get_rotation_matrix(x_i_info['pitch'], x_i_info['yaw'], x_i_info['roll'])\n",
    "\n",
    "    item_dct = {\n",
    "        'scale': x_i_info['scale'].cpu().numpy().astype(np.float32),\n",
    "        'R': R_i.cpu().numpy().astype(np.float32),\n",
    "        'exp': x_i_info['exp'].cpu().numpy().astype(np.float32),\n",
    "        't': x_i_info['t'].cpu().numpy().astype(np.float32),\n",
    "    }\n",
    "\n",
    "    template_dct['motion'].append(item_dct)\n",
    "\n",
    "    # c_eyes = c_d_eyes_lst[i].astype(np.float32)\n",
    "    # template_dct['c_d_eyes_lst'].append(c_eyes)\n",
    "\n",
    "    # c_lip = c_d_lip_lst[i].astype(np.float32)\n",
    "    # template_dct['c_d_lip_lst'].append(c_lip)\n",
    "\n",
    "    template_dct['x_i_info_lst'].append(x_i_info)\n",
    "    print(f'frame {i} done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frontalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = template_dct['motion'][0]['R']\n",
    "# exp = template_dct['motion'][0]['exp']\n",
    "# t = template_dct['motion'][0]['t']\n",
    "# scale = template_dct['motion'][0]['scale']\n",
    "# # print dims\n",
    "# print(R.shape, exp.shape, t.shape, scale.shape)\n",
    "# # print flatten dims\n",
    "# print(R.flatten().shape, exp.flatten().shape, t.flatten().shape, scale.flatten().shape)\n",
    "# # print range\n",
    "# print(R.min(), R.max(), exp.min(), exp.max(), t.min(), t.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(n_frames):\n",
    "#     R = template_dct['motion'][i]['R']\n",
    "#     exp = template_dct['motion'][i]['exp']\n",
    "#     t = template_dct['motion'][i]['t']\n",
    "#     scale = template_dct['motion'][i]['scale']\n",
    "#     info = template_dct['x_i_info_lst'][i]\n",
    "#     roll, pitch, yaw = info['roll'], info['pitch'], info['yaw']\n",
    "\n",
    "#     new_R = get_rotation_matrix(pitch, yaw, roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def angular_distance(pose1, pose2):\n",
    "    diff = torch.abs(pose1 - pose2)\n",
    "    diff = torch.min(diff, 2*torch.pi - diff)\n",
    "    return torch.norm(diff)\n",
    "\n",
    "def find_dominant_pose(poses):\n",
    "    N = poses.shape[0]\n",
    "    total_distances = torch.zeros(N, device=poses.device)\n",
    "    for i in range(N):\n",
    "        distances = angular_distance(poses[i].unsqueeze(0), poses)\n",
    "        total_distances[i] = torch.sum(distances)\n",
    "    min_distance_index = torch.argmin(total_distances)\n",
    "    return poses[min_distance_index], min_distance_index\n",
    "\n",
    "# Prepare data\n",
    "n_frames = len(template_dct['motion'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Collect all poses and t values\n",
    "all_poses = torch.zeros(n_frames, 3, device=device)\n",
    "all_t = torch.zeros(n_frames, 3, device=device)\n",
    "\n",
    "for i in range(n_frames):\n",
    "    info = template_dct['x_i_info_lst'][i]\n",
    "    roll, pitch, yaw = info['roll'], info['pitch'], info['yaw']\n",
    "    all_poses[i] = torch.tensor([roll, pitch, yaw], device=device).squeeze()\n",
    "    all_t[i] = torch.tensor(template_dct['motion'][i]['t'], device=device)\n",
    "\n",
    "# Find dominant pose\n",
    "dominant_pose, _ = find_dominant_pose(all_poses)\n",
    "\n",
    "# Find median t\n",
    "median_t = torch.median(all_t, dim=0).values\n",
    "\n",
    "# Subtract dominant pose and median t from the sequence\n",
    "for i in range(n_frames):\n",
    "    # Update pose\n",
    "    template_dct['x_i_info_lst'][i]['roll'] = (all_poses[i, 0]  - 1 * dominant_pose[0]).unsqueeze(0)\n",
    "    template_dct['x_i_info_lst'][i]['pitch'] = (all_poses[i, 1] - 1 * dominant_pose[1]).unsqueeze(0)\n",
    "    template_dct['x_i_info_lst'][i]['yaw'] = (all_poses[i, 2]   - 1 * dominant_pose[2]).unsqueeze(0)\n",
    "\n",
    "    # Update t\n",
    "    template_dct['motion'][i]['t'] = (all_t[i] - median_t).cpu().numpy()\n",
    "\n",
    "    # Recalculate R with the updated pose\n",
    "    new_R = get_rotation_matrix(\n",
    "        template_dct['x_i_info_lst'][i]['pitch'],\n",
    "        template_dct['x_i_info_lst'][i]['yaw'],\n",
    "        template_dct['x_i_info_lst'][i]['roll']\n",
    "    )\n",
    "    template_dct['motion'][i]['R'] = new_R.cpu().numpy()\n",
    "\n",
    "print(f\"Dominant pose (roll, pitch, yaw): {dominant_pose.cpu().numpy()}\")\n",
    "print(f\"Median t: {median_t.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Speed test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_source(img: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    img: HxWx3, uint8, 256x256\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    x = img.copy()\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        x = x[np.newaxis].astype(np.float32) / 255.  # HxWx3 -> 1xHxWx3, normalized to 0~1\n",
    "    elif x.ndim == 4:\n",
    "        x = x.astype(np.float32) / 255.  # BxHxWx3, normalized to 0~1\n",
    "    else:\n",
    "        raise ValueError(f'img ndim should be 3 or 4: {x.ndim}')\n",
    "    x = np.clip(x, 0, 1)  # clip to 0~1\n",
    "    x = torch.from_numpy(x).permute(0, 3, 1, 2)  # 1xHxWx3 -> 1x3xHxW\n",
    "    x = x.to(device)\n",
    "    return x\n",
    "\n",
    "def warp_decode(feature_3d: torch.Tensor, kp_source: torch.Tensor, kp_driving: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the image after the warping of the implicit keypoints\n",
    "    feature_3d: Bx32x16x64x64, feature volume\n",
    "    kp_source: BxNx3\n",
    "    kp_driving: BxNx3\n",
    "    \"\"\"\n",
    "    # The line 18 in Algorithm 1: D(W(f_s; x_s, x′_d,i)）\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        # get decoder input\n",
    "        ret_dct = warping_module(feature_3d, kp_source=kp_source, kp_driving=kp_driving)\n",
    "        # decode\n",
    "        ret_dct['out'] = spade_generator(feature=ret_dct['out'])\n",
    "\n",
    "    return ret_dct\n",
    "\n",
    "def extract_feature_3d( x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the appearance feature of the image by F\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        feature_3d = appearance_feature_extractor(x)\n",
    "\n",
    "    return feature_3d.float()\n",
    "\n",
    "\n",
    "\n",
    "def parse_output(out: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\" construct the output as standard\n",
    "    return: 1xHxWx3, uint8\n",
    "    \"\"\"\n",
    "    out = np.transpose(out.data.cpu().numpy(), [0, 2, 3, 1])  # 1x3xHxW -> 1xHxWx3\n",
    "    out = np.clip(out, 0, 1)  # clip to 0~1\n",
    "    out = np.clip(out * 255, 0, 255).astype(np.uint8)  # 0~1 -> 0~255\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize source image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/mnt/c/Users/mjh/Downloads/live_in/t4.jpg'\n",
    "img_rgb = load_image_rgb(input_path)\n",
    "source_rgb_lst = [img_rgb]\n",
    "\n",
    "source_lmk = cropper.calc_lmk_from_cropped_image(source_rgb_lst[0])\n",
    "img_crop_256x256 = cv2.resize(source_rgb_lst[0], (256, 256))  # force to resize to 256x256\n",
    "\n",
    "I_s = prepare_source(img_crop_256x256)\n",
    "x_s_info = get_kp_info(I_s)\n",
    "x_c_s = x_s_info['kp']\n",
    "x_s = transform_keypoint(x_s_info)\n",
    "f_s = extract_feature_3d(I_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single frame retarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = template_dct['motion'][0]['R']\n",
    "# exp = template_dct['motion'][0]['exp']\n",
    "# t = template_dct['motion'][0]['t']\n",
    "# scale = template_dct['motion'][0]['scale']\n",
    "\n",
    "# scale_tensor = torch.tensor(scale, device=device)\n",
    "# R_tensor = torch.tensor(R, device=device)\n",
    "# exp_tensor = torch.tensor(exp, device=device)\n",
    "# t_tensor = torch.tensor(t, device=device)\n",
    "# print(scale_tensor.shape, R_tensor.shape, exp_tensor.shape, t_tensor.shape)\n",
    "\n",
    "# start = time.time()\n",
    "# x_d_i_new = scale_tensor * (x_c_s @ R_tensor + exp_tensor) + t_tensor\n",
    "\n",
    "# # x_d_i_new = scale * (x_c_s @ R + exp) + t\n",
    "# out = warp_decode(f_s, x_s, x_d_i_new)\n",
    "# # print(out)\n",
    "# # I_p_i = parse_output(out['out'])[0]\n",
    "# end_time = time.time() - start\n",
    "# print(f'warp_decode time: {end_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large chunk of frames generator. Performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "# Initialize variables\n",
    "frame_index = 0\n",
    "total_frames = len(template_dct['motion'])\n",
    "\n",
    "# Create a window for display\n",
    "# cv2.namedWindow('Processed Frame', cv2.WINDOW_NORMAL)\n",
    "# cv2.resizeWindow('Processed Frame', 512, 512)  # Adjust size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "# Initialize variables\n",
    "frame_index = 0\n",
    "total_frames = len(template_dct['motion'])\n",
    "\n",
    "# Create a window for display\n",
    "cv2.namedWindow('Processed Frame', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Processed Frame', 512, 512)  # Adjust size as needed\n",
    "\n",
    "while frame_index < total_frames:\n",
    "    # Get motion data for the current frame\n",
    "    R = template_dct['motion'][frame_index]['R']\n",
    "    exp = template_dct['motion'][frame_index]['exp']\n",
    "    t = template_dct['motion'][frame_index]['t']\n",
    "    scale = template_dct['motion'][frame_index]['scale']\n",
    "\n",
    "    # Convert to tensors\n",
    "    scale_tensor = torch.tensor(scale, device=device)\n",
    "    R_tensor = torch.tensor(R, device=device)\n",
    "    exp_tensor = torch.tensor(exp, device=device)\n",
    "    t_tensor = torch.tensor(t, device=device)\n",
    "\n",
    "    # Process the frame\n",
    "    x_d_i_new = scale_tensor * (x_c_s @ R_tensor + exp_tensor) + t_tensor\n",
    "    out = warp_decode(f_s, x_s, x_d_i_new)\n",
    "\n",
    "    # Convert tensor to numpy array and rescale to 0-255 range\n",
    "    img_np = (out['out'][0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    # Convert from RGB to BGR for cv2\n",
    "    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Processed Frame', img_bgr)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Processed frame {frame_index+1}/{total_frames}\")\n",
    "\n",
    "    # Wait for a short time and check for 'q' key to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_index += 1\n",
    "\n",
    "    # Optional: add a small delay to make the display more visible\n",
    "    time.sleep(0.01)  # Adjust as needed\n",
    "\n",
    "# Clean up\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "live_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
