{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full pipeline inference\n",
    "1. load target image to drive\n",
    "2. load audio\n",
    "3. inference dit with audio to get latent\n",
    "4. drive target with latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load target image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Prepare full live pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import contextlib\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "import tyro\n",
    "import subprocess\n",
    "from rich.progress import track\n",
    "import torchvision\n",
    "import cv2\n",
    "import threading\n",
    "import queue\n",
    "import torchvision.transforms as transforms\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import imageio\n",
    "\n",
    "from src.config.argument_config import ArgumentConfig\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.config.crop_config import CropConfig\n",
    "\n",
    "def partial_fields(target_class, kwargs):\n",
    "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
    "\n",
    "args = ArgumentConfig()\n",
    "inference_cfg = partial_fields(InferenceConfig, args.__dict__)\n",
    "crop_cfg = partial_fields(CropConfig, args.__dict__)\n",
    "# print(\"inference_cfg: \", inference_cfg)\n",
    "# print(\"crop_cfg: \", crop_cfg)\n",
    "device = 'cuda'\n",
    "print(\"Compile complete\")\n",
    "\n",
    "'''\n",
    "Common modules\n",
    "'''\n",
    "\n",
    "from src.utils.helper import load_model, concat_feat\n",
    "from src.utils.camera import headpose_pred_to_degree, get_rotation_matrix\n",
    "from src.utils.retargeting_utils import calc_eye_close_ratio, calc_lip_close_ratio\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.utils.cropper import Cropper\n",
    "from src.utils.camera import get_rotation_matrix\n",
    "from src.utils.video import images2video, concat_frames, get_fps, add_audio_to_video, has_audio_stream\n",
    "from src.utils.crop import _transform_img, prepare_paste_back, paste_back\n",
    "from src.utils.io import load_image_rgb, load_video, resize_to_limit, dump, load\n",
    "from src.utils.helper import mkdir, basename, dct2device, is_video, is_template, remove_suffix, is_image\n",
    "from src.utils.filter import smooth\n",
    "\n",
    "\n",
    "'''\n",
    "Util functions\n",
    "'''\n",
    "\n",
    "def calculate_distance_ratio(lmk: np.ndarray, idx1: int, idx2: int, idx3: int, idx4: int, eps: float = 1e-6) -> np.ndarray:\n",
    "    return (np.linalg.norm(lmk[:, idx1] - lmk[:, idx2], axis=1, keepdims=True) /\n",
    "            (np.linalg.norm(lmk[:, idx3] - lmk[:, idx4], axis=1, keepdims=True) + eps))\n",
    "\n",
    "\n",
    "def calc_eye_close_ratio(lmk: np.ndarray, target_eye_ratio: np.ndarray = None) -> np.ndarray:\n",
    "    lefteye_close_ratio = calculate_distance_ratio(lmk, 6, 18, 0, 12)\n",
    "    righteye_close_ratio = calculate_distance_ratio(lmk, 30, 42, 24, 36)\n",
    "    if target_eye_ratio is not None:\n",
    "        return np.concatenate([lefteye_close_ratio, righteye_close_ratio, target_eye_ratio], axis=1)\n",
    "    else:\n",
    "        return np.concatenate([lefteye_close_ratio, righteye_close_ratio], axis=1)\n",
    "\n",
    "\n",
    "def calc_lip_close_ratio(lmk: np.ndarray) -> np.ndarray:\n",
    "    return calculate_distance_ratio(lmk, 90, 102, 48, 66)\n",
    "\n",
    "def calc_ratio(lmk_lst):\n",
    "    input_eye_ratio_lst = []\n",
    "    input_lip_ratio_lst = []\n",
    "    for lmk in lmk_lst:\n",
    "        # for eyes retargeting\n",
    "        input_eye_ratio_lst.append(calc_eye_close_ratio(lmk[None]))\n",
    "        # for lip retargeting\n",
    "        input_lip_ratio_lst.append(calc_lip_close_ratio(lmk[None]))\n",
    "    return input_eye_ratio_lst, input_lip_ratio_lst\n",
    "\n",
    "def prepare_videos_(imgs, device):\n",
    "    \"\"\" construct the input as standard\n",
    "    imgs: NxHxWx3, uint8\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, list):\n",
    "        _imgs = np.array(imgs)\n",
    "    elif isinstance(imgs, np.ndarray):\n",
    "        _imgs = imgs\n",
    "    else:\n",
    "        raise ValueError(f'imgs type error: {type(imgs)}')\n",
    "\n",
    "    # y = _imgs.astype(np.float32) / 255.\n",
    "    y = _imgs\n",
    "    y = torch.from_numpy(y).permute(0, 3, 1, 2)  # NxHxWx3 -> Nx3xHxW\n",
    "    y = y.to(device)\n",
    "    y = y / 255.\n",
    "    y = torch.clamp(y, 0, 1)\n",
    "\n",
    "    return y\n",
    "\n",
    "def get_kp_info(x: torch.Tensor, **kwargs) -> dict:\n",
    "    \"\"\" get the implicit keypoint information\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    flag_refine_info: whether to trandform the pose to degrees and the dimention of the reshape\n",
    "    return: A dict contains keys: 'pitch', 'yaw', 'roll', 't', 'exp', 'scale', 'kp'\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        kp_info = motion_extractor(x)\n",
    "\n",
    "        if inference_cfg.flag_use_half_precision:\n",
    "            # float the dict\n",
    "            for k, v in kp_info.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    kp_info[k] = v.float()\n",
    "\n",
    "    flag_refine_info: bool = kwargs.get('flag_refine_info', True)\n",
    "    if flag_refine_info:\n",
    "        bs = kp_info['kp'].shape[0]\n",
    "        kp_info['pitch'] = headpose_pred_to_degree(kp_info['pitch'])[:, None]  # Bx1\n",
    "        kp_info['yaw'] = headpose_pred_to_degree(kp_info['yaw'])[:, None]  # Bx1\n",
    "        kp_info['roll'] = headpose_pred_to_degree(kp_info['roll'])[:, None]  # Bx1\n",
    "        kp_info['kp'] = kp_info['kp'].reshape(bs, -1, 3)  # BxNx3\n",
    "        kp_info['exp'] = kp_info['exp'].reshape(bs, -1, 3)  # BxNx3\n",
    "\n",
    "    return kp_info\n",
    "\n",
    "def read_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (256, 256))  # Resize to 256x256\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return video_path, frames\n",
    "\n",
    "def read_multiple_videos(video_paths, num_threads=4):\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        results = list(executor.map(read_video_frames, video_paths))\n",
    "    return results\n",
    "\n",
    "def euler_to_quaternion(pitch, yaw, roll):\n",
    "    cy = torch.cos(yaw * 0.5)\n",
    "    sy = torch.sin(yaw * 0.5)\n",
    "    cp = torch.cos(pitch * 0.5)\n",
    "    sp = torch.sin(pitch * 0.5)\n",
    "    cr = torch.cos(roll * 0.5)\n",
    "    sr = torch.sin(roll * 0.5)\n",
    "\n",
    "    w = cr * cp * cy + sr * sp * sy\n",
    "    x = sr * cp * cy - cr * sp * sy\n",
    "    y = cr * sp * cy + sr * cp * sy\n",
    "    z = cr * cp * sy - sr * sp * cy\n",
    "\n",
    "    return torch.stack([w, x, y, z], dim=-1)\n",
    "\n",
    "def quaternion_to_euler(q):\n",
    "    \"\"\"\n",
    "    Convert quaternion to Euler angles (pitch, yaw, roll) in radians.\n",
    "    q: torch.Tensor of shape (..., 4) representing quaternions (w, x, y, z)\n",
    "    Returns: tuple of (pitch, yaw, roll) as torch.Tensor\n",
    "    \"\"\"\n",
    "    # Extract the values from q\n",
    "    w, x, y, z = q[..., 0], q[..., 1], q[..., 2], q[..., 3]\n",
    "\n",
    "    # Roll (x-axis rotation)\n",
    "    sinr_cosp = 2 * (w * x + y * z)\n",
    "    cosr_cosp = 1 - 2 * (x * x + y * y)\n",
    "    roll = torch.atan2(sinr_cosp, cosr_cosp)\n",
    "\n",
    "    # Pitch (y-axis rotation)\n",
    "    sinp = 2 * (w * y - z * x)\n",
    "    pitch = torch.where(\n",
    "        torch.abs(sinp) >= 1,\n",
    "        torch.sign(sinp) * torch.pi / 2,\n",
    "        torch.asin(sinp)\n",
    "    )\n",
    "\n",
    "    # Yaw (z-axis rotation)\n",
    "    siny_cosp = 2 * (w * z + x * y)\n",
    "    cosy_cosp = 1 - 2 * (y * y + z * z)\n",
    "    yaw = torch.atan2(siny_cosp, cosy_cosp)\n",
    "\n",
    "    return pitch, yaw, roll\n",
    "\n",
    "def quaternion_to_euler_degrees(q):\n",
    "    \"\"\"\n",
    "    Convert quaternion to Euler angles (pitch, yaw, roll) in degrees.\n",
    "    q: torch.Tensor of shape (..., 4) representing quaternions (w, x, y, z)\n",
    "    Returns: tuple of (pitch, yaw, roll) as torch.Tensor in degrees\n",
    "    \"\"\"\n",
    "    pitch, yaw, roll = quaternion_to_euler(q)\n",
    "    return torch.rad2deg(pitch), torch.rad2deg(yaw), torch.rad2deg(roll)\n",
    "\n",
    "\n",
    "'''\n",
    "Loading source related modules\n",
    "'''\n",
    "def prepare_source(img: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    img: HxWx3, uint8, 256x256\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    x = img.copy()\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        x = x[np.newaxis].astype(np.float32) / 255.  # HxWx3 -> 1xHxWx3, normalized to 0~1\n",
    "    elif x.ndim == 4:\n",
    "        x = x.astype(np.float32) / 255.  # BxHxWx3, normalized to 0~1\n",
    "    else:\n",
    "        raise ValueError(f'img ndim should be 3 or 4: {x.ndim}')\n",
    "    x = np.clip(x, 0, 1)  # clip to 0~1\n",
    "    x = torch.from_numpy(x).permute(0, 3, 1, 2)  # 1xHxWx3 -> 1x3xHxW\n",
    "    x = x.to(device)\n",
    "    return x\n",
    "\n",
    "def warp_decode(feature_3d: torch.Tensor, kp_source: torch.Tensor, kp_driving: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the image after the warping of the implicit keypoints\n",
    "    feature_3d: Bx32x16x64x64, feature volume\n",
    "    kp_source: BxNx3\n",
    "    kp_driving: BxNx3\n",
    "    \"\"\"\n",
    "    # The line 18 in Algorithm 1: D(W(f_s; x_s, x′_d,i)）\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        # get decoder input\n",
    "        ret_dct = warping_module(feature_3d, kp_source=kp_source, kp_driving=kp_driving)\n",
    "        # decode\n",
    "        ret_dct['out'] = spade_generator(feature=ret_dct['out'])\n",
    "\n",
    "    return ret_dct\n",
    "\n",
    "def extract_feature_3d( x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the appearance feature of the image by F\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        feature_3d = appearance_feature_extractor(x)\n",
    "\n",
    "    return feature_3d.float()\n",
    "\n",
    "def transform_keypoint(kp_info: dict):\n",
    "    \"\"\"\n",
    "    transform the implicit keypoints with the pose, shift, and expression deformation\n",
    "    kp: BxNx3\n",
    "    \"\"\"\n",
    "    kp = kp_info['kp']    # (bs, k, 3)\n",
    "    pitch, yaw, roll = kp_info['pitch'], kp_info['yaw'], kp_info['roll']\n",
    "\n",
    "    t, exp = kp_info['t'], kp_info['exp']\n",
    "    scale = kp_info['scale']\n",
    "\n",
    "    pitch = headpose_pred_to_degree(pitch)\n",
    "    yaw = headpose_pred_to_degree(yaw)\n",
    "    roll = headpose_pred_to_degree(roll)\n",
    "\n",
    "    bs = kp.shape[0]\n",
    "    if kp.ndim == 2:\n",
    "        num_kp = kp.shape[1] // 3  # Bx(num_kpx3)\n",
    "    else:\n",
    "        num_kp = kp.shape[1]  # Bxnum_kpx3\n",
    "\n",
    "    rot_mat = get_rotation_matrix(pitch, yaw, roll)    # (bs, 3, 3)\n",
    "\n",
    "    # Eqn.2: s * (R * x_c,s + exp) + t\n",
    "    kp_transformed = kp.view(bs, num_kp, 3) @ rot_mat + exp.view(bs, num_kp, 3)\n",
    "    kp_transformed *= scale[..., None]  # (bs, k, 3) * (bs, 1, 1) = (bs, k, 3)\n",
    "    kp_transformed[:, :, 0:2] += t[:, None, 0:2]  # remove z, only apply tx ty\n",
    "\n",
    "    return kp_transformed\n",
    "\n",
    "def parse_output(out: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\" construct the output as standard\n",
    "    return: 1xHxWx3, uint8\n",
    "    \"\"\"\n",
    "    out = np.transpose(out.data.cpu().numpy(), [0, 2, 3, 1])  # 1x3xHxW -> 1xHxWx3\n",
    "    out = np.clip(out, 0, 1)  # clip to 0~1\n",
    "    out = np.clip(out * 255, 0, 255).astype(np.uint8)  # 0~1 -> 0~255\n",
    "\n",
    "    return out\n",
    "'''\n",
    "Main module for inference\n",
    "'''\n",
    "model_config = yaml.load(open(inference_cfg.models_config, 'r'), Loader=yaml.SafeLoader)\n",
    "# init F\n",
    "appearance_feature_extractor = load_model(inference_cfg.checkpoint_F, model_config, device, 'appearance_feature_extractor')\n",
    "# init M\n",
    "motion_extractor = load_model(inference_cfg.checkpoint_M, model_config, device, 'motion_extractor')\n",
    "# init W\n",
    "warping_module = load_model(inference_cfg.checkpoint_W, model_config, device, 'warping_module')\n",
    "# init G\n",
    "spade_generator = load_model(inference_cfg.checkpoint_G, model_config, device, 'spade_generator')\n",
    "# init S and R\n",
    "if inference_cfg.checkpoint_S is not None and os.path.exists(inference_cfg.checkpoint_S):\n",
    "    stitching_retargeting_module = load_model(inference_cfg.checkpoint_S, model_config, device, 'stitching_retargeting_module')\n",
    "else:\n",
    "    stitching_retargeting_module = None\n",
    "\n",
    "cropper = Cropper(crop_cfg=crop_cfg, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Load single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/mnt/c/Users/mjh/Downloads/live_in/t1.jpg'\n",
    "img_rgb = load_image_rgb(input_path)\n",
    "source_rgb_lst = [img_rgb]\n",
    "\n",
    "source_lmk = cropper.calc_lmk_from_cropped_image(source_rgb_lst[0])\n",
    "img_crop_256x256 = cv2.resize(source_rgb_lst[0], (256, 256))  # force to resize to 256x256\n",
    "\n",
    "I_s = prepare_source(img_crop_256x256)\n",
    "x_s_info = get_kp_info(I_s)\n",
    "x_c_s = x_s_info['kp']\n",
    "x_s = transform_keypoint(x_s_info)\n",
    "f_s = extract_feature_3d(I_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Load audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Prepare audio pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\"\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "FRAME_RATE = 25\n",
    "SECTION_LENGTH = 3\n",
    "OVERLAP = 10\n",
    "\n",
    "DB_ROOT = 'vox2-audio-tx'\n",
    "LOG = 'log'\n",
    "AUDIO = 'audio/audio'\n",
    "OUTPUT_DIR = 'audio_encoder_output'\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "# Move model and processor to global scope\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(device)\n",
    "wav2vec_processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def read_multiple_audios(paths, num_threads=12):\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        results = list(executor.map(load_and_process_audio, paths))\n",
    "    return results\n",
    "\n",
    "\n",
    "def read_json_and_form_paths(data,id_key):\n",
    "    filenames=[]\n",
    "    file_paths = []\n",
    "\n",
    "    # Iterate through the nested structure\n",
    "    for id_key, id_value in data.items():\n",
    "        os.makedirs(os.path.join(DB_ROOT,OUTPUT_DIR,id_key), exist_ok=True)\n",
    "        for url_key, url_value in id_value.items():\n",
    "            for clip_id in url_value.keys():\n",
    "                # Form the file path\n",
    "                file_path = os.path.join(DB_ROOT,AUDIO,id_key, url_key, clip_id.replace('.txt', '.wav'))\n",
    "                file_name = os.path.join(DB_ROOT,OUTPUT_DIR,id_key, url_key+'+'+clip_id.replace('.txt', ''))\n",
    "                filenames.append(file_name)\n",
    "                file_paths.append(file_path)\n",
    "\n",
    "    return file_paths, filenames\n",
    "\n",
    "def load_and_process_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    original_sample_rate = sample_rate\n",
    "\n",
    "    if sample_rate != TARGET_SAMPLE_RATE:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, TARGET_SAMPLE_RATE)\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    print(file_path,\" waveform.shape \",waveform.shape)\n",
    "\n",
    "    # Calculate section length and overlap in samples\n",
    "    section_samples = SECTION_LENGTH * 16027\n",
    "    overlap_samples = int(OVERLAP / FRAME_RATE * TARGET_SAMPLE_RATE)\n",
    "    print('section_samples',section_samples,'overlap_samples',overlap_samples)\n",
    "\n",
    "    # Pad if shorter than 3 seconds\n",
    "    if waveform.shape[1] < section_samples:\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, section_samples - waveform.shape[1]))\n",
    "        return [waveform.squeeze(0)], original_sample_rate\n",
    "\n",
    "    # Split into sections with overlap\n",
    "    sections = []\n",
    "    start = 0\n",
    "\n",
    "    print('starting to segment', file_path)\n",
    "    while start < waveform.shape[1]:\n",
    "        end = start + section_samples\n",
    "        if end >= waveform.shape[1]:\n",
    "            tmp=waveform[:, start:min(end, waveform.shape[1])]\n",
    "            tmp = torch.nn.functional.pad(tmp, (0, section_samples - tmp.shape[1]))\n",
    "            sections.append(tmp.squeeze(0))\n",
    "            print(tmp.shape)\n",
    "            break\n",
    "        else:\n",
    "            sections.append(waveform[:, start:min(end, waveform.shape[1])].squeeze(0))\n",
    "\n",
    "        start = int(end - overlap_samples)\n",
    "\n",
    "\n",
    "    return file_path, sections\n",
    "\n",
    "def inference_process_wav_file(path):\n",
    "    audio_path, segments = load_and_process_audio(path)\n",
    "    segments = np.array(segments)\n",
    "\n",
    "    inputs = wav2vec_processor(segments, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\", padding=True).input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = wav2vec_model(inputs)\n",
    "        latent = outputs.last_hidden_state\n",
    "\n",
    "        seq_length = latent.shape[1]\n",
    "        new_seq_length = int(seq_length * (FRAME_RATE / 50))\n",
    "\n",
    "        latent_features_interpolated = F.interpolate(latent.transpose(1,2),\n",
    "                                                     size=new_seq_length,\n",
    "                                                     mode='linear',\n",
    "                                                     align_corners=True).transpose(1,2)\n",
    "    return latent_features_interpolated\n",
    "\n",
    "\n",
    "def process_wav_file(paths, output_paths, uid):\n",
    "    device = torch.device(f\"cuda\")\n",
    "\n",
    "    model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(device)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    read_2_gpu_batch_size = 2048\n",
    "    gpu_batch_size = BATCH_SIZE\n",
    "    process_queue = torch.Tensor().to(device)\n",
    "\n",
    "    audio_segments = read_multiple_audios(paths, num_threads=4)\n",
    "    all_segments = []\n",
    "    total_segments = 0\n",
    "\n",
    "    audio_lengths = []\n",
    "    output_fns = []\n",
    "\n",
    "    for (audio_path, segments), output_fn in zip(audio_segments,output_paths):\n",
    "        all_segments.extend(segments)\n",
    "        segment_count = len(segments)\n",
    "        total_segments += segment_count\n",
    "        audio_lengths.append(segment_count)\n",
    "\n",
    "        output_fns.append(output_fn)\n",
    "\n",
    "    all_segments = np.array(all_segments)\n",
    "    print(all_segments.size)\n",
    "\n",
    "    read_data_2_gpu_pointer = 0\n",
    "    pbar = tqdm(total=total_segments, desc=f\"Processing {uid}\")\n",
    "\n",
    "    while read_data_2_gpu_pointer < total_segments:\n",
    "        current_batch_size = min(read_2_gpu_batch_size, total_segments - read_data_2_gpu_pointer)\n",
    "\n",
    "        batch_input = all_segments[read_data_2_gpu_pointer:read_data_2_gpu_pointer + current_batch_size]\n",
    "\n",
    "        mini_batch_start = 0\n",
    "        all_info = []\n",
    "        while mini_batch_start < batch_input.shape[0]:\n",
    "            mini_batch_end = min(mini_batch_start + gpu_batch_size, batch_input.shape[0])\n",
    "            mini_batch = batch_input[mini_batch_start:mini_batch_end]\n",
    "\n",
    "            inputs = processor(mini_batch, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\", padding=True).input_values.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            latent = outputs.last_hidden_state\n",
    "            print('latent',latent.shape)\n",
    "            seq_length = latent.shape[1]\n",
    "            new_seq_length = int(seq_length * (FRAME_RATE / 50))  # Assuming Wav2Vec2 outputs at ~50Hz\n",
    "\n",
    "            latent_features_interpolated = F.interpolate(latent.transpose(1,2),\n",
    "                                                            size=new_seq_length,\n",
    "                                                            mode='linear',\n",
    "                                                            align_corners=True).transpose(1,2)\n",
    "            print('latent_features_interpolated',latent_features_interpolated.shape)\n",
    "            all_info.append(latent_features_interpolated)\n",
    "\n",
    "            mini_batch_start = mini_batch_end\n",
    "        all_info_tensor = torch.cat(all_info, dim=0)\n",
    "\n",
    "        process_queue = torch.cat((process_queue, all_info_tensor), dim=0)\n",
    "\n",
    "        print(audio_lengths)\n",
    "        while len(output_fns) > 0 and len(process_queue) >= audio_lengths[0]:\n",
    "            current_output_fn = output_fns[0]\n",
    "            current_segment_count = audio_lengths[0]\n",
    "\n",
    "            audio_tensor = process_queue[:current_segment_count]\n",
    "            np.save(current_output_fn, audio_tensor.cpu().numpy())\n",
    "            print('save',current_output_fn)\n",
    "            process_queue = process_queue[current_segment_count:]\n",
    "            output_fns.pop(0)\n",
    "            audio_lengths.pop(0)\n",
    "\n",
    "\n",
    "        read_data_2_gpu_pointer += current_batch_size\n",
    "        pbar.update(current_batch_size)\n",
    "\n",
    "    pbar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Load pretrained latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_pair(audio_file, motion_file):\n",
    "    # Load audio file\n",
    "    audio_data = np.load(audio_file)\n",
    "\n",
    "    # Load and process motion file\n",
    "    motion_data = np.load(motion_file)\n",
    "    pad_length = (65 - (motion_data.shape[0] - 10) % 65) % 65\n",
    "    padded_data = np.pad(motion_data, ((0, pad_length), (0, 0)), mode='constant')\n",
    "\n",
    "    data_without_first_10 = padded_data[10:]\n",
    "    N = data_without_first_10.shape[0] // 65\n",
    "    reshaped_data = data_without_first_10[:N*65].reshape(N, 65, 133)\n",
    "    last_10 = reshaped_data[:, -10:, :]\n",
    "    prev_10 = np.concatenate([padded_data[:10][None, :, :], last_10[:-1]], axis=0)\n",
    "    final_windows = np.concatenate([prev_10, reshaped_data], axis=1)\n",
    "\n",
    "    # Ensure audio and motion data have the same number of frames.\n",
    "    # Prev lookup show 1 frame mismatch is common. In this case we only fix batch size mismatch\n",
    "    min_frames = min(audio_data.shape[0], final_windows.shape[0])\n",
    "    audio_data = audio_data[:min_frames]\n",
    "    final_windows = final_windows[:min_frames]\n",
    "\n",
    "    return audio_data, final_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Single input inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\n",
    "    \"motion\":[\n",
    "        '/mnt/e/data/live_latent/motion_latent/id00078/P0OU4bFhwCI+00227.npy',\n",
    "        '/mnt/e/data/live_latent/motion_latent/id00019/anX4gftNLoc+00140.npy',\n",
    "        '/mnt/e/data/live_latent/motion_latent/id00012/aE4Om0EEiuk+00117.npy',\n",
    "        '/mnt/e/data/live_latent/motion_latent/id09263/ARVWnF_NcCI+00002.npy',\n",
    "        # HDTF\n",
    "        '/mnt/e/data/diffposetalk_data/TFHP_raw/test_split/live_latent/TH_00226/001.npy',\n",
    "        '/mnt/e/data/diffposetalk_data/TFHP_raw/train_split/live_latent/TH_00028/000.npy',\n",
    "        '/mnt/e/data/diffposetalk_data/TFHP_raw/train_split/live_latent/TH_00212/000.npy',\n",
    "\n",
    "    ],\n",
    "    \"audio\":[\n",
    "        '/mnt/e/data/live_latent/audio_latent/id00078/P0OU4bFhwCI+00227.npy',\n",
    "        '/mnt/e/data/live_latent/audio_latent/id00019/anX4gftNLoc+00140.npy',\n",
    "        '/mnt/e/data/live_latent/audio_latent/id00012/aE4Om0EEiuk+00117.npy',\n",
    "        '/mnt/e/data/live_latent/audio_latent/id09263/ARVWnF_NcCI+00002.npy',\n",
    "        # HDTF\n",
    "        '/mnt/e/data/diffposetalk_data/TFHP_raw/test_split/audio_latent/TH_00226/001.npy',\n",
    "        '/mnt/e/data/diffposetalk_data/TFHP_raw/train_split/audio_latent/TH_00028/000.npy',\n",
    "        '/mnt/e/data/diffposetalk_data/TFHP_raw/train_split/audio_latent/TH_00212/000.npy',\n",
    "    ],\n",
    "    \"wav\":[\n",
    "        '/mnt/c/Users/mjh/Downloads/audio_id00078_P0OU4bFhwCI_00227.wav',\n",
    "        '/mnt/c/Users/mjh/Downloads/audio_id00019_anX4gftNLoc_00140.wav',\n",
    "        '/mnt/c/Users/mjh/Downloads/audio_id00012_aE4Om0EEiuk_00117.wav',\n",
    "        '/mnt/c/Users/mjh/Downloads/audio_id09263_ARVWnF_NcCI_00002.wav',\n",
    "        # HDTF\n",
    "        '/mnt/e/data/diffposetalk_data/TFHP_raw/audio/TH_00226/001.wav',\n",
    "        '/mnt/e/data/diffposetalk_data/TFHP_raw/audio/TH_00028/000.wav',\n",
    "        '/mnt/e/data/diffposetalk_data/TFHP_raw/audio/TH_00212/000.wav',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "index = 6\n",
    "motion_latent_npy = input_dict['motion'][index]\n",
    "audio_latent_path = input_dict['audio'][index]\n",
    "training_audio_example = input_dict['wav'][index]\n",
    "audio_latent, motion_latent_input = load_and_process_pair(audio_latent_path, motion_latent_npy)\n",
    "\n",
    "\n",
    "# valid_audio_example = '/mnt/c/Users/mjh/Downloads/l8.wav'\n",
    "used_audio_example = training_audio_example\n",
    "\n",
    "# audio_latent = inference_process_wav_file(used_audio_example)\n",
    "# audio_latent = np.load(audio_latent_path)\n",
    "# audio_latent = torch.from_numpy(audio_latent).to(device)\n",
    "# audio_latent = audio_latent[0].unsqueeze(0)\n",
    "audio_latent = torch.from_numpy(audio_latent).to(device)\n",
    "motion_latent_input = torch.from_numpy(motion_latent_input).to(device)\n",
    "print(audio_latent.shape, motion_latent_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Load DiT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.0 decide DiT type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_dit.inference import example_inference\n",
    "from audio_dit.dataset import process_motion_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_latent_input = audio_latent\n",
    "motion_latent_input = motion_latent_input\n",
    "mask_1 = [i for i in range(21)]\n",
    "motion_latent_processed, audio_latent_input, shape_in = process_motion_tensor(motion_latent_input, audio_latent_input, latent_type='exp', latent_mask_1=mask_1)\n",
    "weight_path = 'audio_dit/output/checkpoint_epoch_8000_vanilla_exp_1/model.pth'\n",
    "config_path = 'audio_dit/output/config.json'\n",
    "audio_seq = audio_latent_input[:, 10:, :]\n",
    "audio_prev = audio_latent_input[:, :10, :]\n",
    "shape_in = motion_latent_processed[:, -1, :]\n",
    "motion_prev = motion_latent_processed[:, :10, :]\n",
    "motion_gt = motion_latent_processed[:, 10:-1, :]\n",
    "\n",
    "# Print shapes of input tensors\n",
    "print(\"Audio input shape:\", audio_seq.shape)\n",
    "print(\"Audio previous shape:\", audio_prev.shape)\n",
    "print(\"Shape input shape:\", shape_in.shape)\n",
    "print(\"Motion previous shape:\", motion_prev.shape)\n",
    "print(\"Motion ground truth shape:\", motion_gt.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.exists(config_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Prepare DiT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_motion = example_inference(config_path, weight_path, audio_seq, shape_in, motion_prev, audio_prev,\n",
    "#                                      total_denoising_steps=5)\n",
    "# loss = F.mse_loss(generated_motion, motion_gt)\n",
    "# print(f\"MSE Loss between generated motion and ground truth: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 check shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_motion.shape, motion_prev.shape, motion_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if generated_motion.shape[1] != motion_gt.shape[1]:\n",
    "#     generated_motion = torch.cat([motion_prev, generated_motion], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE loss between generated_motion and motion_latent_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff = generated_motion - motion_gt\n",
    "# # Create masks for differences and ground truth with absolute values >= 1e-3\n",
    "# diff_mask = torch.abs(diff) >= 1e-3\n",
    "# gt_mask = torch.abs(motion_gt) < 1e-5\n",
    "# total_elements = torch.numel(diff)\n",
    "\n",
    "# # Count overlaps and non-overlaps\n",
    "# overlap_count = torch.sum(diff_mask & gt_mask).item()\n",
    "# diff_only_count = torch.sum(diff_mask & ~gt_mask).item()\n",
    "# gt_only_count = torch.sum(~diff_mask & gt_mask).item()\n",
    "\n",
    "# # Apply the mask to the diff tensor\n",
    "# large_diffs = diff * diff_mask\n",
    "\n",
    "# # Add the large differences to the generated_motion\n",
    "# generated_motion = generated_motion - large_diffs * 0.8\n",
    "\n",
    "# print(f\"Number of large differences (>= 1e-3) adjusted: {torch.sum(diff_mask).item()} out of {total_elements}\")\n",
    "# print(f\"Number of large values (>= 1e-3) in ground truth: {torch.sum(gt_mask).item()} out of {total_elements}\")\n",
    "# print(f\"Number of overlapping large values: {overlap_count}\")\n",
    "# print(f\"Number of large differences not in ground truth: {diff_only_count}\")\n",
    "# print(f\"Number of large ground truth values not in differences: {gt_only_count}\")\n",
    "# print(f\"Max difference after adjustment: {torch.max(torch.abs(generated_motion - motion_gt)).item():.6f}\")\n",
    "# print(f\"Min difference after adjustment: {torch.min(torch.abs(generated_motion - motion_gt)).item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(motion_gt.shape[1]):\n",
    "    print(f\"Frame {i}\")\n",
    "    print(\"Generated motion:\", generated_motion[0, i])\n",
    "    print(\"Ground truth motion:\", motion_gt[0, i])\n",
    "    print(\"Difference:\", generated_motion[0, i] - motion_gt[0, i])\n",
    "    print(\"Per frame MSE loss:\", F.mse_loss(generated_motion[0, i], motion_gt[0, i]).item())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Do Render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_s.shape, x_s.shape, x_c_s.shape)\n",
    "print(\"Input scale is \", x_s_info['scale'])\n",
    "\n",
    "\n",
    "I_s = prepare_source(img_crop_256x256)\n",
    "x_s_info = get_kp_info(I_s)\n",
    "x_c_s = x_s_info['kp']\n",
    "x_s = transform_keypoint(x_s_info)\n",
    "f_s = extract_feature_3d(I_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "t = x_s_info['t']\n",
    "pitch = x_s_info['pitch']\n",
    "yaw = x_s_info['yaw']\n",
    "roll = x_s_info['roll']\n",
    "# scale = motion_latent_input_local[j, 132:133]\n",
    "scale = x_s_info['scale']\n",
    "\n",
    "# Extract values from motion\n",
    "exp_identity = torch.zeros_like(x_s_info['exp'])\n",
    "exp_gen = generated_motion[0, 11, :].reshape(21, 3)\n",
    "# exp = x_s_info['exp']\n",
    "exp = exp_gen\n",
    "\n",
    "x_d_i = scale * (x_c_s @ get_rotation_matrix(pitch, yaw, roll) + exp) + t\n",
    "\n",
    "# Combine into x_d_i\n",
    "# x_d_i = motions[i, j].unsqueeze(0).reshape(-1, 21, 3)\n",
    "\n",
    "# Generate frame\n",
    "out = warp_decode(f_s, x_s, x_d_i)\n",
    "\n",
    "# Convert to numpy array\n",
    "frame_index = (out['out'][0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_identity = torch.zeros_like(x_s_info['exp']).squeeze()\n",
    "exp_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_gt = motion_gt.reshape(-1, 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import torch\n",
    "motion_gt = motion_gt[0]\n",
    "# Calculate the global min and max for each axis\n",
    "all_keypoints = np.array([motion_gt[frame].reshape(21, 3).squeeze().cpu().numpy() for frame in range(0, motion_gt.shape[0])])\n",
    "global_min = all_keypoints.min(axis=(0, 1))\n",
    "global_max = all_keypoints.max(axis=(0, 1))\n",
    "exp_identity = torch.zeros_like(x_s_info['exp'])\n",
    "# Generate frame\n",
    "t = x_s_info['t']\n",
    "pitch = x_s_info['pitch']\n",
    "yaw = x_s_info['yaw']\n",
    "roll = x_s_info['roll']\n",
    "scale = x_s_info['scale']\n",
    "\n",
    "# Create a function to update the plot\n",
    "def update_plot(frame_index, elev, azim, **point_selections):\n",
    "    # Extract keypoints from all_keypoints\n",
    "    keypoints = all_keypoints[frame_index]\n",
    "\n",
    "    # Set exp to identity plus the currently selected points\n",
    "    exp = exp_identity.clone()\n",
    "    selected_points = [int(point.split('_')[1]) for point, selected in point_selections.items() if selected]\n",
    "\n",
    "    for point in selected_points:\n",
    "        exp[0][point] = torch.tensor(keypoints[point])\n",
    "\n",
    "    x_d_i = scale * (x_c_s @ get_rotation_matrix(pitch, yaw, roll) + exp) + t\n",
    "\n",
    "    out = warp_decode(f_s, x_s, x_d_i)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    frame_img = (out['out'][0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # 3D plot\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "    # Plot the keypoints in 3D\n",
    "    ax1.scatter(keypoints[:, 0], keypoints[:, 1], keypoints[:, 2], c='r', s=20)\n",
    "\n",
    "    # Highlight selected points\n",
    "    if selected_points:\n",
    "        selected_keypoints = keypoints[selected_points]\n",
    "        ax1.scatter(selected_keypoints[:, 0], selected_keypoints[:, 1], selected_keypoints[:, 2], c='b', s=40)\n",
    "\n",
    "    # Add labels to each keypoint\n",
    "    for i, (x, y, z) in enumerate(keypoints):\n",
    "        ax1.text(x, y, z, str(i), fontsize=8)\n",
    "\n",
    "    # Set labels for each axis\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.set_zlabel('Z')\n",
    "\n",
    "    # Set title\n",
    "    ax1.set_title(f'3D Keypoints - Frame {frame_index}')\n",
    "\n",
    "    # Set fixed axis limits\n",
    "    ax1.set_xlim(global_min[0], global_max[0])\n",
    "    ax1.set_ylim(global_min[1], global_max[1])\n",
    "    ax1.set_zlim(global_min[2], global_max[2])\n",
    "\n",
    "    # Set the view angle\n",
    "    ax1.view_init(elev=elev, azim=azim)\n",
    "\n",
    "    # Generated frame\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.imshow(frame_img)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Generated Frame')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Create sliders for frame selection and view angle\n",
    "frame_slider = widgets.IntSlider(value=0, min=0, max=motion_gt.shape[0]-1, step=1, description='Frame:')\n",
    "elev_slider = widgets.FloatSlider(value=20, min=0, max=90, step=1, description='Elevation:')\n",
    "azim_slider = widgets.FloatSlider(value=45, min=-180, max=180, step=1, description='Azimuth:')\n",
    "\n",
    "# Create checkboxes for point selection\n",
    "point_checkboxes = [widgets.Checkbox(value=False, description=f'Point {i}') for i in range(21)]\n",
    "\n",
    "# Create the interactive plot\n",
    "interactive_plot = widgets.interactive(update_plot,\n",
    "                                       frame_index=frame_slider,\n",
    "                                       elev=elev_slider,\n",
    "                                       azim=azim_slider,\n",
    "                                       **{f'point_{i}': checkbox for i, checkbox in enumerate(point_checkboxes)})\n",
    "\n",
    "# Display the interactive plot\n",
    "display(widgets.VBox([interactive_plot, widgets.HBox(point_checkboxes)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Calculate the global min and max for each axis\n",
    "all_keypoints = np.array([motion_gt[frame, :].reshape(21, 3).squeeze().cpu().numpy() for frame in range(0, motion_gt.shape[0])])\n",
    "global_min = all_keypoints.min(axis=(0, 1))\n",
    "global_max = all_keypoints.max(axis=(0, 1))\n",
    "exp_identity = torch.zeros_like(x_s_info['exp'])\n",
    "# Generate frame\n",
    "t = x_s_info['t']\n",
    "pitch = x_s_info['pitch']\n",
    "yaw = x_s_info['yaw']\n",
    "roll = x_s_info['roll']\n",
    "scale = x_s_info['scale']\n",
    "\n",
    "# Create a function to update the plot\n",
    "def update_plotNew(frame_index, elev, azim, **dim_values):\n",
    "    # Extract keypoints from all_keypoints\n",
    "    keypoints = all_keypoints[frame_index]\n",
    "\n",
    "    # Set exp to identity plus the currently selected dimensions\n",
    "    exp = exp_identity.clone()\n",
    "    exp = exp.reshape(-1, 63)\n",
    "    for dim, value in dim_values.items():\n",
    "        if value is not None:\n",
    "            dim_index = int(dim.split('_')[1])\n",
    "            exp[0][dim_index] = torch.tensor(value)\n",
    "    exp = exp.reshape(-1, 21, 3)\n",
    "    x_d_i = scale * (x_c_s @ get_rotation_matrix(pitch, yaw, roll) + exp) + t\n",
    "\n",
    "    out = warp_decode(f_s, x_s, x_d_i)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    frame_img = (out['out'][0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # 3D plot\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "    # Plot the keypoints in 3D\n",
    "    ax1.scatter(keypoints[:, 0], keypoints[:, 1], keypoints[:, 2], c='r', s=20)\n",
    "\n",
    "    # Highlight modified dimensions\n",
    "    modified_dims = [int(dim.split('_')[1]) for dim, value in dim_values.items() if value is not None]\n",
    "    if modified_dims:\n",
    "        modified_keypoints = keypoints[np.array(modified_dims) // 3]\n",
    "        ax1.scatter(modified_keypoints[:, 0], modified_keypoints[:, 1], modified_keypoints[:, 2], c='b', s=40)\n",
    "\n",
    "    # Add labels to each keypoint\n",
    "    for i, (x, y, z) in enumerate(keypoints):\n",
    "        ax1.text(x, y, z, str(i), fontsize=8)\n",
    "\n",
    "    # Set labels for each axis\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.set_zlabel('Z')\n",
    "\n",
    "    # Set title\n",
    "    ax1.set_title(f'3D Keypoints - Frame {frame_index}')\n",
    "\n",
    "    # Set fixed axis limits\n",
    "    ax1.set_xlim(global_min[0], global_max[0])\n",
    "    ax1.set_ylim(global_min[1], global_max[1])\n",
    "    ax1.set_zlim(global_min[2], global_max[2])\n",
    "\n",
    "    # Set the view angle\n",
    "    ax1.view_init(elev=elev, azim=azim)\n",
    "\n",
    "    # Generated frame\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.imshow(frame_img)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Generated Frame')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Create sliders for frame selection and view angle\n",
    "frame_slider = widgets.IntSlider(value=0, min=0, max=motion_gt.shape[0]-1, step=1, description='Frame:')\n",
    "elev_slider = widgets.FloatSlider(value=20, min=0, max=90, step=1, description='Elevation:')\n",
    "azim_slider = widgets.FloatSlider(value=45, min=-180, max=180, step=1, description='Azimuth:')\n",
    "\n",
    "# Create input boxes for each dimension\n",
    "dim_inputs = [widgets.FloatText(value=None, description=f'Dim {i}:', continuous_update=False) for i in range(63)]\n",
    "\n",
    "# Create the interactive plot\n",
    "interactive_plot = widgets.interactive(update_plotNew,\n",
    "                                       frame_index=frame_slider,\n",
    "                                       elev=elev_slider,\n",
    "                                       azim=azim_slider,\n",
    "                                       **{f'dim_{i}': input_box for i, input_box in enumerate(dim_inputs)})\n",
    "\n",
    "# Display the interactive plot\n",
    "display(widgets.VBox([\n",
    "    interactive_plot,\n",
    "    widgets.HBox([widgets.VBox(dim_inputs[:21]), widgets.VBox(dim_inputs[21:42]), widgets.VBox(dim_inputs[42:])])\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vasa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
