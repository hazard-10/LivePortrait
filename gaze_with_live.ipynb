{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "portrait_imgs = [\n",
    "    '/mnt/c/Users/mjh/Downloads/live_in/t4.jpg'\n",
    "]\n",
    "audio_paths = [\n",
    "    # '/mnt/c/Users/mjh/Downloads/live_in/i3.wav',\n",
    "    # '/mnt/c/Users/mjh/Downloads/live_in/i5.wav',\n",
    "    # '/mnt/c/Users/mjh/Downloads/live_in/i7.wav',\n",
    "    '/mnt/c/Users/mjh/Downloads/live_in/i8.wav'\n",
    "    # '/mnt/c/Users/mjh/Downloads/live_in/speech.wav'\n",
    "]\n",
    "model_weights_pairs = [\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/model_1023.pth'),\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/model_sterotype_0_125.pth'),\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/model_sterotype_1_140.pth'),\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/model_1111.pth'),\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/model_0.2_2.pth'),\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/model_5_5.pth'),\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/redeem_5_vel_5_acc_ep_90.pth'),\n",
    "    ('audio_dit/output/config.json', 'audio_dit/output/norm_no_vel_ep_60.pth'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import liveportrait modules\n",
    "import time\n",
    "import os\n",
    "import contextlib\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "import tyro\n",
    "import subprocess\n",
    "from rich.progress import track\n",
    "import torchvision\n",
    "import cv2\n",
    "import threading\n",
    "import queue\n",
    "import torchvision.transforms as transforms\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import imageio\n",
    "\n",
    "from src.config.argument_config import ArgumentConfig\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.config.crop_config import CropConfig\n",
    "\n",
    "def partial_fields(target_class, kwargs):\n",
    "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
    "\n",
    "args = ArgumentConfig()\n",
    "inference_cfg = partial_fields(InferenceConfig, args.__dict__)\n",
    "crop_cfg = partial_fields(CropConfig, args.__dict__)\n",
    "# print(\"inference_cfg: \", inference_cfg)\n",
    "# print(\"crop_cfg: \", crop_cfg)\n",
    "device = 'cuda'\n",
    "print(\"Compile complete\")\n",
    "\n",
    "'''\n",
    "Common modules\n",
    "'''\n",
    "from src.utils.helper import load_model\n",
    "from src.utils.camera import headpose_pred_to_degree, get_rotation_matrix\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.utils.cropper import Cropper\n",
    "from src.utils.camera import get_rotation_matrix\n",
    "from src.utils.io import load_image_rgb\n",
    "\n",
    "'''\n",
    "Main module for inference\n",
    "'''\n",
    "model_config = yaml.load(open(inference_cfg.models_config, 'r'), Loader=yaml.SafeLoader)\n",
    "# init F\n",
    "appearance_feature_extractor = load_model(inference_cfg.checkpoint_F, model_config, device, 'appearance_feature_extractor')\n",
    "# init M\n",
    "motion_extractor = load_model(inference_cfg.checkpoint_M, model_config, device, 'motion_extractor')\n",
    "# init W\n",
    "warping_module = load_model(inference_cfg.checkpoint_W, model_config, device, 'warping_module')\n",
    "# init G\n",
    "spade_generator = load_model(inference_cfg.checkpoint_G, model_config, device, 'spade_generator')\n",
    "# init S and R\n",
    "if inference_cfg.checkpoint_S is not None and os.path.exists(inference_cfg.checkpoint_S):\n",
    "    stitching_retargeting_module = load_model(inference_cfg.checkpoint_S, model_config, device, 'stitching_retargeting_module')\n",
    "else:\n",
    "    stitching_retargeting_module = None\n",
    "\n",
    "cropper = Cropper(crop_cfg=crop_cfg, device=device)\n",
    "\n",
    "'''\n",
    "Main function for inference\n",
    "'''\n",
    "\n",
    "def get_kp_info(x: torch.Tensor, **kwargs) -> dict:\n",
    "    \"\"\" get the implicit keypoint information\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    flag_refine_info: whether to trandform the pose to degrees and the dimention of the reshape\n",
    "    return: A dict contains keys: 'pitch', 'yaw', 'roll', 't', 'exp', 'scale', 'kp'\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        kp_info = motion_extractor(x)\n",
    "\n",
    "        if inference_cfg.flag_use_half_precision:\n",
    "            # float the dict\n",
    "            for k, v in kp_info.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    kp_info[k] = v.float()\n",
    "\n",
    "    flag_refine_info: bool = kwargs.get('flag_refine_info', True)\n",
    "    if flag_refine_info:\n",
    "        bs = kp_info['kp'].shape[0]\n",
    "        kp_info['pitch'] = headpose_pred_to_degree(kp_info['pitch'])[:, None]  # Bx1\n",
    "        kp_info['yaw'] = headpose_pred_to_degree(kp_info['yaw'])[:, None]  # Bx1\n",
    "        kp_info['roll'] = headpose_pred_to_degree(kp_info['roll'])[:, None]  # Bx1\n",
    "        kp_info['kp'] = kp_info['kp'].reshape(bs, -1, 3)  # BxNx3\n",
    "        kp_info['exp'] = kp_info['exp'].reshape(bs, -1, 3)  # BxNx3\n",
    "\n",
    "    return kp_info\n",
    "\n",
    "def prepare_source(img: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    img: HxWx3, uint8, 256x256\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    x = img.copy()\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        x = x[np.newaxis].astype(np.float32) / 255.  # HxWx3 -> 1xHxWx3, normalized to 0~1\n",
    "    elif x.ndim == 4:\n",
    "        x = x.astype(np.float32) / 255.  # BxHxWx3, normalized to 0~1\n",
    "    else:\n",
    "        raise ValueError(f'img ndim should be 3 or 4: {x.ndim}')\n",
    "    x = np.clip(x, 0, 1)  # clip to 0~1\n",
    "    x = torch.from_numpy(x).permute(0, 3, 1, 2)  # 1xHxWx3 -> 1x3xHxW\n",
    "    x = x.to(device)\n",
    "    return x\n",
    "\n",
    "def warp_decode(feature_3d: torch.Tensor, kp_source: torch.Tensor, kp_driving: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the image after the warping of the implicit keypoints\n",
    "    feature_3d: Bx32x16x64x64, feature volume\n",
    "    kp_source: BxNx3\n",
    "    kp_driving: BxNx3\n",
    "    \"\"\"\n",
    "    # The line 18 in Algorithm 1: D(W(f_s; x_s, x′_d,i)）\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        # get decoder input\n",
    "        ret_dct = warping_module(feature_3d, kp_source=kp_source, kp_driving=kp_driving)\n",
    "        # decode\n",
    "        ret_dct['out'] = spade_generator(feature=ret_dct['out'])\n",
    "\n",
    "    return ret_dct\n",
    "\n",
    "def extract_feature_3d( x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the appearance feature of the image by F\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        feature_3d = appearance_feature_extractor(x)\n",
    "\n",
    "    return feature_3d.float()\n",
    "\n",
    "def transform_keypoint(kp_info: dict):\n",
    "    \"\"\"\n",
    "    transform the implicit keypoints with the pose, shift, and expression deformation\n",
    "    kp: BxNx3\n",
    "    \"\"\"\n",
    "    kp = kp_info['kp']    # (bs, k, 3)\n",
    "    pitch, yaw, roll = kp_info['pitch'], kp_info['yaw'], kp_info['roll']\n",
    "\n",
    "    t, exp = kp_info['t'], kp_info['exp']\n",
    "    scale = kp_info['scale']\n",
    "\n",
    "    pitch = headpose_pred_to_degree(pitch)\n",
    "    yaw = headpose_pred_to_degree(yaw)\n",
    "    roll = headpose_pred_to_degree(roll)\n",
    "\n",
    "    bs = kp.shape[0]\n",
    "    if kp.ndim == 2:\n",
    "        num_kp = kp.shape[1] // 3  # Bx(num_kpx3)\n",
    "    else:\n",
    "        num_kp = kp.shape[1]  # Bxnum_kpx3\n",
    "\n",
    "    rot_mat = get_rotation_matrix(pitch, yaw, roll)    # (bs, 3, 3)\n",
    "\n",
    "    # Eqn.2: s * (R * x_c,s + exp) + t\n",
    "    kp_transformed = kp.view(bs, num_kp, 3) @ rot_mat + exp.view(bs, num_kp, 3)\n",
    "    kp_transformed *= scale[..., None]  # (bs, k, 3) * (bs, 1, 1) = (bs, k, 3)\n",
    "    kp_transformed[:, :, 0:2] += t[:, None, 0:2]  # remove z, only apply tx ty\n",
    "\n",
    "    return kp_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = portrait_imgs[0]\n",
    "offset_std = 0.002\n",
    "# Load and prepare image\n",
    "img_rgb = load_image_rgb(image_path)\n",
    "img_crop_256x256 = cv2.resize(img_rgb, (256, 256))\n",
    "I_s = prepare_source(img_crop_256x256)\n",
    "\n",
    "# Get keypoint info\n",
    "x_s_info = get_kp_info(I_s)\n",
    "x_c_s = x_s_info['kp']\n",
    "x_exp = x_s_info['exp']\n",
    "x_s = transform_keypoint(x_s_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s_info.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = [0.018, 0.018, 0.018]\n",
    "rest_val = [0.006, -0.002, -0.002]\n",
    "min_val = [-0.005, -0.015, -0.015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = portrait_imgs[0]\n",
    "offset_std = 0.002\n",
    "# Load and prepare image\n",
    "img_rgb = load_image_rgb(image_path)\n",
    "img_crop_256x256 = cv2.resize(img_rgb, (256, 256))\n",
    "I_s = prepare_source(img_crop_256x256)\n",
    "\n",
    "# Get keypoint info\n",
    "x_s_info = get_kp_info(I_s)\n",
    "x_c_s = x_s_info['kp']\n",
    "x_exp = x_s_info['exp']\n",
    "x_scale = x_s_info['scale']\n",
    "x_s = transform_keypoint(x_s_info)\n",
    "\n",
    "t_identity = torch.zeros((1, 3), dtype=torch.float32, device=device)\n",
    "pitch_identity = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "yaw_identity = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "roll_identity = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "scale_identity = torch.ones((1), dtype=torch.float32, device=device) * 1.5\n",
    "\n",
    "if False:\n",
    "    t_s = x_s_info['t']\n",
    "    pitch_s = x_s_info['pitch'] - 10\n",
    "    yaw_s = yaw_identity\n",
    "    roll_s = roll_identity\n",
    "    scale_s = x_s_info['scale']\n",
    "\n",
    "pitch = pitch_identity\n",
    "yaw = yaw_identity\n",
    "roll = roll_identity\n",
    "\n",
    "# Extract features\n",
    "f_s = extract_feature_3d(I_s)\n",
    "x_exp = torch.zeros_like(x_exp)\n",
    "x_exp = x_exp.reshape(-1)\n",
    "x_exp[4] = -0.018\n",
    "x_exp[33] = 0.018\n",
    "x_exp[45] = 0.011\n",
    "x_exp[48] = 0.011\n",
    "x_exp = x_exp.reshape(1, 21, 3)\n",
    "\n",
    "x_d_i = x_scale * (x_c_s @ get_rotation_matrix(pitch, yaw, roll) + x_exp) + t_identity\n",
    "\n",
    "# Add random offset to final dimension\n",
    "random_offset = torch.randn(1, device=device) * offset_std\n",
    "x_s[:, :, -1] += random_offset\n",
    "\n",
    "# Render with modified latents\n",
    "out = warp_decode(f_s, x_s, x_d_i)\n",
    "out_img = (out['out'].permute(0, 2, 3, 1).cpu().numpy() * 255).astype(np.uint8)\n",
    "print(out_img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2cs import Pipeline, render\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "\n",
    "gaze_pipeline = Pipeline(\n",
    "    weights='/mnt/c/Users/mjh/Downloads/L2CSNet_gaze360.pkl',\n",
    "    arch='ResNet50',\n",
    "    device=torch.device('cpu') # or 'gpu'\n",
    ")\n",
    "frame = out_img[0]\n",
    "results = gaze_pipeline.step(frame)\n",
    "gaze_pitch = results.pitch\n",
    "gaze_yaw = results.yaw\n",
    "# Convert radians to degrees\n",
    "gaze_pitch = gaze_pitch * 180 / np.pi\n",
    "gaze_yaw = gaze_yaw * 180 / np.pi\n",
    "\n",
    "print(f'gaze_pitch: {gaze_pitch}, gaze_yaw: {gaze_yaw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(out_img[0])\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vasa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
