{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile\n",
    "and initialize args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile complete\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import contextlib\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "import tyro\n",
    "import subprocess\n",
    "from rich.progress import track\n",
    "import torchvision\n",
    "import cv2\n",
    "import threading\n",
    "import queue\n",
    "import torchvision.transforms as transforms\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import imageio\n",
    "\n",
    "from src.config.argument_config import ArgumentConfig\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.config.crop_config import CropConfig\n",
    "\n",
    "def partial_fields(target_class, kwargs):\n",
    "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
    "\n",
    "args = ArgumentConfig()\n",
    "inference_cfg = partial_fields(InferenceConfig, args.__dict__)\n",
    "crop_cfg = partial_fields(CropConfig, args.__dict__)\n",
    "# print(\"inference_cfg: \", inference_cfg)\n",
    "# print(\"crop_cfg: \", crop_cfg)\n",
    "device = 'cuda'\n",
    "print(\"Compile complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize util functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helper import load_model, concat_feat\n",
    "from src.utils.camera import headpose_pred_to_degree, get_rotation_matrix\n",
    "from src.utils.retargeting_utils import calc_eye_close_ratio, calc_lip_close_ratio\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.utils.cropper import Cropper\n",
    "from src.utils.camera import get_rotation_matrix\n",
    "from src.utils.video import images2video, concat_frames, get_fps, add_audio_to_video, has_audio_stream\n",
    "from src.utils.crop import _transform_img, prepare_paste_back, paste_back\n",
    "from src.utils.io import load_image_rgb, load_video, resize_to_limit, dump, load\n",
    "from src.utils.helper import mkdir, basename, dct2device, is_video, is_template, remove_suffix, is_image\n",
    "from src.utils.filter import smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11:43:01] </span>LandmarkRunner warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.</span>020s                                                 <a href=\"file:///mnt/e/wsl_projects/LivePortrait/src/utils/landmark_runner.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">landmark_runner.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/e/wsl_projects/LivePortrait/src/utils/landmark_runner.py#95\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">95</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11:43:01]\u001b[0m\u001b[2;36m \u001b[0mLandmarkRunner warmup time: \u001b[1;36m6.\u001b[0m020s                                                 \u001b]8;id=682267;file:///mnt/e/wsl_projects/LivePortrait/src/utils/landmark_runner.py\u001b\\\u001b[2mlandmark_runner.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=207724;file:///mnt/e/wsl_projects/LivePortrait/src/utils/landmark_runner.py#95\u001b\\\u001b[2m95\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11:43:04] </span>FaceAnalysisDIY warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>853s                                              <a href=\"file:///mnt/e/wsl_projects/LivePortrait/src/utils/face_analysis_diy.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">face_analysis_diy.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///mnt/e/wsl_projects/LivePortrait/src/utils/face_analysis_diy.py#79\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">79</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11:43:04]\u001b[0m\u001b[2;36m \u001b[0mFaceAnalysisDIY warmup time: \u001b[1;36m1.\u001b[0m853s                                              \u001b]8;id=38620;file:///mnt/e/wsl_projects/LivePortrait/src/utils/face_analysis_diy.py\u001b\\\u001b[2mface_analysis_diy.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=889464;file:///mnt/e/wsl_projects/LivePortrait/src/utils/face_analysis_diy.py#79\u001b\\\u001b[2m79\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_config = yaml.load(open(inference_cfg.models_config, 'r'), Loader=yaml.SafeLoader)\n",
    "# init F\n",
    "appearance_feature_extractor = load_model(inference_cfg.checkpoint_F, model_config, device, 'appearance_feature_extractor')\n",
    "# init M\n",
    "motion_extractor = load_model(inference_cfg.checkpoint_M, model_config, device, 'motion_extractor')\n",
    "# init W\n",
    "warping_module = load_model(inference_cfg.checkpoint_W, model_config, device, 'warping_module')\n",
    "# init G\n",
    "spade_generator = load_model(inference_cfg.checkpoint_G, model_config, device, 'spade_generator')\n",
    "# init S and R\n",
    "if inference_cfg.checkpoint_S is not None and os.path.exists(inference_cfg.checkpoint_S):\n",
    "    stitching_retargeting_module = load_model(inference_cfg.checkpoint_S, model_config, device, 'stitching_retargeting_module')\n",
    "else:\n",
    "    stitching_retargeting_module = None\n",
    "\n",
    "cropper = Cropper(crop_cfg=crop_cfg, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_distance_ratio(lmk: np.ndarray, idx1: int, idx2: int, idx3: int, idx4: int, eps: float = 1e-6) -> np.ndarray:\n",
    "    return (np.linalg.norm(lmk[:, idx1] - lmk[:, idx2], axis=1, keepdims=True) /\n",
    "            (np.linalg.norm(lmk[:, idx3] - lmk[:, idx4], axis=1, keepdims=True) + eps))\n",
    "\n",
    "\n",
    "def calc_eye_close_ratio(lmk: np.ndarray, target_eye_ratio: np.ndarray = None) -> np.ndarray:\n",
    "    lefteye_close_ratio = calculate_distance_ratio(lmk, 6, 18, 0, 12)\n",
    "    righteye_close_ratio = calculate_distance_ratio(lmk, 30, 42, 24, 36)\n",
    "    if target_eye_ratio is not None:\n",
    "        return np.concatenate([lefteye_close_ratio, righteye_close_ratio, target_eye_ratio], axis=1)\n",
    "    else:\n",
    "        return np.concatenate([lefteye_close_ratio, righteye_close_ratio], axis=1)\n",
    "\n",
    "\n",
    "def calc_lip_close_ratio(lmk: np.ndarray) -> np.ndarray:\n",
    "    return calculate_distance_ratio(lmk, 90, 102, 48, 66)\n",
    "\n",
    "def calc_ratio(lmk_lst):\n",
    "    input_eye_ratio_lst = []\n",
    "    input_lip_ratio_lst = []\n",
    "    for lmk in lmk_lst:\n",
    "        # for eyes retargeting\n",
    "        input_eye_ratio_lst.append(calc_eye_close_ratio(lmk[None]))\n",
    "        # for lip retargeting\n",
    "        input_lip_ratio_lst.append(calc_lip_close_ratio(lmk[None]))\n",
    "    return input_eye_ratio_lst, input_lip_ratio_lst\n",
    "\n",
    "def prepare_videos(imgs, device) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    imgs: NxBxHxWx3, uint8\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, list):\n",
    "        _imgs = np.array(imgs)[..., np.newaxis]  # TxHxWx3x1\n",
    "    elif isinstance(imgs, np.ndarray):\n",
    "        _imgs = imgs\n",
    "    else:\n",
    "        raise ValueError(f'imgs type error: {type(imgs)}')\n",
    "\n",
    "    y = _imgs.astype(np.float32) / 255.\n",
    "    y = np.clip(y, 0, 1)  # clip to 0~1\n",
    "    y = torch.from_numpy(y).permute(0, 4, 3, 1, 2)  # TxHxWx3x1 -> Tx1x3xHxW\n",
    "    y = y.to(device)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading single vid or dir of vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_videos_(imgs, device):\n",
    "    \"\"\" construct the input as standard\n",
    "    imgs: NxHxWx3, uint8\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, list):\n",
    "        _imgs = np.array(imgs)\n",
    "    elif isinstance(imgs, np.ndarray):\n",
    "        _imgs = imgs\n",
    "    else:\n",
    "        raise ValueError(f'imgs type error: {type(imgs)}')\n",
    "\n",
    "    # y = _imgs.astype(np.float32) / 255.\n",
    "    y = _imgs\n",
    "    y = torch.from_numpy(y).permute(0, 3, 1, 2)  # NxHxWx3 -> Nx3xHxW\n",
    "    y = y.to(device)\n",
    "    y = y / 255.\n",
    "    y = torch.clamp(y, 0, 1)\n",
    "\n",
    "    return y\n",
    "\n",
    "def read_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (256, 256))  # Resize to 256x256\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return video_path, frames\n",
    "\n",
    "def read_multiple_videos(video_paths, num_threads=4):\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        results = list(executor.map(read_video_frames, video_paths))\n",
    "    return results\n",
    "\n",
    "def process_videos(input_path, num_threads=4):\n",
    "    if os.path.isdir(input_path):\n",
    "        video_paths = sorted(glob.glob(os.path.join(input_path, '*.mp4')))  # Sort to ensure consistent order\n",
    "        print(f\"Found {len(video_paths)} video files.\")\n",
    "        video_frames = read_multiple_videos(video_paths, num_threads)\n",
    "    else:\n",
    "        print(f\"Processing single video file: {input_path}\")\n",
    "        video_frames = [read_video_frames(input_path)]\n",
    "\n",
    "    all_frames = []\n",
    "    total_frames = 0\n",
    "    video_lengths = []\n",
    "\n",
    "    for video_path, frames in video_frames:\n",
    "        all_frames.extend(frames)\n",
    "        frame_count = len(frames)\n",
    "        total_frames += frame_count\n",
    "        video_lengths.append(frame_count)\n",
    "        print(f\"Processed video: {video_path}, frames: {frame_count}\")\n",
    "\n",
    "    print(f\"\\nTotal frames across all videos: {total_frames}\")\n",
    "    print(f\"Video lengths: {video_lengths}\")\n",
    "\n",
    "    # Convert to numpy array\n",
    "    all_frames = np.array(all_frames)\n",
    "\n",
    "    print(f\"Shape of concatenated array: {all_frames.shape}\")\n",
    "    return all_frames, video_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def extract_audio(video_path):\n",
    "    # Generate a unique filename for the audio in the current directory\n",
    "    audio_filename = f\"extracted_audio_{os.path.basename(video_path).split('.')[0]}.wav\"\n",
    "    audio_path = os.path.join(os.getcwd(), audio_filename)\n",
    "\n",
    "    # Use ffmpeg to extract audio\n",
    "    try:\n",
    "        subprocess.run(['ffmpeg', '-i', video_path, '-q:a', '0', '-map', 'a', audio_path], check=True)\n",
    "        print(f\"Audio extracted successfully: {audio_path}\")\n",
    "        return audio_path\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error extracting audio: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# video_path = \"path/to/your/video.mp4\"\n",
    "# audio_path = extract_audio(video_path)\n",
    "# if audio_path:\n",
    "#     print(f\"Audio saved to: {audio_path}\")\n",
    "# else:\n",
    "#     print(\"Failed to extract audio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motion Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kp_info(x: torch.Tensor, **kwargs) -> dict:\n",
    "    \"\"\" get the implicit keypoint information\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    flag_refine_info: whether to trandform the pose to degrees and the dimention of the reshape\n",
    "    return: A dict contains keys: 'pitch', 'yaw', 'roll', 't', 'exp', 'scale', 'kp'\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        kp_info = motion_extractor(x)\n",
    "\n",
    "        if inference_cfg.flag_use_half_precision:\n",
    "            # float the dict\n",
    "            for k, v in kp_info.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    kp_info[k] = v.float()\n",
    "\n",
    "    flag_refine_info: bool = kwargs.get('flag_refine_info', True)\n",
    "    if flag_refine_info:\n",
    "        bs = kp_info['kp'].shape[0]\n",
    "        kp_info['pitch'] = headpose_pred_to_degree(kp_info['pitch'])[:, None]  # Bx1\n",
    "        kp_info['yaw'] = headpose_pred_to_degree(kp_info['yaw'])[:, None]  # Bx1\n",
    "        kp_info['roll'] = headpose_pred_to_degree(kp_info['roll'])[:, None]  # Bx1\n",
    "        kp_info['kp'] = kp_info['kp'].reshape(bs, -1, 3)  # BxNx3\n",
    "        kp_info['exp'] = kp_info['exp'].reshape(bs, -1, 3)  # BxNx3\n",
    "\n",
    "    return kp_info\n",
    "\n",
    "def process_driving_video(I_d_lst):\n",
    "    n_frames = I_d_lst.shape[0]\n",
    "    template_dct = {\n",
    "        'n_frames': n_frames,\n",
    "        'output_fps': 25,\n",
    "        'motion': [],\n",
    "        'c_d_eyes_lst': [],\n",
    "        'c_d_lip_lst': [],\n",
    "        'x_i_info_lst': [],\n",
    "    }\n",
    "\n",
    "    for i in range(n_frames):\n",
    "        # collect s, R, δ and t for inference\n",
    "        I_i = I_d_lst[i]\n",
    "        x_i_info = get_kp_info(I_i)\n",
    "        R_i = get_rotation_matrix(x_i_info['pitch'], x_i_info['yaw'], x_i_info['roll'])\n",
    "\n",
    "        item_dct = {\n",
    "            'scale': x_i_info['scale'].cpu().numpy().astype(np.float32),\n",
    "            'R': R_i.cpu().numpy().astype(np.float32),\n",
    "            'exp': x_i_info['exp'].cpu().numpy().astype(np.float32),\n",
    "            't': x_i_info['t'].cpu().numpy().astype(np.float32),\n",
    "        }\n",
    "\n",
    "        template_dct['motion'].append(item_dct)\n",
    "\n",
    "        # c_eyes = c_d_eyes_lst[i].astype(np.float32)\n",
    "        # template_dct['c_d_eyes_lst'].append(c_eyes)\n",
    "\n",
    "        # c_lip = c_d_lip_lst[i].astype(np.float32)\n",
    "        # template_dct['c_d_lip_lst'].append(c_lip)\n",
    "\n",
    "        template_dct['x_i_info_lst'].append(x_i_info)\n",
    "        print(f'frame {i} done')\n",
    "\n",
    "    return template_dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source image extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_source(img: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    img: HxWx3, uint8, 256x256\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    x = img.copy()\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        x = x[np.newaxis].astype(np.float32) / 255.  # HxWx3 -> 1xHxWx3, normalized to 0~1\n",
    "    elif x.ndim == 4:\n",
    "        x = x.astype(np.float32) / 255.  # BxHxWx3, normalized to 0~1\n",
    "    else:\n",
    "        raise ValueError(f'img ndim should be 3 or 4: {x.ndim}')\n",
    "    x = np.clip(x, 0, 1)  # clip to 0~1\n",
    "    x = torch.from_numpy(x).permute(0, 3, 1, 2)  # 1xHxWx3 -> 1x3xHxW\n",
    "    x = x.to(device)\n",
    "    return x\n",
    "\n",
    "def warp_decode(feature_3d: torch.Tensor, kp_source: torch.Tensor, kp_driving: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the image after the warping of the implicit keypoints\n",
    "    feature_3d: Bx32x16x64x64, feature volume\n",
    "    kp_source: BxNx3\n",
    "    kp_driving: BxNx3\n",
    "    \"\"\"\n",
    "    # The line 18 in Algorithm 1: D(W(f_s; x_s, x′_d,i)）\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        # get decoder input\n",
    "        ret_dct = warping_module(feature_3d, kp_source=kp_source, kp_driving=kp_driving)\n",
    "        # decode\n",
    "        ret_dct['out'] = spade_generator(feature=ret_dct['out'])\n",
    "\n",
    "    return ret_dct\n",
    "\n",
    "def extract_feature_3d( x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the appearance feature of the image by F\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        feature_3d = appearance_feature_extractor(x)\n",
    "\n",
    "    return feature_3d.float()\n",
    "\n",
    "def transform_keypoint(kp_info: dict):\n",
    "    \"\"\"\n",
    "    transform the implicit keypoints with the pose, shift, and expression deformation\n",
    "    kp: BxNx3\n",
    "    \"\"\"\n",
    "    kp = kp_info['kp']    # (bs, k, 3)\n",
    "    pitch, yaw, roll = kp_info['pitch'], kp_info['yaw'], kp_info['roll']\n",
    "\n",
    "    t, exp = kp_info['t'], kp_info['exp']\n",
    "    scale = kp_info['scale']\n",
    "\n",
    "    pitch = headpose_pred_to_degree(pitch)\n",
    "    yaw = headpose_pred_to_degree(yaw)\n",
    "    roll = headpose_pred_to_degree(roll)\n",
    "\n",
    "    bs = kp.shape[0]\n",
    "    if kp.ndim == 2:\n",
    "        num_kp = kp.shape[1] // 3  # Bx(num_kpx3)\n",
    "    else:\n",
    "        num_kp = kp.shape[1]  # Bxnum_kpx3\n",
    "\n",
    "    rot_mat = get_rotation_matrix(pitch, yaw, roll)    # (bs, 3, 3)\n",
    "\n",
    "    # Eqn.2: s * (R * x_c,s + exp) + t\n",
    "    kp_transformed = kp.view(bs, num_kp, 3) @ rot_mat + exp.view(bs, num_kp, 3)\n",
    "    kp_transformed *= scale[..., None]  # (bs, k, 3) * (bs, 1, 1) = (bs, k, 3)\n",
    "    kp_transformed[:, :, 0:2] += t[:, None, 0:2]  # remove z, only apply tx ty\n",
    "\n",
    "    return kp_transformed\n",
    "\n",
    "def parse_output(out: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\" construct the output as standard\n",
    "    return: 1xHxWx3, uint8\n",
    "    \"\"\"\n",
    "    out = np.transpose(out.data.cpu().numpy(), [0, 2, 3, 1])  # 1x3xHxW -> 1xHxWx3\n",
    "    out = np.clip(out, 0, 1)  # clip to 0~1\n",
    "    out = np.clip(out * 255, 0, 255).astype(np.uint8)  # 0~1 -> 0~255\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing single video file: /mnt/e/data/vox2/videos/512/id00078/P0OU4bFhwCI/00227.mp4\n",
      "Processed video: /mnt/e/data/vox2/videos/512/id00078/P0OU4bFhwCI/00227.mp4, frames: 370\n",
      "\n",
      "Total frames across all videos: 370\n",
      "Video lengths: [370]\n",
      "Shape of concatenated array: (370, 256, 256, 3)\n",
      "Shape of driving video: torch.Size([370, 1, 3, 256, 256])\n",
      "frame 0 done\n",
      "frame 1 done\n",
      "frame 2 done\n",
      "frame 3 done\n",
      "frame 4 done\n",
      "frame 5 done\n",
      "frame 6 done\n",
      "frame 7 done\n",
      "frame 8 done\n",
      "frame 9 done\n",
      "frame 10 done\n",
      "frame 11 done\n",
      "frame 12 done\n",
      "frame 13 done\n",
      "frame 14 done\n",
      "frame 15 done\n",
      "frame 16 done\n",
      "frame 17 done\n",
      "frame 18 done\n",
      "frame 19 done\n",
      "frame 20 done\n",
      "frame 21 done\n",
      "frame 22 done\n",
      "frame 23 done\n",
      "frame 24 done\n",
      "frame 25 done\n",
      "frame 26 done\n",
      "frame 27 done\n",
      "frame 28 done\n",
      "frame 29 done\n",
      "frame 30 done\n",
      "frame 31 done\n",
      "frame 32 done\n",
      "frame 33 done\n",
      "frame 34 done\n",
      "frame 35 done\n",
      "frame 36 done\n",
      "frame 37 done\n",
      "frame 38 done\n",
      "frame 39 done\n",
      "frame 40 done\n",
      "frame 41 done\n",
      "frame 42 done\n",
      "frame 43 done\n",
      "frame 44 done\n",
      "frame 45 done\n",
      "frame 46 done\n",
      "frame 47 done\n",
      "frame 48 done\n",
      "frame 49 done\n",
      "frame 50 done\n",
      "frame 51 done\n",
      "frame 52 done\n",
      "frame 53 done\n",
      "frame 54 done\n",
      "frame 55 done\n",
      "frame 56 done\n",
      "frame 57 done\n",
      "frame 58 done\n",
      "frame 59 done\n",
      "frame 60 done\n",
      "frame 61 done\n",
      "frame 62 done\n",
      "frame 63 done\n",
      "frame 64 done\n",
      "frame 65 done\n",
      "frame 66 done\n",
      "frame 67 done\n",
      "frame 68 done\n",
      "frame 69 done\n",
      "frame 70 done\n",
      "frame 71 done\n",
      "frame 72 done\n",
      "frame 73 done\n",
      "frame 74 done\n",
      "frame 75 done\n",
      "frame 76 done\n",
      "frame 77 done\n",
      "frame 78 done\n",
      "frame 79 done\n",
      "frame 80 done\n",
      "frame 81 done\n",
      "frame 82 done\n",
      "frame 83 done\n",
      "frame 84 done\n",
      "frame 85 done\n",
      "frame 86 done\n",
      "frame 87 done\n",
      "frame 88 done\n",
      "frame 89 done\n",
      "frame 90 done\n",
      "frame 91 done\n",
      "frame 92 done\n",
      "frame 93 done\n",
      "frame 94 done\n",
      "frame 95 done\n",
      "frame 96 done\n",
      "frame 97 done\n",
      "frame 98 done\n",
      "frame 99 done\n",
      "frame 100 done\n",
      "frame 101 done\n",
      "frame 102 done\n",
      "frame 103 done\n",
      "frame 104 done\n",
      "frame 105 done\n",
      "frame 106 done\n",
      "frame 107 done\n",
      "frame 108 done\n",
      "frame 109 done\n",
      "frame 110 done\n",
      "frame 111 done\n",
      "frame 112 done\n",
      "frame 113 done\n",
      "frame 114 done\n",
      "frame 115 done\n",
      "frame 116 done\n",
      "frame 117 done\n",
      "frame 118 done\n",
      "frame 119 done\n",
      "frame 120 done\n",
      "frame 121 done\n",
      "frame 122 done\n",
      "frame 123 done\n",
      "frame 124 done\n",
      "frame 125 done\n",
      "frame 126 done\n",
      "frame 127 done\n",
      "frame 128 done\n",
      "frame 129 done\n",
      "frame 130 done\n",
      "frame 131 done\n",
      "frame 132 done\n",
      "frame 133 done\n",
      "frame 134 done\n",
      "frame 135 done\n",
      "frame 136 done\n",
      "frame 137 done\n",
      "frame 138 done\n",
      "frame 139 done\n",
      "frame 140 done\n",
      "frame 141 done\n",
      "frame 142 done\n",
      "frame 143 done\n",
      "frame 144 done\n",
      "frame 145 done\n",
      "frame 146 done\n",
      "frame 147 done\n",
      "frame 148 done\n",
      "frame 149 done\n",
      "frame 150 done\n",
      "frame 151 done\n",
      "frame 152 done\n",
      "frame 153 done\n",
      "frame 154 done\n",
      "frame 155 done\n",
      "frame 156 done\n",
      "frame 157 done\n",
      "frame 158 done\n",
      "frame 159 done\n",
      "frame 160 done\n",
      "frame 161 done\n",
      "frame 162 done\n",
      "frame 163 done\n",
      "frame 164 done\n",
      "frame 165 done\n",
      "frame 166 done\n",
      "frame 167 done\n",
      "frame 168 done\n",
      "frame 169 done\n",
      "frame 170 done\n",
      "frame 171 done\n",
      "frame 172 done\n",
      "frame 173 done\n",
      "frame 174 done\n",
      "frame 175 done\n",
      "frame 176 done\n",
      "frame 177 done\n",
      "frame 178 done\n",
      "frame 179 done\n",
      "frame 180 done\n",
      "frame 181 done\n",
      "frame 182 done\n",
      "frame 183 done\n",
      "frame 184 done\n",
      "frame 185 done\n",
      "frame 186 done\n",
      "frame 187 done\n",
      "frame 188 done\n",
      "frame 189 done\n",
      "frame 190 done\n",
      "frame 191 done\n",
      "frame 192 done\n",
      "frame 193 done\n",
      "frame 194 done\n",
      "frame 195 done\n",
      "frame 196 done\n",
      "frame 197 done\n",
      "frame 198 done\n",
      "frame 199 done\n",
      "frame 200 done\n",
      "frame 201 done\n",
      "frame 202 done\n",
      "frame 203 done\n",
      "frame 204 done\n",
      "frame 205 done\n",
      "frame 206 done\n",
      "frame 207 done\n",
      "frame 208 done\n",
      "frame 209 done\n",
      "frame 210 done\n",
      "frame 211 done\n",
      "frame 212 done\n",
      "frame 213 done\n",
      "frame 214 done\n",
      "frame 215 done\n",
      "frame 216 done\n",
      "frame 217 done\n",
      "frame 218 done\n",
      "frame 219 done\n",
      "frame 220 done\n",
      "frame 221 done\n",
      "frame 222 done\n",
      "frame 223 done\n",
      "frame 224 done\n",
      "frame 225 done\n",
      "frame 226 done\n",
      "frame 227 done\n",
      "frame 228 done\n",
      "frame 229 done\n",
      "frame 230 done\n",
      "frame 231 done\n",
      "frame 232 done\n",
      "frame 233 done\n",
      "frame 234 done\n",
      "frame 235 done\n",
      "frame 236 done\n",
      "frame 237 done\n",
      "frame 238 done\n",
      "frame 239 done\n",
      "frame 240 done\n",
      "frame 241 done\n",
      "frame 242 done\n",
      "frame 243 done\n",
      "frame 244 done\n",
      "frame 245 done\n",
      "frame 246 done\n",
      "frame 247 done\n",
      "frame 248 done\n",
      "frame 249 done\n",
      "frame 250 done\n",
      "frame 251 done\n",
      "frame 252 done\n",
      "frame 253 done\n",
      "frame 254 done\n",
      "frame 255 done\n",
      "frame 256 done\n",
      "frame 257 done\n",
      "frame 258 done\n",
      "frame 259 done\n",
      "frame 260 done\n",
      "frame 261 done\n",
      "frame 262 done\n",
      "frame 263 done\n",
      "frame 264 done\n",
      "frame 265 done\n",
      "frame 266 done\n",
      "frame 267 done\n",
      "frame 268 done\n",
      "frame 269 done\n",
      "frame 270 done\n",
      "frame 271 done\n",
      "frame 272 done\n",
      "frame 273 done\n",
      "frame 274 done\n",
      "frame 275 done\n",
      "frame 276 done\n",
      "frame 277 done\n",
      "frame 278 done\n",
      "frame 279 done\n",
      "frame 280 done\n",
      "frame 281 done\n",
      "frame 282 done\n",
      "frame 283 done\n",
      "frame 284 done\n",
      "frame 285 done\n",
      "frame 286 done\n",
      "frame 287 done\n",
      "frame 288 done\n",
      "frame 289 done\n",
      "frame 290 done\n",
      "frame 291 done\n",
      "frame 292 done\n",
      "frame 293 done\n",
      "frame 294 done\n",
      "frame 295 done\n",
      "frame 296 done\n",
      "frame 297 done\n",
      "frame 298 done\n",
      "frame 299 done\n",
      "frame 300 done\n",
      "frame 301 done\n",
      "frame 302 done\n",
      "frame 303 done\n",
      "frame 304 done\n",
      "frame 305 done\n",
      "frame 306 done\n",
      "frame 307 done\n",
      "frame 308 done\n",
      "frame 309 done\n",
      "frame 310 done\n",
      "frame 311 done\n",
      "frame 312 done\n",
      "frame 313 done\n",
      "frame 314 done\n",
      "frame 315 done\n",
      "frame 316 done\n",
      "frame 317 done\n",
      "frame 318 done\n",
      "frame 319 done\n",
      "frame 320 done\n",
      "frame 321 done\n",
      "frame 322 done\n",
      "frame 323 done\n",
      "frame 324 done\n",
      "frame 325 done\n",
      "frame 326 done\n",
      "frame 327 done\n",
      "frame 328 done\n",
      "frame 329 done\n",
      "frame 330 done\n",
      "frame 331 done\n",
      "frame 332 done\n",
      "frame 333 done\n",
      "frame 334 done\n",
      "frame 335 done\n",
      "frame 336 done\n",
      "frame 337 done\n",
      "frame 338 done\n",
      "frame 339 done\n",
      "frame 340 done\n",
      "frame 341 done\n",
      "frame 342 done\n",
      "frame 343 done\n",
      "frame 344 done\n",
      "frame 345 done\n",
      "frame 346 done\n",
      "frame 347 done\n",
      "frame 348 done\n",
      "frame 349 done\n",
      "frame 350 done\n",
      "frame 351 done\n",
      "frame 352 done\n",
      "frame 353 done\n",
      "frame 354 done\n",
      "frame 355 done\n",
      "frame 356 done\n",
      "frame 357 done\n",
      "frame 358 done\n",
      "frame 359 done\n",
      "frame 360 done\n",
      "frame 361 done\n",
      "frame 362 done\n",
      "frame 363 done\n",
      "frame 364 done\n",
      "frame 365 done\n",
      "frame 366 done\n",
      "frame 367 done\n",
      "frame 368 done\n",
      "frame 369 done\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "input_vid_path = '/mnt/e/data/vox2/videos/512/id00078/P0OU4bFhwCI/00227.mp4'  # Can be a directory or a single video file\n",
    "input_src_path = '/mnt/c/Users/mjh/Downloads/live_in/t4.jpg'\n",
    "input_audio_path = '/mnt/c/Users/mjh/Downloads/audio_id00078_P0OU4bFhwCI_00227.wav'\n",
    "\n",
    "# Read video frames\n",
    "all_frames, video_lengths = process_videos(input_vid_path)\n",
    "driving_rgb_lst = all_frames\n",
    "I_d_lst = prepare_videos_(driving_rgb_lst, device)\n",
    "I_d_lst = I_d_lst.unsqueeze(1)\n",
    "I_d_lst = I_d_lst[0 : video_lengths[0]]\n",
    "print(f\"Shape of driving video: {I_d_lst.shape}\")\n",
    "# read audio if exists\n",
    "audio_path = None\n",
    "if has_audio_stream(input_vid_path):\n",
    "    audio_path = extract_audio(input_vid_path)  # Extract audio from the video\n",
    "else:\n",
    "    audio_path = input_audio_path\n",
    "\n",
    "# Extract motion information\n",
    "template_dct = process_driving_video(I_d_lst)\n",
    "\n",
    "# Load source image\n",
    "img_rgb = load_image_rgb(input_src_path)\n",
    "source_rgb_lst = [img_rgb]\n",
    "\n",
    "source_lmk = cropper.calc_lmk_from_cropped_image(source_rgb_lst[0])\n",
    "img_crop_256x256 = cv2.resize(source_rgb_lst[0], (256, 256))  # force to resize to 256x256\n",
    "\n",
    "# extract the src implicit keypoint information\n",
    "I_s = prepare_source(img_crop_256x256)\n",
    "x_s_info = get_kp_info(I_s)\n",
    "x_c_s = x_s_info['kp']\n",
    "x_s = transform_keypoint(x_s_info)\n",
    "f_s = extract_feature_3d(I_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frontalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = template_dct['motion'][0]['R']\n",
    "# exp = template_dct['motion'][0]['exp']\n",
    "# t = template_dct['motion'][0]['t']\n",
    "# scale = template_dct['motion'][0]['scale']\n",
    "# # print dims\n",
    "# print(R.shape, exp.shape, t.shape, scale.shape)\n",
    "# # print flatten dims\n",
    "# print(R.flatten().shape, exp.flatten().shape, t.flatten().shape, scale.flatten().shape)\n",
    "# # print range\n",
    "# print(R.min(), R.max(), exp.min(), exp.max(), t.min(), t.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(n_frames):\n",
    "#     R = template_dct['motion'][i]['R']\n",
    "#     exp = template_dct['motion'][i]['exp']\n",
    "#     t = template_dct['motion'][i]['t']\n",
    "#     scale = template_dct['motion'][i]['scale']\n",
    "#     info = template_dct['x_i_info_lst'][i]\n",
    "#     roll, pitch, yaw = info['roll'], info['pitch'], info['yaw']\n",
    "\n",
    "#     new_R = get_rotation_matrix(pitch, yaw, roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant pose (roll, pitch, yaw): [  2.9246292   2.5950623 -20.104614 ]\n",
      "Median t: [-0.00798798  0.08911133  0.        ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def angular_distance(pose1, pose2):\n",
    "    diff = torch.abs(pose1 - pose2)\n",
    "    diff = torch.min(diff, 2*torch.pi - diff)\n",
    "    return torch.norm(diff)\n",
    "\n",
    "def find_dominant_pose(poses):\n",
    "    N = poses.shape[0]\n",
    "    total_distances = torch.zeros(N, device=poses.device)\n",
    "    for i in range(N):\n",
    "        distances = angular_distance(poses[i].unsqueeze(0), poses)\n",
    "        total_distances[i] = torch.sum(distances)\n",
    "    min_distance_index = torch.argmin(total_distances)\n",
    "    return poses[min_distance_index], min_distance_index\n",
    "\n",
    "# Prepare data\n",
    "n_frames = len(template_dct['motion'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Collect all poses and t values\n",
    "all_poses = torch.zeros(n_frames, 3, device=device)\n",
    "all_t = torch.zeros(n_frames, 3, device=device)\n",
    "\n",
    "for i in range(n_frames):\n",
    "    info = template_dct['x_i_info_lst'][i]\n",
    "    roll, pitch, yaw = info['roll'], info['pitch'], info['yaw']\n",
    "    all_poses[i] = torch.tensor([roll, pitch, yaw], device=device).squeeze()\n",
    "    all_t[i] = torch.tensor(template_dct['motion'][i]['t'], device=device)\n",
    "\n",
    "# Find dominant pose\n",
    "dominant_pose, _ = find_dominant_pose(all_poses)\n",
    "\n",
    "# Find median t\n",
    "median_t = torch.median(all_t, dim=0).values\n",
    "\n",
    "# Subtract dominant pose and median t from the sequence\n",
    "for i in range(n_frames):\n",
    "    # Update pose\n",
    "    template_dct['x_i_info_lst'][i]['roll'] = (all_poses[i, 0]  - 1 * dominant_pose[0]).unsqueeze(0)\n",
    "    template_dct['x_i_info_lst'][i]['pitch'] = (all_poses[i, 1] - 1 * dominant_pose[1]).unsqueeze(0)\n",
    "    template_dct['x_i_info_lst'][i]['yaw'] = (all_poses[i, 2]   - 1 * dominant_pose[2]).unsqueeze(0)\n",
    "\n",
    "    # Update t\n",
    "    template_dct['motion'][i]['t'] = (all_t[i] - median_t).cpu().numpy()\n",
    "\n",
    "    # Recalculate R with the updated pose\n",
    "    new_R = get_rotation_matrix(\n",
    "        template_dct['x_i_info_lst'][i]['pitch'],\n",
    "        template_dct['x_i_info_lst'][i]['yaw'],\n",
    "        template_dct['x_i_info_lst'][i]['roll']\n",
    "    )\n",
    "    template_dct['motion'][i]['R'] = new_R.cpu().numpy()\n",
    "\n",
    "print(f\"Dominant pose (roll, pitch, yaw): {dominant_pose.cpu().numpy()}\")\n",
    "print(f\"Median t: {median_t.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single frame retarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = template_dct['motion'][0]['R']\n",
    "# exp = template_dct['motion'][0]['exp']\n",
    "# t = template_dct['motion'][0]['t']\n",
    "# scale = template_dct['motion'][0]['scale']\n",
    "\n",
    "# scale_tensor = torch.tensor(scale, device=device)\n",
    "# R_tensor = torch.tensor(R, device=device)\n",
    "# exp_tensor = torch.tensor(exp, device=device)\n",
    "# t_tensor = torch.tensor(t, device=device)\n",
    "# print(scale_tensor.shape, R_tensor.shape, exp_tensor.shape, t_tensor.shape)\n",
    "\n",
    "# start = time.time()\n",
    "# x_d_i_new = scale_tensor * (x_c_s @ R_tensor + exp_tensor) + t_tensor\n",
    "\n",
    "# # x_d_i_new = scale * (x_c_s @ R + exp) + t\n",
    "# out = warp_decode(f_s, x_s, x_d_i_new)\n",
    "# # print(out)\n",
    "# # I_p_i = parse_output(out['out'])[0]\n",
    "# end_time = time.time() - start\n",
    "# print(f'warp_decode time: {end_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large chunk of frames generator. Performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video with audio saved to video_driven_output.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  WARNING: library configuration mismatch\n",
      "  avcodec     configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared --enable-version3 --disable-doc --disable-programs --enable-libaribb24 --enable-libopencore_amrnb --enable-libopencore_amrwb --enable-libtesseract --enable-libvo_amrwbenc --enable-libsmbclient\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'video_driven_no_audio.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:00:14.80, start: 0.000000, bitrate: 2994 kb/s\n",
      "  Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 512x512 [SAR 1:1 DAR 1:1], 2992 kb/s, 25 fps, 25 tbr, 12800 tbn, 25 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Guessed Channel Layout for Input Stream #1.0 : stereo\n",
      "Input #1, wav, from '/mnt/c/Users/mjh/Downloads/audio_id00078_P0OU4bFhwCI_00227.wav':\n",
      "  Metadata:\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:00:14.80, bitrate: 1411 kb/s\n",
      "  Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (copy)\n",
      "  Stream #1:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp4, to 'video_driven_output.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0(und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 512x512 [SAR 1:1 DAR 1:1], q=2-31, 2992 kb/s, 25 fps, 25 tbr, 12800 tbn, 12800 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 aac\n",
      "frame=  370 fps=0.0 q=-1.0 Lsize=    5650kB time=00:00:14.76 bitrate=3135.6kbits/s speed= 129x    \n",
      "video:5407kB audio:232kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.190813%\n",
      "[aac @ 0x55cb6b879140] Qavg: 341.617\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def generate_frames(template_dct, x_c_s, f_s, x_s, device, show_cv=False):\n",
    "    total_frames = len(template_dct['motion'])\n",
    "    if show_cv:\n",
    "        cv2.namedWindow('Processed Frame', cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('Processed Frame', 512, 512)  # Adjust size as needed\n",
    "\n",
    "    for frame_index in range(total_frames):\n",
    "        # Get motion data for the current frame\n",
    "        R = template_dct['motion'][frame_index]['R']\n",
    "        exp = template_dct['motion'][frame_index]['exp']\n",
    "        t = template_dct['motion'][frame_index]['t']\n",
    "        scale = template_dct['motion'][frame_index]['scale']\n",
    "\n",
    "        # Convert to tensors\n",
    "        scale_tensor = torch.tensor(scale, device=device)\n",
    "        R_tensor = torch.tensor(R, device=device)\n",
    "        exp_tensor = torch.tensor(exp, device=device)\n",
    "        t_tensor = torch.tensor(t, device=device)\n",
    "\n",
    "        # Process the frame\n",
    "        x_d_i_new = scale_tensor * (x_c_s @ R_tensor + exp_tensor) + t_tensor\n",
    "        out = warp_decode(f_s, x_s, x_d_i_new)\n",
    "\n",
    "        # Convert tensor to numpy array and rescale to 0-255 range\n",
    "        img_np = (out['out'][0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "        if show_cv:\n",
    "            # Convert from RGB to BGR for cv2\n",
    "            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "            # Display the frame\n",
    "            cv2.imshow('Processed Frame', img_bgr)\n",
    "            # Print progress\n",
    "            print(f\"Processed frame {frame_index+1}/{total_frames}\")\n",
    "            # Wait for a short time and check for 'q' key to quit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        yield frame_index, img_np\n",
    "\n",
    "    if show_cv:\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def display_frames(frame_generator, display_option='opencv'):\n",
    "    if display_option == 'opencv':\n",
    "        cv2.namedWindow('Processed Frame', cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('Processed Frame', 512, 512)  # Adjust size as needed\n",
    "\n",
    "    for frame_index, img_np in frame_generator:\n",
    "        if display_option == 'opencv':\n",
    "            # Convert from RGB to BGR for cv2\n",
    "            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Display the frame\n",
    "            cv2.imshow('Processed Frame', img_bgr)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Processed frame {frame_index+1}/{len(template_dct['motion'])}\")\n",
    "\n",
    "            # Wait for a short time and check for 'q' key to quit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # Optional: add a small delay to make the display more visible\n",
    "        time.sleep(0.01)  # Adjust as needed\n",
    "\n",
    "    if display_option == 'opencv':\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "def save_video(frame_generator, audio_path, output_video='video_driven_output.mp4', fps=25):\n",
    "    assert os.path.exists(audio_path), f\"Audio file not found: {audio_path}\"\n",
    "    output_no_audio_path = 'video_driven_no_audio.mp4'\n",
    "\n",
    "    # Remove the files if they exist\n",
    "    if os.path.exists(output_no_audio_path):\n",
    "        os.remove(output_no_audio_path)\n",
    "    if os.path.exists(output_video):\n",
    "        os.remove(output_video)\n",
    "\n",
    "    # Get the first frame to determine video dimensions\n",
    "    _, first_frame = next(frame_generator)\n",
    "    height, width, layers = first_frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_no_audio_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write the first frame\n",
    "    video.write(cv2.cvtColor(first_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Write the rest of the frames\n",
    "    for _, frame in frame_generator:\n",
    "        video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    video.release()\n",
    "\n",
    "    # Add audio to the video using ffmpeg\n",
    "    ffmpeg_cmd = [\n",
    "        'ffmpeg',\n",
    "        '-i', output_no_audio_path,\n",
    "        '-i', audio_path,\n",
    "        '-c:v', 'copy',\n",
    "        '-c:a', 'aac',\n",
    "        '-shortest',\n",
    "        output_video\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(ffmpeg_cmd, check=True)\n",
    "        os.remove(output_no_audio_path)\n",
    "        print(f\"Video with audio saved to {output_video}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error adding audio to video: {e}\")\n",
    "\n",
    "# Generate frames and save video\n",
    "frame_generator = generate_frames(template_dct, x_c_s, f_s, x_s, device)\n",
    "save_video(frame_generator, audio_path)  # Assuming audio_path is defined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "live_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
