{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile\n",
    "and initialize args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import contextlib\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "import tyro\n",
    "import subprocess\n",
    "from rich.progress import track\n",
    "import torchvision\n",
    "import cv2\n",
    "import threading\n",
    "import queue\n",
    "import torchvision.transforms as transforms\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import imageio\n",
    "\n",
    "from src.config.argument_config import ArgumentConfig\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.config.crop_config import CropConfig\n",
    "\n",
    "def partial_fields(target_class, kwargs):\n",
    "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
    "\n",
    "args = ArgumentConfig()\n",
    "inference_cfg = partial_fields(InferenceConfig, args.__dict__)\n",
    "crop_cfg = partial_fields(CropConfig, args.__dict__)\n",
    "# print(\"inference_cfg: \", inference_cfg)\n",
    "# print(\"crop_cfg: \", crop_cfg)\n",
    "device = 'cuda'\n",
    "print(\"Compile complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize util functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helper import load_model, concat_feat\n",
    "from src.utils.camera import headpose_pred_to_degree, get_rotation_matrix\n",
    "from src.utils.retargeting_utils import calc_eye_close_ratio, calc_lip_close_ratio\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.utils.cropper import Cropper\n",
    "from src.utils.camera import get_rotation_matrix\n",
    "from src.utils.video import images2video, concat_frames, get_fps, add_audio_to_video, has_audio_stream\n",
    "from src.utils.crop import _transform_img, prepare_paste_back, paste_back\n",
    "from src.utils.io import load_image_rgb, load_video, resize_to_limit, dump, load\n",
    "from src.utils.helper import mkdir, basename, dct2device, is_video, is_template, remove_suffix, is_image\n",
    "from src.utils.filter import smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = yaml.load(open(inference_cfg.models_config, 'r'), Loader=yaml.SafeLoader)\n",
    "# init F\n",
    "appearance_feature_extractor = load_model(inference_cfg.checkpoint_F, model_config, device, 'appearance_feature_extractor')\n",
    "# init M\n",
    "motion_extractor = load_model(inference_cfg.checkpoint_M, model_config, device, 'motion_extractor')\n",
    "# init W\n",
    "warping_module = load_model(inference_cfg.checkpoint_W, model_config, device, 'warping_module')\n",
    "# init G\n",
    "spade_generator = load_model(inference_cfg.checkpoint_G, model_config, device, 'spade_generator')\n",
    "# init S and R\n",
    "if inference_cfg.checkpoint_S is not None and os.path.exists(inference_cfg.checkpoint_S):\n",
    "    stitching_retargeting_module = load_model(inference_cfg.checkpoint_S, model_config, device, 'stitching_retargeting_module')\n",
    "else:\n",
    "    stitching_retargeting_module = None\n",
    "\n",
    "cropper = Cropper(crop_cfg=crop_cfg, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_distance_ratio(lmk: np.ndarray, idx1: int, idx2: int, idx3: int, idx4: int, eps: float = 1e-6) -> np.ndarray:\n",
    "    return (np.linalg.norm(lmk[:, idx1] - lmk[:, idx2], axis=1, keepdims=True) /\n",
    "            (np.linalg.norm(lmk[:, idx3] - lmk[:, idx4], axis=1, keepdims=True) + eps))\n",
    "\n",
    "\n",
    "def calc_eye_close_ratio(lmk: np.ndarray, target_eye_ratio: np.ndarray = None) -> np.ndarray:\n",
    "    lefteye_close_ratio = calculate_distance_ratio(lmk, 6, 18, 0, 12)\n",
    "    righteye_close_ratio = calculate_distance_ratio(lmk, 30, 42, 24, 36)\n",
    "    if target_eye_ratio is not None:\n",
    "        return np.concatenate([lefteye_close_ratio, righteye_close_ratio, target_eye_ratio], axis=1)\n",
    "    else:\n",
    "        return np.concatenate([lefteye_close_ratio, righteye_close_ratio], axis=1)\n",
    "\n",
    "\n",
    "def calc_lip_close_ratio(lmk: np.ndarray) -> np.ndarray:\n",
    "    return calculate_distance_ratio(lmk, 90, 102, 48, 66)\n",
    "\n",
    "def calc_ratio(lmk_lst):\n",
    "    input_eye_ratio_lst = []\n",
    "    input_lip_ratio_lst = []\n",
    "    for lmk in lmk_lst:\n",
    "        # for eyes retargeting\n",
    "        input_eye_ratio_lst.append(calc_eye_close_ratio(lmk[None]))\n",
    "        # for lip retargeting\n",
    "        input_lip_ratio_lst.append(calc_lip_close_ratio(lmk[None]))\n",
    "    return input_eye_ratio_lst, input_lip_ratio_lst\n",
    "\n",
    "def prepare_videos(imgs, device) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    imgs: NxBxHxWx3, uint8\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, list):\n",
    "        _imgs = np.array(imgs)[..., np.newaxis]  # TxHxWx3x1\n",
    "    elif isinstance(imgs, np.ndarray):\n",
    "        _imgs = imgs\n",
    "    else:\n",
    "        raise ValueError(f'imgs type error: {type(imgs)}')\n",
    "\n",
    "    y = _imgs.astype(np.float32) / 255.\n",
    "    y = np.clip(y, 0, 1)  # clip to 0~1\n",
    "    y = torch.from_numpy(y).permute(0, 4, 3, 1, 2)  # TxHxWx3x1 -> Tx1x3xHxW\n",
    "    y = y.to(device)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading single vid or dir of vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_videos_(imgs, device):\n",
    "    \"\"\" construct the input as standard\n",
    "    imgs: NxHxWx3, uint8\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, list):\n",
    "        _imgs = np.array(imgs)\n",
    "    elif isinstance(imgs, np.ndarray):\n",
    "        _imgs = imgs\n",
    "    else:\n",
    "        raise ValueError(f'imgs type error: {type(imgs)}')\n",
    "\n",
    "    # y = _imgs.astype(np.float32) / 255.\n",
    "    y = _imgs\n",
    "    y = torch.from_numpy(y).permute(0, 3, 1, 2)  # NxHxWx3 -> Nx3xHxW\n",
    "    y = y.to(device)\n",
    "    y = y / 255.\n",
    "    y = torch.clamp(y, 0, 1)\n",
    "\n",
    "    return y\n",
    "\n",
    "def read_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (256, 256))  # Resize to 256x256\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return video_path, frames\n",
    "\n",
    "def read_multiple_videos(video_paths, num_threads=4):\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        results = list(executor.map(read_video_frames, video_paths))\n",
    "    return results\n",
    "\n",
    "def process_videos(input_path, num_threads=4):\n",
    "    if os.path.isdir(input_path):\n",
    "        video_paths = sorted(glob.glob(os.path.join(input_path, '*.mp4')))  # Sort to ensure consistent order\n",
    "        print(f\"Found {len(video_paths)} video files.\")\n",
    "        video_frames = read_multiple_videos(video_paths, num_threads)\n",
    "    else:\n",
    "        print(f\"Processing single video file: {input_path}\")\n",
    "        video_frames = [read_video_frames(input_path)]\n",
    "\n",
    "    all_frames = []\n",
    "    total_frames = 0\n",
    "    video_lengths = []\n",
    "\n",
    "    for video_path, frames in video_frames:\n",
    "        all_frames.extend(frames)\n",
    "        frame_count = len(frames)\n",
    "        total_frames += frame_count\n",
    "        video_lengths.append(frame_count)\n",
    "        print(f\"Processed video: {video_path}, frames: {frame_count}\")\n",
    "\n",
    "    print(f\"\\nTotal frames across all videos: {total_frames}\")\n",
    "    print(f\"Video lengths: {video_lengths}\")\n",
    "\n",
    "    # Convert to numpy array\n",
    "    all_frames = np.array(all_frames)\n",
    "\n",
    "    print(f\"Shape of concatenated array: {all_frames.shape}\")\n",
    "    return all_frames, video_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def get_audio_path(video_path):\n",
    "    audio_filename = f\"extracted_audio_{os.path.basename(video_path).split('.')[0]}.wav\"\n",
    "    audio_path = os.path.join(os.getcwd(), audio_filename)\n",
    "    return audio_path\n",
    "\n",
    "def extract_audio(video_path):\n",
    "    # Generate a unique filename for the audio in the current directory\n",
    "    audio_filename = f\"extracted_audio_{os.path.basename(video_path).split('.')[0]}.wav\"\n",
    "    audio_path = os.path.join(os.getcwd(), audio_filename)\n",
    "\n",
    "    # Use ffmpeg to extract audio\n",
    "    try:\n",
    "        subprocess.run(['ffmpeg', '-i', video_path, '-q:a', '0', '-map', 'a', audio_path], check=True)\n",
    "        print(f\"Audio extracted successfully: {audio_path}\")\n",
    "        return audio_path\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error extracting audio: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# video_path = \"path/to/your/video.mp4\"\n",
    "# audio_path = extract_audio(video_path)\n",
    "# if audio_path:\n",
    "#     print(f\"Audio saved to: {audio_path}\")\n",
    "# else:\n",
    "#     print(\"Failed to extract audio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motion Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kp_info(x: torch.Tensor, **kwargs) -> dict:\n",
    "    \"\"\" get the implicit keypoint information\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    flag_refine_info: whether to trandform the pose to degrees and the dimention of the reshape\n",
    "    return: A dict contains keys: 'pitch', 'yaw', 'roll', 't', 'exp', 'scale', 'kp'\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        kp_info = motion_extractor(x)\n",
    "\n",
    "        if inference_cfg.flag_use_half_precision:\n",
    "            # float the dict\n",
    "            for k, v in kp_info.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    kp_info[k] = v.float()\n",
    "\n",
    "    flag_refine_info: bool = kwargs.get('flag_refine_info', True)\n",
    "    if flag_refine_info:\n",
    "        bs = kp_info['kp'].shape[0]\n",
    "        kp_info['pitch'] = headpose_pred_to_degree(kp_info['pitch'])[:, None]  # Bx1\n",
    "        kp_info['yaw'] = headpose_pred_to_degree(kp_info['yaw'])[:, None]  # Bx1\n",
    "        kp_info['roll'] = headpose_pred_to_degree(kp_info['roll'])[:, None]  # Bx1\n",
    "        kp_info['kp'] = kp_info['kp'].reshape(bs, -1, 3)  # BxNx3\n",
    "        kp_info['exp'] = kp_info['exp'].reshape(bs, -1, 3)  # BxNx3\n",
    "\n",
    "    return kp_info\n",
    "\n",
    "def process_driving_video(I_d_lst):\n",
    "    n_frames = I_d_lst.shape[0]\n",
    "    template_dct = {\n",
    "        'n_frames': n_frames,\n",
    "        'output_fps': 25,\n",
    "        'motion': [],\n",
    "        'c_d_eyes_lst': [],\n",
    "        'c_d_lip_lst': [],\n",
    "        'x_i_info_lst': [],\n",
    "    }\n",
    "\n",
    "    for i in range(n_frames):\n",
    "        # collect s, R, δ and t for inference\n",
    "        I_i = I_d_lst[i]\n",
    "        x_i_info = get_kp_info(I_i)\n",
    "        R_i = get_rotation_matrix(x_i_info['pitch'], x_i_info['yaw'], x_i_info['roll'])\n",
    "\n",
    "        item_dct = {\n",
    "            'scale': x_i_info['scale'].cpu().numpy().astype(np.float32),\n",
    "            'R': R_i.cpu().numpy().astype(np.float32),\n",
    "            'exp': x_i_info['exp'].cpu().numpy().astype(np.float32),\n",
    "            't': x_i_info['t'].cpu().numpy().astype(np.float32),\n",
    "        }\n",
    "\n",
    "        template_dct['motion'].append(item_dct)\n",
    "\n",
    "        # c_eyes = c_d_eyes_lst[i].astype(np.float32)\n",
    "        # template_dct['c_d_eyes_lst'].append(c_eyes)\n",
    "\n",
    "        # c_lip = c_d_lip_lst[i].astype(np.float32)\n",
    "        # template_dct['c_d_lip_lst'].append(c_lip)\n",
    "\n",
    "        template_dct['x_i_info_lst'].append(x_i_info)\n",
    "        print(f'frame {i} done')\n",
    "\n",
    "    return template_dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source image extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_source(img: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    img: HxWx3, uint8, 256x256\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    x = img.copy()\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        x = x[np.newaxis].astype(np.float32) / 255.  # HxWx3 -> 1xHxWx3, normalized to 0~1\n",
    "    elif x.ndim == 4:\n",
    "        x = x.astype(np.float32) / 255.  # BxHxWx3, normalized to 0~1\n",
    "    else:\n",
    "        raise ValueError(f'img ndim should be 3 or 4: {x.ndim}')\n",
    "    x = np.clip(x, 0, 1)  # clip to 0~1\n",
    "    x = torch.from_numpy(x).permute(0, 3, 1, 2)  # 1xHxWx3 -> 1x3xHxW\n",
    "    x = x.to(device)\n",
    "    return x\n",
    "\n",
    "def warp_decode(feature_3d: torch.Tensor, kp_source: torch.Tensor, kp_driving: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the image after the warping of the implicit keypoints\n",
    "    feature_3d: Bx32x16x64x64, feature volume\n",
    "    kp_source: BxNx3\n",
    "    kp_driving: BxNx3\n",
    "    \"\"\"\n",
    "    # The line 18 in Algorithm 1: D(W(f_s; x_s, x′_d,i)）\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        # get decoder input\n",
    "        ret_dct = warping_module(feature_3d, kp_source=kp_source, kp_driving=kp_driving)\n",
    "        # decode\n",
    "        ret_dct['out'] = spade_generator(feature=ret_dct['out'])\n",
    "\n",
    "    return ret_dct\n",
    "\n",
    "def extract_feature_3d( x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the appearance feature of the image by F\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        feature_3d = appearance_feature_extractor(x)\n",
    "\n",
    "    return feature_3d.float()\n",
    "\n",
    "def transform_keypoint(kp_info: dict):\n",
    "    \"\"\"\n",
    "    transform the implicit keypoints with the pose, shift, and expression deformation\n",
    "    kp: BxNx3\n",
    "    \"\"\"\n",
    "    kp = kp_info['kp']    # (bs, k, 3)\n",
    "    pitch, yaw, roll = kp_info['pitch'], kp_info['yaw'], kp_info['roll']\n",
    "\n",
    "    t, exp = kp_info['t'], kp_info['exp']\n",
    "    scale = kp_info['scale']\n",
    "\n",
    "    pitch = headpose_pred_to_degree(pitch)\n",
    "    yaw = headpose_pred_to_degree(yaw)\n",
    "    roll = headpose_pred_to_degree(roll)\n",
    "\n",
    "    bs = kp.shape[0]\n",
    "    if kp.ndim == 2:\n",
    "        num_kp = kp.shape[1] // 3  # Bx(num_kpx3)\n",
    "    else:\n",
    "        num_kp = kp.shape[1]  # Bxnum_kpx3\n",
    "\n",
    "    rot_mat = get_rotation_matrix(pitch, yaw, roll)    # (bs, 3, 3)\n",
    "\n",
    "    # Eqn.2: s * (R * x_c,s + exp) + t\n",
    "    kp_transformed = kp.view(bs, num_kp, 3) @ rot_mat + exp.view(bs, num_kp, 3)\n",
    "    kp_transformed *= scale[..., None]  # (bs, k, 3) * (bs, 1, 1) = (bs, k, 3)\n",
    "    kp_transformed[:, :, 0:2] += t[:, None, 0:2]  # remove z, only apply tx ty\n",
    "\n",
    "    return kp_transformed\n",
    "\n",
    "def parse_output(out: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\" construct the output as standard\n",
    "    return: 1xHxWx3, uint8\n",
    "    \"\"\"\n",
    "    out = np.transpose(out.data.cpu().numpy(), [0, 2, 3, 1])  # 1x3xHxW -> 1xHxWx3\n",
    "    out = np.clip(out, 0, 1)  # clip to 0~1\n",
    "    out = np.clip(out * 255, 0, 255).astype(np.uint8)  # 0~1 -> 0~255\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "input_vid_path = '/mnt/e/data/diffposetalk_data/TFHP_raw/crop/TH_00203/000.mp4'  # Can be a directory or a single video file\n",
    "input_src_path = '/mnt/c/Users/mjh/Downloads/live_in/t3.jpg'\n",
    "input_audio_path = get_audio_path(input_vid_path)\n",
    "# input_audio_path = '/mnt/c/Users/mjh/Downloads/live_in/i5.wav'\n",
    "\n",
    "# Read video frames\n",
    "all_frames, video_lengths = process_videos(input_vid_path)\n",
    "driving_rgb_lst = all_frames\n",
    "I_d_lst = prepare_videos_(driving_rgb_lst, device)\n",
    "I_d_lst = I_d_lst.unsqueeze(1)\n",
    "I_d_lst = I_d_lst[0 : video_lengths[0]]\n",
    "print(f\"Shape of driving video: {I_d_lst.shape}\")\n",
    "# read audio if exists\n",
    "audio_path = None\n",
    "if has_audio_stream(input_vid_path) and not os.path.exists(input_audio_path):\n",
    "    audio_path = extract_audio(input_vid_path)  # Extract audio from the video\n",
    "elif os.path.exists(input_audio_path):\n",
    "    audio_path = input_audio_path\n",
    "else:\n",
    "    raise ValueError(\"No audio stream found in the video and no audio file provided.\")\n",
    "\n",
    "# Extract motion information\n",
    "template_dct = process_driving_video(I_d_lst)\n",
    "\n",
    "# Load source image\n",
    "img_rgb = load_image_rgb(input_src_path)\n",
    "source_rgb_lst = [img_rgb]\n",
    "\n",
    "source_lmk = cropper.calc_lmk_from_cropped_image(source_rgb_lst[0])\n",
    "img_crop_256x256 = cv2.resize(source_rgb_lst[0], (256, 256))  # force to resize to 256x256\n",
    "\n",
    "# extract the src implicit keypoint information\n",
    "I_s = prepare_source(img_crop_256x256)\n",
    "x_s_info = get_kp_info(I_s)\n",
    "x_c_s = x_s_info['kp']\n",
    "x_s = transform_keypoint(x_s_info)\n",
    "f_s = extract_feature_3d(I_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frontalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = template_dct['motion'][0]['R']\n",
    "# exp = template_dct['motion'][0]['exp']\n",
    "# t = template_dct['motion'][0]['t']\n",
    "# scale = template_dct['motion'][0]['scale']\n",
    "# # print dims\n",
    "# print(R.shape, exp.shape, t.shape, scale.shape)\n",
    "# # print flatten dims\n",
    "# print(R.flatten().shape, exp.flatten().shape, t.flatten().shape, scale.flatten().shape)\n",
    "# # print range\n",
    "# print(R.min(), R.max(), exp.min(), exp.max(), t.min(), t.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(n_frames):\n",
    "#     R = template_dct['motion'][i]['R']\n",
    "#     exp = template_dct['motion'][i]['exp']\n",
    "#     t = template_dct['motion'][i]['t']\n",
    "#     scale = template_dct['motion'][i]['scale']\n",
    "#     info = template_dct['x_i_info_lst'][i]\n",
    "#     roll, pitch, yaw = info['roll'], info['pitch'], info['yaw']\n",
    "\n",
    "#     new_R = get_rotation_matrix(pitch, yaw, roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def angular_distance(pose1, pose2):\n",
    "    diff = torch.abs(pose1 - pose2)\n",
    "    diff = torch.min(diff, 2*torch.pi - diff)\n",
    "    return torch.norm(diff)\n",
    "\n",
    "def find_dominant_pose(poses):\n",
    "    N = poses.shape[0]\n",
    "    total_distances = torch.zeros(N, device=poses.device)\n",
    "    for i in range(N):\n",
    "        distances = angular_distance(poses[i].unsqueeze(0), poses)\n",
    "        total_distances[i] = torch.sum(distances)\n",
    "    min_distance_index = torch.argmin(total_distances)\n",
    "    return poses[min_distance_index], min_distance_index\n",
    "\n",
    "# Prepare data\n",
    "n_frames = len(template_dct['motion'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Collect all poses and t values\n",
    "all_poses = torch.zeros(n_frames, 3, device=device)\n",
    "all_t = torch.zeros(n_frames, 3, device=device)\n",
    "\n",
    "for i in range(n_frames):\n",
    "    info = template_dct['x_i_info_lst'][i]\n",
    "    roll, pitch, yaw = info['roll'], info['pitch'], info['yaw']\n",
    "    all_poses[i] = torch.tensor([roll, pitch, yaw], device=device).squeeze()\n",
    "    all_t[i] = torch.tensor(template_dct['motion'][i]['t'], device=device)\n",
    "\n",
    "# Find dominant pose\n",
    "dominant_pose, _ = find_dominant_pose(all_poses)\n",
    "\n",
    "# Find median t\n",
    "median_t = torch.median(all_t, dim=0).values\n",
    "\n",
    "# Subtract dominant pose and median t from the sequence\n",
    "for i in range(n_frames):\n",
    "    # Update pose\n",
    "    template_dct['x_i_info_lst'][i]['roll'] = (all_poses[i, 0]  - 1 * dominant_pose[0]).unsqueeze(0)\n",
    "    template_dct['x_i_info_lst'][i]['pitch'] = (all_poses[i, 1] - 1 * dominant_pose[1]).unsqueeze(0)\n",
    "    template_dct['x_i_info_lst'][i]['yaw'] = (all_poses[i, 2]   - 1 * dominant_pose[2]).unsqueeze(0)\n",
    "\n",
    "    # Update t\n",
    "    template_dct['motion'][i]['t'] = (all_t[i] - median_t).cpu().numpy()\n",
    "\n",
    "    # Recalculate R with the updated pose\n",
    "    new_R = get_rotation_matrix(\n",
    "        template_dct['x_i_info_lst'][i]['pitch'],\n",
    "        template_dct['x_i_info_lst'][i]['yaw'],\n",
    "        template_dct['x_i_info_lst'][i]['roll']\n",
    "    )\n",
    "    template_dct['motion'][i]['R'] = new_R.cpu().numpy()\n",
    "\n",
    "print(f\"Dominant pose (roll, pitch, yaw): {dominant_pose.cpu().numpy()}\")\n",
    "print(f\"Median t: {median_t.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single frame retarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = template_dct['motion'][0]['R']\n",
    "# exp = template_dct['motion'][0]['exp']\n",
    "# t = template_dct['motion'][0]['t']\n",
    "# scale = template_dct['motion'][0]['scale']\n",
    "\n",
    "# scale_tensor = torch.tensor(scale, device=device)\n",
    "# R_tensor = torch.tensor(R, device=device)\n",
    "# exp_tensor = torch.tensor(exp, device=device)\n",
    "# t_tensor = torch.tensor(t, device=device)\n",
    "# print(scale_tensor.shape, R_tensor.shape, exp_tensor.shape, t_tensor.shape)\n",
    "\n",
    "# start = time.time()\n",
    "# x_d_i_new = scale_tensor * (x_c_s @ R_tensor + exp_tensor) + t_tensor\n",
    "\n",
    "# # x_d_i_new = scale * (x_c_s @ R + exp) + t\n",
    "# out = warp_decode(f_s, x_s, x_d_i_new)\n",
    "# # print(out)\n",
    "# # I_p_i = parse_output(out['out'])[0]\n",
    "# end_time = time.time() - start\n",
    "# print(f'warp_decode time: {end_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large chunk of frames generator. Performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dct['motion'][0]['exp'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape exp to (63, num_frames) and calculate average across frames\n",
    "exp_reshaped = torch.tensor([frame['exp'] for frame in template_dct['motion']]).T.to(device)\n",
    "exp_reshaped = exp_reshaped.permute(3, 2, 1, 0).unsqueeze(1).reshape(-1, 63)\n",
    "print(exp_reshaped.shape)\n",
    "# Calculate average of each feature in the last dimension\n",
    "exp_avg = torch.mean(exp_reshaped, dim=0)\n",
    "\n",
    "# Print the average for each feature\n",
    "print(\"Average of each feature, shape: \", exp_avg.shape)\n",
    "for i, avg in enumerate(exp_avg):\n",
    "    print(f\"Feature {i}: {avg.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "correct_indices = [\n",
    "     4, 6, 7, 22, 33, 34, 40, 43, 45, 46, 48, 51, 52, 53, 57, 58, 59, 60, 61, 62\n",
    "] # deleted 49,\n",
    "incorrect_indices = [i for i in range(63) if i not in correct_indices]\n",
    "\n",
    "bool_mask = torch.zeros(63, device=device)\n",
    "bool_mask[correct_indices] = True\n",
    "correct_indices = torch.tensor(correct_indices, device=device)\n",
    "incorrect_indices = torch.tensor(incorrect_indices, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def generate_frames(template_dct, x_c_s, f_s, x_s, device, show_cv=False):\n",
    "    total_frames = len(template_dct['motion'])\n",
    "    if show_cv:\n",
    "        cv2.namedWindow('Processed Frame', cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('Processed Frame', 512, 512)  # Adjust size as needed\n",
    "\n",
    "    # useful cache\n",
    "    t_identity = torch.zeros((1, 3), dtype=torch.float32, device=device)\n",
    "    pitch_identity = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "    yaw_identity = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "    roll_identity = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "    scale_identity = torch.ones((1), dtype=torch.float32, device=device) * 1.3\n",
    "    use_identity_pose = False\n",
    "    # mask to use\n",
    "    # bool_mask, correct_indices, incorrect_indices = get_latent_mask()\n",
    "    for frame_index in range(total_frames):\n",
    "        exp = template_dct['motion'][frame_index]['exp']\n",
    "\n",
    "        if not use_identity_pose:\n",
    "            R = template_dct['motion'][frame_index]['R']\n",
    "            t = template_dct['motion'][frame_index]['t']\n",
    "            # scale = template_dct['motion'][frame_index]['scale'] * 1.3\n",
    "            scale = scale_identity\n",
    "        else:\n",
    "            R = get_rotation_matrix(pitch_identity, yaw_identity, roll_identity)\n",
    "            t = t_identity\n",
    "            scale = scale_identity\n",
    "\n",
    "        # Convert to tensors\n",
    "        scale_tensor = torch.tensor(scale, device=device)\n",
    "        R_tensor = torch.tensor(R, device=device)\n",
    "        exp_tensor = torch.tensor(exp, device=device)\n",
    "        exp_tensor = exp_tensor.reshape(-1, 63)\n",
    "        # exp_tensor[:, incorrect_indices] = 0\n",
    "        exp_tensor = exp_tensor.reshape(-1, 21, 3)\n",
    "        t_tensor = torch.tensor(t, device=device)\n",
    "\n",
    "        # Process the frame\n",
    "        x_d_i_new = scale_tensor * (x_c_s @ R_tensor + exp_tensor) + t_tensor\n",
    "        out = warp_decode(f_s, x_s, x_d_i_new)\n",
    "\n",
    "        # Convert tensor to numpy array and rescale to 0-255 range\n",
    "        img_np = (out['out'][0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "        if show_cv:\n",
    "            # Convert from RGB to BGR for cv2\n",
    "            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "            # Display the frame\n",
    "            cv2.imshow('Processed Frame', img_bgr)\n",
    "            # Print progress\n",
    "            print(f\"Processed frame {frame_index+1}/{total_frames}\")\n",
    "            # Wait for a short time and check for 'q' key to quit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        yield frame_index, img_np\n",
    "\n",
    "    if show_cv:\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def display_frames(frame_generator, display_option='opencv'):\n",
    "    if display_option == 'opencv':\n",
    "        cv2.namedWindow('Processed Frame', cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('Processed Frame', 512, 512)  # Adjust size as needed\n",
    "\n",
    "    for frame_index, img_np in frame_generator:\n",
    "        if display_option == 'opencv':\n",
    "            # Convert from RGB to BGR for cv2\n",
    "            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Display the frame\n",
    "            cv2.imshow('Processed Frame', img_bgr)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Processed frame {frame_index+1}/{len(template_dct['motion'])}\")\n",
    "\n",
    "            # Wait for a short time and check for 'q' key to quit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # Optional: add a small delay to make the display more visible\n",
    "        time.sleep(0.01)  # Adjust as needed\n",
    "\n",
    "    if display_option == 'opencv':\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "def save_video(frame_generator, audio_path, output_video='video_driven_output.mp4', fps=25):\n",
    "    assert os.path.exists(audio_path), f\"Audio file not found: {audio_path}\"\n",
    "    output_no_audio_path = 'video_driven_no_audio.mp4'\n",
    "\n",
    "    # Remove the files if they exist\n",
    "    if os.path.exists(output_no_audio_path):\n",
    "        os.remove(output_no_audio_path)\n",
    "    if os.path.exists(output_video):\n",
    "        os.remove(output_video)\n",
    "\n",
    "    # Get the first frame to determine video dimensions\n",
    "    _, first_frame = next(frame_generator)\n",
    "    height, width, layers = first_frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_no_audio_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write the first frame\n",
    "    video.write(cv2.cvtColor(first_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Write the rest of the frames\n",
    "    for _, frame in frame_generator:\n",
    "        video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    video.release()\n",
    "\n",
    "    # Add audio to the video using ffmpeg\n",
    "    ffmpeg_cmd = [\n",
    "        'ffmpeg',\n",
    "        '-i', output_no_audio_path,\n",
    "        '-i', audio_path,\n",
    "        '-c:v', 'copy',\n",
    "        '-c:a', 'aac',\n",
    "        '-shortest',\n",
    "        output_video\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(ffmpeg_cmd, check=True)\n",
    "        os.remove(output_no_audio_path)\n",
    "        print(f\"Video with audio saved to {output_video}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error adding audio to video: {e}\")\n",
    "\n",
    "# Generate frames and save video\n",
    "frame_generator = generate_frames(template_dct, x_c_s, f_s, x_s, device)\n",
    "save_video(frame_generator, audio_path)  # Assuming audio_path is defined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "live_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
