{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\fp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\JackeyTY\\AppData\\Local\\Temp\\ipykernel_124328\\1826022602.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "import glob\n",
    "import tyro\n",
    "from time import sleep\n",
    "import imageio\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "from rich.progress import track\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "import cv2\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "torchaudio.set_audio_backend(\"soundfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live Portrait\n",
    "from src.config.argument_config import ArgumentConfig\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.config.crop_config import CropConfig\n",
    "from src.utils.helper import load_model, concat_feat\n",
    "from src.utils.camera import headpose_pred_to_degree, get_rotation_matrix\n",
    "from src.utils.retargeting_utils import calc_eye_close_ratio, calc_lip_close_ratio\n",
    "from src.utils.cropper import Cropper\n",
    "from src.utils.video import images2video, concat_frames, get_fps, add_audio_to_video, has_audio_stream\n",
    "from src.utils.crop import _transform_img, prepare_paste_back, paste_back\n",
    "from src.utils.io import load_image_rgb, load_video, resize_to_limit, dump, load\n",
    "from src.utils.helper import mkdir, basename, dct2device, is_video, is_template, remove_suffix, is_image\n",
    "from src.utils.filter import smooth\n",
    "\n",
    "# DiT\n",
    "from audio_dit.inference import InferenceManager, get_model\n",
    "from audio_dit.dataset import load_and_process_pair\n",
    "from audio_dit.dataset import process_motion_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wave2vec\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\"\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "FRAME_RATE = 25\n",
    "SECTION_LENGTH = 3\n",
    "OVERLAP = 10\n",
    "\n",
    "DB_ROOT = 'vox2-audio-tx'\n",
    "LOG = 'log'\n",
    "AUDIO = 'audio/audio'\n",
    "OUTPUT_DIR = 'audio_encoder_output'\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "prev_context_len = 67\n",
    "gen_len_per_window = 8\n",
    "\n",
    "# DiT model\n",
    "config_path = 'D:/Projects/Upenn_CIS_5650/final-project/config/config.json'\n",
    "weight_path = 'D:/Projects/Upenn_CIS_5650/final-project/config/model.pth'\n",
    "motion_mean_exp_path = 'D:/Projects/Upenn_CIS_5650/final-project/config/000.npy'\n",
    "\n",
    "cfg_s = 0.65\n",
    "mouth_ratio = 0.25\n",
    "subtract_avg_motion = False\n",
    "\n",
    "headpose_bound_list = [-21, 25, -30, 30, -23, 23, -0.3, 0.3, -0.3, 0.28]\n",
    "\n",
    "# input\n",
    "input_image_path = 'D:/Projects/Upenn_CIS_5650/final-project/data/img/test5.jpg'\n",
    "input_audio_path = 'D:/Projects/Upenn_CIS_5650/final-project/data/audio/test3.wav'\n",
    "\n",
    "# output\n",
    "output_no_audio_path = 'D:/Projects/Upenn_CIS_5650/final-project/LivePortrait/inference/animations/no_audio.mp4'\n",
    "output_video = 'D:/Projects/Upenn_CIS_5650/final-project/LivePortrait/inference/animations/5_3_full_10_65_with_audio.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:19:07] </span>LandmarkRunner warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>405s                                                 <a href=\"file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\landmark_runner.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">landmark_runner.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\landmark_runner.py#95\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">95</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:19:07]\u001b[0m\u001b[2;36m \u001b[0mLandmarkRunner warmup time: \u001b[1;36m1.\u001b[0m405s                                                 \u001b]8;id=858707;file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\landmark_runner.py\u001b\\\u001b[2mlandmark_runner.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=709848;file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\landmark_runner.py#95\u001b\\\u001b[2m95\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:19:08] </span>FaceAnalysisDIY warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>029s                                              <a href=\"file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\face_analysis_diy.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">face_analysis_diy.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\face_analysis_diy.py#79\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">79</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:19:08]\u001b[0m\u001b[2;36m \u001b[0mFaceAnalysisDIY warmup time: \u001b[1;36m1.\u001b[0m029s                                              \u001b]8;id=782776;file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\face_analysis_diy.py\u001b\\\u001b[2mface_analysis_diy.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=99683;file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\face_analysis_diy.py#79\u001b\\\u001b[2m79\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LivePortrait Pipeline\n",
    "\n",
    "def partial_fields(target_class, kwargs):\n",
    "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
    "\n",
    "args = ArgumentConfig()\n",
    "inference_cfg = partial_fields(InferenceConfig, args.__dict__)\n",
    "crop_cfg = partial_fields(CropConfig, args.__dict__)\n",
    "print(\"Compile complete\")\n",
    "\n",
    "'''\n",
    "Main function for inference\n",
    "'''\n",
    "\n",
    "def get_kp_info(x: torch.Tensor, **kwargs) -> dict:\n",
    "    \"\"\" get the implicit keypoint information\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    flag_refine_info: whether to trandform the pose to degrees and the dimention of the reshape\n",
    "    return: A dict contains keys: 'pitch', 'yaw', 'roll', 't', 'exp', 'scale', 'kp'\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        kp_info = motion_extractor(x)\n",
    "\n",
    "        if inference_cfg.flag_use_half_precision:\n",
    "            # float the dict\n",
    "            for k, v in kp_info.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    kp_info[k] = v.float()\n",
    "\n",
    "    flag_refine_info: bool = kwargs.get('flag_refine_info', True)\n",
    "    if flag_refine_info:\n",
    "        bs = kp_info['kp'].shape[0]\n",
    "        kp_info['pitch'] = headpose_pred_to_degree(kp_info['pitch'])[:, None]  # Bx1\n",
    "        kp_info['yaw'] = headpose_pred_to_degree(kp_info['yaw'])[:, None]  # Bx1\n",
    "        kp_info['roll'] = headpose_pred_to_degree(kp_info['roll'])[:, None]  # Bx1\n",
    "        kp_info['kp'] = kp_info['kp'].reshape(bs, -1, 3)  # BxNx3\n",
    "        kp_info['exp'] = kp_info['exp'].reshape(bs, -1, 3)  # BxNx3\n",
    "\n",
    "    return kp_info\n",
    "\n",
    "def prepare_source(img: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    img: HxWx3, uint8, 256x256\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    x = img.copy()\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        x = x[np.newaxis].astype(np.float32) / 255.  # HxWx3 -> 1xHxWx3, normalized to 0~1\n",
    "    elif x.ndim == 4:\n",
    "        x = x.astype(np.float32) / 255.  # BxHxWx3, normalized to 0~1\n",
    "    else:\n",
    "        raise ValueError(f'img ndim should be 3 or 4: {x.ndim}')\n",
    "    x = np.clip(x, 0, 1)  # clip to 0~1\n",
    "    x = torch.from_numpy(x).permute(0, 3, 1, 2)  # 1xHxWx3 -> 1x3xHxW\n",
    "    x = x.to(device)\n",
    "    return x\n",
    "\n",
    "def warp_decode(feature_3d: torch.Tensor, kp_source: torch.Tensor, kp_driving: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the image after the warping of the implicit keypoints\n",
    "    feature_3d: Bx32x16x64x64, feature volume\n",
    "    kp_source: BxNx3\n",
    "    kp_driving: BxNx3\n",
    "    \"\"\"\n",
    "    # The line 18 in Algorithm 1: D(W(f_s; x_s, x′_d,i)）\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        # get decoder input\n",
    "        ret_dct = warping_module(feature_3d, kp_source=kp_source, kp_driving=kp_driving)\n",
    "        # decode\n",
    "        ret_dct['out'] = spade_generator(feature=ret_dct['out'])\n",
    "\n",
    "    return ret_dct\n",
    "\n",
    "def extract_feature_3d( x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the appearance feature of the image by F\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        feature_3d = appearance_feature_extractor(x)\n",
    "\n",
    "    return feature_3d.float()\n",
    "\n",
    "def transform_keypoint(kp_info: dict):\n",
    "    \"\"\"\n",
    "    transform the implicit keypoints with the pose, shift, and expression deformation\n",
    "    kp: BxNx3\n",
    "    \"\"\"\n",
    "    kp = kp_info['kp']    # (bs, k, 3)\n",
    "    pitch, yaw, roll = kp_info['pitch'], kp_info['yaw'], kp_info['roll']\n",
    "\n",
    "    t, exp = kp_info['t'], kp_info['exp']\n",
    "    scale = kp_info['scale']\n",
    "\n",
    "    pitch = headpose_pred_to_degree(pitch)\n",
    "    yaw = headpose_pred_to_degree(yaw)\n",
    "    roll = headpose_pred_to_degree(roll)\n",
    "\n",
    "    bs = kp.shape[0]\n",
    "    if kp.ndim == 2:\n",
    "        num_kp = kp.shape[1] // 3  # Bx(num_kpx3)\n",
    "    else:\n",
    "        num_kp = kp.shape[1]  # Bxnum_kpx3\n",
    "\n",
    "    rot_mat = get_rotation_matrix(pitch, yaw, roll)    # (bs, 3, 3)\n",
    "\n",
    "    # Eqn.2: s * (R * x_c,s + exp) + t\n",
    "    kp_transformed = kp.view(bs, num_kp, 3) @ rot_mat + exp.view(bs, num_kp, 3)\n",
    "    kp_transformed *= scale[..., None]  # (bs, k, 3) * (bs, 1, 1) = (bs, k, 3)\n",
    "    kp_transformed[:, :, 0:2] += t[:, None, 0:2]  # remove z, only apply tx ty\n",
    "\n",
    "    return kp_transformed\n",
    "\n",
    "'''\n",
    "Main module for inference\n",
    "'''\n",
    "\n",
    "model_config = yaml.load(open(inference_cfg.models_config, 'r'), Loader=yaml.SafeLoader)\n",
    "# init F\n",
    "appearance_feature_extractor = load_model(inference_cfg.checkpoint_F, model_config, device, 'appearance_feature_extractor')\n",
    "# init M\n",
    "motion_extractor = load_model(inference_cfg.checkpoint_M, model_config, device, 'motion_extractor')\n",
    "# init W\n",
    "warping_module = load_model(inference_cfg.checkpoint_W, model_config, device, 'warping_module')\n",
    "# init G\n",
    "spade_generator = load_model(inference_cfg.checkpoint_G, model_config, device, 'spade_generator')\n",
    "# init S and R\n",
    "if inference_cfg.checkpoint_S is not None and os.path.exists(inference_cfg.checkpoint_S):\n",
    "    stitching_retargeting_module = load_model(inference_cfg.checkpoint_S, model_config, device, 'stitching_retargeting_module')\n",
    "else:\n",
    "    stitching_retargeting_module = None\n",
    "\n",
    "cropper = Cropper(crop_cfg=crop_cfg, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# wav2vec Pipeline\n",
    "\n",
    "def load_and_process_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    original_sample_rate = sample_rate\n",
    "\n",
    "    if sample_rate != TARGET_SAMPLE_RATE:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, TARGET_SAMPLE_RATE)\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    # print(file_path,\" waveform.shape \",waveform.shape)\n",
    "\n",
    "    # Calculate section length and overlap in samples\n",
    "    section_samples = SECTION_LENGTH * 16027\n",
    "    overlap_samples = int(OVERLAP / FRAME_RATE * TARGET_SAMPLE_RATE)\n",
    "    # print('section_samples',section_samples,'overlap_samples',overlap_samples)\n",
    "\n",
    "    # pad 10 overlap at the beginning\n",
    "    waveform = torch.nn.functional.pad(waveform, (overlap_samples, 0))\n",
    "    # Pad if shorter than 3 seconds\n",
    "    if waveform.shape[1] < section_samples:\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, section_samples - waveform.shape[1]))\n",
    "        return [waveform.squeeze(0)], original_sample_rate\n",
    "\n",
    "    # Split into sections with overlap\n",
    "    sections = []\n",
    "    start = 0\n",
    "\n",
    "    # print('starting to segment', file_path)\n",
    "    while start < waveform.shape[1]:\n",
    "        end = start + section_samples\n",
    "        if end >= waveform.shape[1]:\n",
    "            tmp=waveform[:, start:min(end, waveform.shape[1])]\n",
    "            tmp = torch.nn.functional.pad(tmp, (0, section_samples - tmp.shape[1]))\n",
    "            sections.append(tmp.squeeze(0))\n",
    "            # print(tmp.shape)\n",
    "            break\n",
    "        else:\n",
    "            sections.append(waveform[:, start:min(end, waveform.shape[1])].squeeze(0))\n",
    "\n",
    "        start = int(end - overlap_samples)\n",
    "\n",
    "\n",
    "    return file_path, sections\n",
    "\n",
    "def inference_process_wav_file(path):\n",
    "    audio_path, segments = load_and_process_audio(path)\n",
    "    # print(audio_path,segments)\n",
    "    segments = np.array(segments)\n",
    "\n",
    "    inputs = wav2vec_processor(segments, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\", padding=True).input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = wav2vec_model(inputs)\n",
    "        latent = outputs.last_hidden_state\n",
    "\n",
    "        seq_length = latent.shape[1]\n",
    "        new_seq_length = int(seq_length * (FRAME_RATE / 50))\n",
    "\n",
    "        latent_features_interpolated = F.interpolate(latent.transpose(1,2),\n",
    "                                                     size=new_seq_length,\n",
    "                                                     mode='linear',\n",
    "                                                     align_corners=True).transpose(1,2)\n",
    "    return latent_features_interpolated\n",
    "\n",
    "def autoregress_load_and_process_audio(file_path):\n",
    "    first_segment_prev_length = 10\n",
    "    first_segment_main_length = 65\n",
    "    remaining_segment_prev_length = prev_context_len\n",
    "    remaining_segment_main_length = gen_len_per_window\n",
    "\n",
    "    # below is the same as load_and_process_audio\n",
    "    waveform, og_sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    if og_sample_rate != TARGET_SAMPLE_RATE:\n",
    "        waveform = torchaudio.functional.resample(waveform, og_sample_rate, TARGET_SAMPLE_RATE)\n",
    "\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "    # define sample count\n",
    "    per_window_samples = SECTION_LENGTH * 16027\n",
    "    first_prev_samples = int(first_segment_prev_length * 16027 / FRAME_RATE)\n",
    "    remaining_overlap_samples = int(remaining_segment_prev_length / FRAME_RATE * TARGET_SAMPLE_RATE)\n",
    "    # pad 10 overlap at the beginning\n",
    "    total_frame = int(waveform.shape[1] / TARGET_SAMPLE_RATE * FRAME_RATE) + 1\n",
    "    waveform = torch.nn.functional.pad(waveform, (first_prev_samples, 0))\n",
    "\n",
    "    # split into windows with overlap\n",
    "    windows = []\n",
    "    start = 0\n",
    "\n",
    "    total_sample_count = waveform.shape[1]\n",
    "    while start < total_sample_count:\n",
    "        end = start + per_window_samples\n",
    "        if end >= total_sample_count: # need to pad since last exceeds total sample count\n",
    "            tmp = waveform[:, start:min(end, total_sample_count)]\n",
    "            tmp = torch.nn.functional.pad(tmp, (0, per_window_samples - tmp.shape[1]))\n",
    "            windows.append(tmp.squeeze(0))\n",
    "            break\n",
    "        else:\n",
    "            windows.append(waveform[:, start:min(end, total_sample_count)].squeeze(0))\n",
    "        start = int(end - remaining_overlap_samples)\n",
    "\n",
    "    return windows, total_frame\n",
    "\n",
    "def autoregress_inference_process_wav_file(path):\n",
    "    windows, total_frame = autoregress_load_and_process_audio(path)\n",
    "    print(f\"total frame {total_frame}\")\n",
    "    windows = np.array(windows)\n",
    "\n",
    "    inputs = wav2vec_processor(windows, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\", padding=True).input_values.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = wav2vec_model(inputs)\n",
    "        latent = outputs.last_hidden_state\n",
    "\n",
    "        seq_length = latent.shape[1]\n",
    "        new_seq_length = int(seq_length * (FRAME_RATE / 50))\n",
    "\n",
    "        latent_features_interpolated = F.interpolate(latent.transpose(1,2),\n",
    "                                                     size=new_seq_length,\n",
    "                                                     mode='linear',\n",
    "                                                     align_corners=True).transpose(1,2)\n",
    "    return latent_features_interpolated, total_frame\n",
    "\n",
    "# Move model and processor to global scope\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(device)\n",
    "wav2vec_processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint loaded from D:/Projects/Upenn_CIS_5650/final-project/config/model.pth\n"
     ]
    }
   ],
   "source": [
    "# DiT Pipeline\n",
    "\n",
    "audio_model_config = json.load(open(config_path))\n",
    "inference_manager = get_model(config_path, weight_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headpose tuning\n",
    "\n",
    "# blink noise\n",
    "def fixed_blink_noise(motion_tensor, motion_gt):\n",
    "    eye_indices = torch.tensor([5, 9], device=motion_tensor.device)\n",
    "    rest_latent = torch.tensor([-0.005], device=motion_tensor.device)\n",
    "    spikes = torch.tensor([0.0015, 0.0085, 0.011, 0.0075, 0.002], device=motion_tensor.device)\n",
    "    freeze_index = [0, 1, 2, 4, 6, 8, 10]\n",
    "    freeze_index = torch.tensor(freeze_index, device=motion_tensor.device)\n",
    "    period = 12\n",
    "    period_counter = 0\n",
    "    reset_flag = False\n",
    "    for i in range(motion_tensor.shape[0]):\n",
    "        in_period_index = i % period\n",
    "        if in_period_index < 5 and period_counter >= period:\n",
    "            for eye_index in eye_indices:\n",
    "                motion_tensor[i, eye_index] = spikes[in_period_index]\n",
    "                motion_tensor[i, eye_index] += torch.randn_like(motion_tensor[i, eye_index]) * 0.002\n",
    "            if in_period_index == 4:\n",
    "                reset_flag = True\n",
    "                period_counter = 0\n",
    "        else:\n",
    "            for eye_index in eye_indices:\n",
    "                motion_tensor[i, eye_index] = rest_latent\n",
    "                motion_tensor[i, eye_index] += torch.randn_like(motion_tensor[i, eye_index]) * 0.0002\n",
    "            period_counter += 1\n",
    "        if reset_flag:\n",
    "            reset_flag = False\n",
    "            period = torch.randint(15, 18, (1,)).item()\n",
    "        for f in freeze_index:\n",
    "            # motion_tensor[i, f] = motion_gt[i, f]|\n",
    "            motion_tensor[i, f] = 0\n",
    "    return motion_tensor\n",
    "\n",
    "# normalize headpose\n",
    "def normalize_pose(full_motion, headpose_bound):\n",
    "    assert headpose_bound is not None and len(headpose_bound) % 2 == 0\n",
    "    headpose_bound = torch.tensor(headpose_bound)\n",
    "    headpose_bound = headpose_bound.reshape(headpose_bound.shape[0] // 2, 2)\n",
    "\n",
    "    # Assuming full_motion is a tensor of shape (batch_size, sequence_length, num_features)\n",
    "    # and the last 5 features are the ones to be normalized\n",
    "    last_5_features = full_motion[:, :, -5:]\n",
    "\n",
    "    # Normalize each of the last 5 features\n",
    "    for i in range(5):\n",
    "        lower_bound = headpose_bound[i][0]\n",
    "        upper_bound = headpose_bound[i][1]\n",
    "\n",
    "        # Clamp the values within the specified bounds\n",
    "        clamped = torch.clamp(last_5_features[:, :, i], min=lower_bound, max=upper_bound)\n",
    "\n",
    "        # Normalize to the range [-0.05, 0.05]\n",
    "        normalized = (clamped - lower_bound) / (upper_bound - lower_bound) * 0.1 - 0.05\n",
    "\n",
    "        # Update the last 5 features with the normalized values\n",
    "        last_5_features[:, :, i] = normalized\n",
    "\n",
    "    # Update the full_motion tensor with the normalized last 5 features\n",
    "    full_motion[:, :, -5:] = last_5_features\n",
    "\n",
    "    return full_motion\n",
    "\n",
    "def reverse_normalize_pose(normalized_motion, headpose_bound):\n",
    "    assert headpose_bound is not None and len(headpose_bound) % 2 == 0\n",
    "    headpose_bound = torch.tensor(headpose_bound)\n",
    "    headpose_bound = headpose_bound.reshape(headpose_bound.shape[0] // 2, 2)\n",
    "\n",
    "    # Assuming normalized_motion is a tensor of shape (batch_size, sequence_length, num_features)\n",
    "    # and the last 5 features are the ones to be reversed\n",
    "    last_5_features = normalized_motion[:, :, -5:]\n",
    "\n",
    "    # Reverse normalization for each of the last 5 features\n",
    "    for i in range(5):\n",
    "        lower_bound = headpose_bound[i][0]\n",
    "        upper_bound = headpose_bound[i][1]\n",
    "\n",
    "        # Reverse the normalization from [-0.05, 0.05] to the original range\n",
    "        original = (last_5_features[:, :, i] + 0.05) / 0.1 * (upper_bound - lower_bound) + lower_bound\n",
    "\n",
    "        # Update the last 5 features with the original values\n",
    "        last_5_features[:, :, i] = original\n",
    "\n",
    "    # Update the normalized_motion tensor with the original last 5 features\n",
    "    normalized_motion[:, :, -5:] = last_5_features\n",
    "\n",
    "    return normalized_motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1175, 25])\n",
      "torch.Size([25])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\audio_dit\\dataset.py:292: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  headpose_bound = torch.tensor(headpose_bound)\n",
      "d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\audio_dit\\dataset.py:332: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  latent_bound = torch.tensor(latent_bound)\n"
     ]
    }
   ],
   "source": [
    "# Motion tuning\n",
    "\n",
    "motion_mean_exp_tensor = torch.from_numpy(np.load(motion_mean_exp_path)).to(device = device)\n",
    "motion_mean_exp_tensor = motion_mean_exp_tensor.unsqueeze(0).to(device = device)\n",
    "motion_tensor, _, _, _ = process_motion_tensor(motion_mean_exp_tensor, None, \\\n",
    "                            latent_type=audio_model_config['motion_latent_type'],\n",
    "                            latent_mask_1=audio_model_config['latent_mask_1'],\n",
    "                            latent_bound=torch.tensor(audio_model_config['latent_bound'], device=device),\n",
    "                            use_headpose=True, headpose_bound=torch.tensor(headpose_bound_list, device=device))\n",
    "mean_exp = torch.mean(motion_tensor.reshape(-1, motion_tensor.shape[-1]), dim=0)\n",
    "print(motion_tensor.shape)\n",
    "print(mean_exp.shape)\n",
    "mouth_open_ratio_input = torch.tensor([mouth_ratio], device=device).unsqueeze(0)\n",
    "motion_dim = audio_model_config['x_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image processing time 0.126749s\n"
     ]
    }
   ],
   "source": [
    "# Process Input Image\n",
    "ts = datetime.now()\n",
    "img_rgb = load_image_rgb(input_image_path)\n",
    "\n",
    "img_crop_256x256 = cv2.resize(img_rgb, (256, 256))  # force to resize to 256x256\n",
    "I_s = prepare_source(img_crop_256x256)\n",
    "x_s_info = get_kp_info(I_s)\n",
    "x_c_s = x_s_info['kp']\n",
    "x_s = transform_keypoint(x_s_info)\n",
    "f_s = extract_feature_3d(I_s)\n",
    "\n",
    "shape_in = x_c_s.reshape(1, -1).to(device)\n",
    "te = datetime.now()\n",
    "print(f\"image processing time {(te - ts).total_seconds()}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total frame 95\n",
      "audio latent shape torch.Size([5, 75, 768])\n",
      "audio processing time 0.095371s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\fp\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:863: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# Process Input Audio\n",
    "ts = datetime.now()\n",
    "audio_latent, total_frame = autoregress_inference_process_wav_file(input_audio_path)\n",
    "print(f\"audio latent shape {audio_latent.shape}\")\n",
    "te = datetime.now()\n",
    "print(f\"audio processing time {(te - ts).total_seconds()}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_stream(audio_latent, f_s, x_s, x_c_s, x_s_info, audio_model_config, warp_decode_func, output_buffer):\n",
    "    ts = datetime.now()\n",
    "    window_count = audio_latent.shape[0]\n",
    "    out_motion = torch.tensor([], device=device)\n",
    "    start_idx = 0\n",
    "\n",
    "    for batch_index in range(0, window_count):\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        if batch_index == 0:\n",
    "            this_audio_prev = audio_latent[0:1, 0:10, :]\n",
    "            audio_seq = audio_latent[0:1, 10:, :]\n",
    "            this_motion_prev = torch.zeros(1, 10, motion_dim , device=device)\n",
    "            gen_length = 65\n",
    "        else:\n",
    "            this_audio_prev = audio_latent[batch_index:batch_index+1, 0:prev_context_len, :]\n",
    "            audio_seq = audio_latent[batch_index:batch_index+1, prev_context_len:, :]\n",
    "            gen_length = gen_len_per_window\n",
    "\n",
    "        mean_exp_expanded = mean_exp.expand(1, -1)\n",
    "\n",
    "        tss = datetime.now()\n",
    "        generated_motion, null_motion = inference_manager.inference(audio_seq,\n",
    "                                                    shape_in, this_motion_prev, this_audio_prev, #seq_mask=seq_mask,\n",
    "                                                    cfg_scale=cfg_s,\n",
    "                                                    mouth_open_ratio = mouth_open_ratio_input,\n",
    "                                                    denoising_steps=10,\n",
    "                                                    gen_length=gen_length,\n",
    "                                                    mean_exp=mean_exp_expanded)\n",
    "        tee = datetime.now()\n",
    "\n",
    "        print(f\"{batch_index} inference time {(tee - tss).total_seconds()}s\")\n",
    "        full_window_motion = torch.cat((this_motion_prev, generated_motion), dim=1)\n",
    "        this_motion_prev = full_window_motion[:, -prev_context_len:, :]\n",
    "\n",
    "        if subtract_avg_motion:\n",
    "            generated_motion = generated_motion - torch.mean(generated_motion, dim=-1, keepdim=True)\n",
    "\n",
    "        generated_motion = (generated_motion - torch.mean(null_motion, dim=-2, keepdim=True)).squeeze(0)\n",
    "        #print(f\"{batch_index} generated motion {generated_motion.shape}\")\n",
    "        out_motion = torch.cat((out_motion, generated_motion), dim=0)\n",
    "        #print(f\"{batch_index} out motion {out_motion.shape}\")\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        print(f\"{batch_index} window time {(end_time - start_time).total_seconds()}s\")\n",
    "\n",
    "    te = datetime.now()\n",
    "    print(f\"motion generation time {(te - ts).total_seconds()}s\")\n",
    "\n",
    "    filtered_start_time = datetime.now()\n",
    "\n",
    "    # Then modify the filtering line to:\n",
    "    out_motion_filtered = savgol_filter(out_motion[:total_frame].cpu().numpy(), window_length=5, polyorder=2, axis=0)\n",
    "    out_motion_f = torch.tensor(out_motion_filtered, device=device)\n",
    "\n",
    "    motion_gt = motion_tensor[:, :out_motion_f.shape[0], :].squeeze(0)\n",
    "    out_motion_f = fixed_blink_noise(out_motion_f, motion_gt)\n",
    "    out_pose_motion = out_motion_f[:, -5:]\n",
    "    out_pose_smoothed = savgol_filter(out_pose_motion.cpu().numpy(), window_length=30, polyorder=2, axis=0)\n",
    "    out_motion_f[:, -5:-2] *= 2\n",
    "    out_motion_f[:, -2:] *= 0.5\n",
    "    out_motion_f[:, -5:] = torch.tensor(out_pose_smoothed, device=device)\n",
    "\n",
    "    full_motion_stacked = out_motion_f\n",
    "    full_motion_stacked = reverse_normalize_pose(full_motion_stacked.unsqueeze(0), headpose_bound=torch.tensor(headpose_bound_list, device=device))\n",
    "    full_motion = full_motion_stacked.squeeze(0)[start_idx:]\n",
    "    start_idx = full_motion_stacked.shape[1]\n",
    "    print(f\"full motion stacked {full_motion_stacked.shape}\")\n",
    "    print(f\"full motion {full_motion.shape}\")\n",
    "    print(f\"next start_idx {start_idx}\")\n",
    "\n",
    "    filtered_end_time = datetime.now()\n",
    "    print(f\"filtering time {(filtered_end_time - filtered_start_time).total_seconds()}\")\n",
    "\n",
    "    time1 = datetime.now()\n",
    "    pose = full_motion[:, -5:]\n",
    "    exp = full_motion[:, :-5]\n",
    "\n",
    "    full_63_exp = torch.zeros(full_motion.shape[0], 63, device=device)\n",
    "\n",
    "    full_63_exp[:, audio_model_config['latent_mask_1']] = exp\n",
    "    full_motion = full_63_exp.reshape(-1, 63)\n",
    "\n",
    "    x_d_list = []\n",
    "    scale = x_s_info['scale']\n",
    "\n",
    "    for i in tqdm(range(full_motion.shape[0]), desc=\"Generating x_d\"):\n",
    "        exp = full_motion[i].reshape(21, 3)\n",
    "        pitch, yaw, roll, t_x, t_y = pose[i].unsqueeze(0).unbind(-1)\n",
    "        t = torch.tensor([t_x, t_y, 0], device=device)\n",
    "\n",
    "        x_d_i = scale * (x_c_s @ get_rotation_matrix(pitch, yaw, roll) + exp) + t\n",
    "        x_d_list.append(x_d_i.squeeze(0))\n",
    "\n",
    "    x_d_batch = torch.stack(x_d_list, dim=0)\n",
    "    time2 = datetime.now()\n",
    "\n",
    "    for i in tqdm(range(full_motion.shape[0]), desc=\"Processing batches\"):\n",
    "        time3 = datetime.now()\n",
    "        out = warp_decode_func(f_s, x_s, x_d_batch[i])\n",
    "\n",
    "        # Convert to numpy array\n",
    "        batch_frame = out['out'].permute(0, 2, 3, 1).mul_(255).to(torch.uint8).squeeze(0)\n",
    "        time4 = datetime.now()\n",
    "        print(f\"{i} {(time4 - time3).total_seconds()}s\")\n",
    "        output_buffer.append(batch_frame)\n",
    "\n",
    "    time5 = datetime.now()\n",
    "    print(f\"{(time2 - time1).total_seconds()}s {(time5 - time2).total_seconds()}s\")\n",
    "    print(f\"cur buffer length {len(output_buffer)}\")\n",
    "\n",
    "    #output_buffer.put(None)\n",
    "\n",
    "def display_frames(output_buffer, pre_time):\n",
    "    while True:\n",
    "        # Retrieve the next frame from the buffer\n",
    "        frame_tensor = output_buffer.get()\n",
    "\n",
    "        # End of processing\n",
    "        if frame_tensor is None:\n",
    "            break\n",
    "\n",
    "        # Transfer the frame from pinned memory back to CPU\n",
    "        frame = frame_tensor.cpu().numpy()\n",
    "        result_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Ensure 25 fps\n",
    "        if (datetime.now() - pre_time).total_seconds() < 0.024:\n",
    "            sleep(0.024 - (datetime.now() - pre_time).total_seconds())\n",
    "\n",
    "        # Display the frame using OpenCV\n",
    "        print(f\"time to display {datetime.now() - pre_time}\")\n",
    "        cv2.imshow(\"Video Stream\", result_bgr)\n",
    "        cv2.waitKey(1)\n",
    "        pre_time = datetime.now()\n",
    "\n",
    "def stream_frames():\n",
    "    # Record start time\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Output buffer tracking\n",
    "    output_buffer = Queue(maxsize = 5000)\n",
    "\n",
    "    # Start frame processing thread\n",
    "    processing_thread = Thread(target = process_audio_stream, args=(audio_latent, f_s, x_s, x_c_s, x_s_info, audio_model_config, warp_decode, output_buffer))\n",
    "    processing_thread.start()\n",
    "\n",
    "    # Start display frame\n",
    "    display_frames(output_buffer, start_time)\n",
    "\n",
    "    # Wait until terminate\n",
    "    processing_thread.join()\n",
    "\n",
    "    # Cleanup\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inference time 0.088378s\n",
      "0 window time 0.089378s\n",
      "1 inference time 0.061624s\n",
      "1 window time 0.061624s\n",
      "2 inference time 0.061601s\n",
      "2 window time 0.061601s\n",
      "3 inference time 0.063757s\n",
      "3 window time 0.063757s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JackeyTY\\AppData\\Local\\Temp\\ipykernel_124328\\861865157.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  headpose_bound = torch.tensor(headpose_bound)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 inference time 0.063608s\n",
      "4 window time 0.063608s\n",
      "motion generation time 0.339968s\n",
      "full motion stacked torch.Size([1, 95, 25])\n",
      "full motion torch.Size([95, 25])\n",
      "next start_idx 95\n",
      "filtering time 0.044666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating x_d: 100%|██████████| 95/95 [00:00<00:00, 1909.00it/s]\n",
      "Processing batches:  11%|█         | 10/95 [00:00<00:02, 30.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.156918s\n",
      "1 0.019035s\n",
      "2 0.024267s\n",
      "3 0.023607s\n",
      "4 0.025154s\n",
      "5 0.022621s\n",
      "6 0.023965s\n",
      "7 0.024035s\n",
      "8 0.026109s\n",
      "9 0.026129s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|█▌        | 15/95 [00:00<00:02, 36.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.023707s\n",
      "11 0.02069s\n",
      "12 0.023403s\n",
      "13 0.021454s\n",
      "14 0.021871s\n",
      "15 0.024161s\n",
      "16 0.021328s\n",
      "17 0.021175s\n",
      "18 0.021648s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▋       | 25/95 [00:00<00:01, 39.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0.037255s\n",
      "20 0.020209s\n",
      "21 0.02318s\n",
      "22 0.021382s\n",
      "23 0.022178s\n",
      "24 0.02364s\n",
      "25 0.021542s\n",
      "26 0.023077s\n",
      "27 0.021528s\n",
      "28 0.021422s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|███▋      | 35/95 [00:00<00:01, 42.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 0.02309s\n",
      "30 0.023085s\n",
      "31 0.023015s\n",
      "32 0.023335s\n",
      "33 0.018695s\n",
      "34 0.022673s\n",
      "35 0.023123s\n",
      "36 0.022312s\n",
      "37 0.02366s\n",
      "38 0.022661s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  47%|████▋     | 45/95 [00:01<00:01, 43.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 0.021407s\n",
      "40 0.02271s\n",
      "41 0.022148s\n",
      "42 0.022637s\n",
      "43 0.021082s\n",
      "44 0.021116s\n",
      "45 0.019082s\n",
      "46 0.022186s\n",
      "47 0.023324s\n",
      "48 0.020665s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  58%|█████▊    | 55/95 [00:01<00:00, 42.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 0.024591s\n",
      "50 0.02506s\n",
      "51 0.019679s\n",
      "52 0.022247s\n",
      "53 0.035541s\n",
      "54 0.02265s\n",
      "55 0.021187s\n",
      "56 0.020695s\n",
      "57 0.023988s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  68%|██████▊   | 65/95 [00:01<00:00, 43.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 0.022912s\n",
      "59 0.021336s\n",
      "60 0.023109s\n",
      "61 0.020684s\n",
      "62 0.021172s\n",
      "63 0.02368s\n",
      "64 0.023121s\n",
      "65 0.019965s\n",
      "66 0.022621s\n",
      "67 0.021046s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  79%|███████▉  | 75/95 [00:01<00:00, 44.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 0.023271s\n",
      "69 0.02339s\n",
      "70 0.022313s\n",
      "71 0.022408s\n",
      "72 0.023099s\n",
      "73 0.018453s\n",
      "74 0.022646s\n",
      "75 0.02226s\n",
      "76 0.023053s\n",
      "77 0.021881s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  89%|████████▉ | 85/95 [00:02<00:00, 44.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 0.022199s\n",
      "79 0.022043s\n",
      "80 0.023209s\n",
      "81 0.022602s\n",
      "82 0.023989s\n",
      "83 0.020065s\n",
      "84 0.022526s\n",
      "85 0.020772s\n",
      "86 0.029686s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 95/95 [00:02<00:00, 41.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 0.030107s\n",
      "88 0.021682s\n",
      "89 0.022131s\n",
      "90 0.021045s\n",
      "91 0.025122s\n",
      "92 0.020011s\n",
      "93 0.024974s\n",
      "94 0.018017s\n",
      "0.052766s 2.315736s\n",
      "cur buffer length 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_frames = []\n",
    "process_audio_stream(audio_latent, f_s, x_s, x_c_s, x_s_info, audio_model_config, warp_decode, all_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_video(all_frames_in, audio_path, output_path):\n",
    "    all_frames = []\n",
    "    for frame in all_frames_in:\n",
    "        all_frames.append(frame.cpu().numpy())\n",
    "    output_no_audio_path = 'D:/Projects/Upenn_CIS_5650/final-project/LivePortrait/inference/animations/test_no_audio.mp4'\n",
    "    if os.path.exists(output_no_audio_path):\n",
    "        os.remove(output_no_audio_path)\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "    fps = 25  # Adjust as needed\n",
    "\n",
    "    height, width, layers = all_frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_no_audio_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in all_frames:\n",
    "        video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    video.release()\n",
    "\n",
    "    # Add audio to the video using ffmpeg\n",
    "    input_video = output_no_audio_path\n",
    "    input_audio = audio_path  # Use the path to your audio file\n",
    "\n",
    "    ffmpeg_cmd = [\n",
    "        'ffmpeg',\n",
    "        '-i', input_video,\n",
    "        '-i', input_audio,\n",
    "        '-c:v', 'copy',\n",
    "        '-c:a', 'aac',\n",
    "        '-shortest',\n",
    "        output_path\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(ffmpeg_cmd, check=True)\n",
    "        os.remove(output_no_audio_path)\n",
    "        print(f\"Video with audio saved to {output_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error adding audio to video: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video with audio saved to D:/Projects/Upenn_CIS_5650/final-project/LivePortrait/inference/animations/5_3_full_10_65_with_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "write_video(all_frames, input_audio_path, output_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
