{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\fp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "import glob\n",
    "import tyro\n",
    "import imageio\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "from rich.progress import track\n",
    "import threading\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live Portrait\n",
    "from src.config.argument_config import ArgumentConfig\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.config.crop_config import CropConfig\n",
    "from src.utils.helper import load_model, concat_feat\n",
    "from src.utils.camera import headpose_pred_to_degree, get_rotation_matrix\n",
    "from src.utils.retargeting_utils import calc_eye_close_ratio, calc_lip_close_ratio\n",
    "from src.utils.cropper import Cropper\n",
    "from src.utils.camera import get_rotation_matrix\n",
    "from src.utils.video import images2video, concat_frames, get_fps, add_audio_to_video, has_audio_stream\n",
    "from src.utils.crop import _transform_img, prepare_paste_back, paste_back\n",
    "from src.utils.io import load_image_rgb, load_video, resize_to_limit, dump, load\n",
    "from src.utils.helper import mkdir, basename, dct2device, is_video, is_template, remove_suffix, is_image\n",
    "from src.utils.filter import smooth\n",
    "\n",
    "# DiT\n",
    "from audio_dit.inference import InferenceManager, get_model\n",
    "from audio_dit.dataset import load_and_process_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio model\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\"\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "FRAME_RATE = 25\n",
    "SECTION_LENGTH = 3\n",
    "OVERLAP = 10\n",
    "\n",
    "DB_ROOT = 'vox2-audio-tx'\n",
    "LOG = 'log'\n",
    "AUDIO = 'audio/audio'\n",
    "OUTPUT_DIR = 'audio_encoder_output'\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# DiT model\n",
    "config_path = 'D:/Projects/Upenn_CIS_5650/final-project/config/config.json'\n",
    "weight_path = 'D:/Projects/Upenn_CIS_5650/final-project/config/model.pth'\n",
    "\n",
    "# input\n",
    "input_image_path = 'D:/Projects/Upenn_CIS_5650/final-project/data/img/test1.jpg'\n",
    "input_audio_path = 'D:/Projects/Upenn_CIS_5650/final-project/data/audio/test1.wav'\n",
    "\n",
    "# output\n",
    "output_no_audio_path = 'D:/Projects/Upenn_CIS_5650/final-project/LivePortrait/inference/animations/test1_no_audio.mp4'\n",
    "output_video = 'D:/Projects/Upenn_CIS_5650/final-project/LivePortrait/inference/animations/test1_with_audio.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Portrait Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:40:39] </span>LandmarkRunner warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>572s                                                 <a href=\"file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\landmark_runner.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">landmark_runner.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\landmark_runner.py#95\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">95</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:40:39]\u001b[0m\u001b[2;36m \u001b[0mLandmarkRunner warmup time: \u001b[1;36m1.\u001b[0m572s                                                 \u001b]8;id=625822;file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\landmark_runner.py\u001b\\\u001b[2mlandmark_runner.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=584158;file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\landmark_runner.py#95\u001b\\\u001b[2m95\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:40:41] </span>FaceAnalysisDIY warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.</span>043s                                              <a href=\"file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\face_analysis_diy.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">face_analysis_diy.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\face_analysis_diy.py#79\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">79</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:40:41]\u001b[0m\u001b[2;36m \u001b[0mFaceAnalysisDIY warmup time: \u001b[1;36m1.\u001b[0m043s                                              \u001b]8;id=148030;file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\face_analysis_diy.py\u001b\\\u001b[2mface_analysis_diy.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=162917;file://d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\src\\utils\\face_analysis_diy.py#79\u001b\\\u001b[2m79\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def partial_fields(target_class, kwargs):\n",
    "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
    "\n",
    "args = ArgumentConfig()\n",
    "inference_cfg = partial_fields(InferenceConfig, args.__dict__)\n",
    "crop_cfg = partial_fields(CropConfig, args.__dict__)\n",
    "print(\"Compile complete\")\n",
    "\n",
    "'''\n",
    "Util functions\n",
    "'''\n",
    "\n",
    "def calculate_distance_ratio(lmk: np.ndarray, idx1: int, idx2: int, idx3: int, idx4: int, eps: float = 1e-6) -> np.ndarray:\n",
    "    return (np.linalg.norm(lmk[:, idx1] - lmk[:, idx2], axis=1, keepdims=True) /\n",
    "            (np.linalg.norm(lmk[:, idx3] - lmk[:, idx4], axis=1, keepdims=True) + eps))\n",
    "\n",
    "\n",
    "def calc_eye_close_ratio(lmk: np.ndarray, target_eye_ratio: np.ndarray = None) -> np.ndarray:\n",
    "    lefteye_close_ratio = calculate_distance_ratio(lmk, 6, 18, 0, 12)\n",
    "    righteye_close_ratio = calculate_distance_ratio(lmk, 30, 42, 24, 36)\n",
    "    if target_eye_ratio is not None:\n",
    "        return np.concatenate([lefteye_close_ratio, righteye_close_ratio, target_eye_ratio], axis=1)\n",
    "    else:\n",
    "        return np.concatenate([lefteye_close_ratio, righteye_close_ratio], axis=1)\n",
    "\n",
    "\n",
    "def calc_lip_close_ratio(lmk: np.ndarray) -> np.ndarray:\n",
    "    return calculate_distance_ratio(lmk, 90, 102, 48, 66)\n",
    "\n",
    "def calc_ratio(lmk_lst):\n",
    "    input_eye_ratio_lst = []\n",
    "    input_lip_ratio_lst = []\n",
    "    for lmk in lmk_lst:\n",
    "        # for eyes retargeting\n",
    "        input_eye_ratio_lst.append(calc_eye_close_ratio(lmk[None]))\n",
    "        # for lip retargeting\n",
    "        input_lip_ratio_lst.append(calc_lip_close_ratio(lmk[None]))\n",
    "    return input_eye_ratio_lst, input_lip_ratio_lst\n",
    "\n",
    "def prepare_videos_(imgs, device):\n",
    "    \"\"\" construct the input as standard\n",
    "    imgs: NxHxWx3, uint8\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, list):\n",
    "        _imgs = np.array(imgs)\n",
    "    elif isinstance(imgs, np.ndarray):\n",
    "        _imgs = imgs\n",
    "    else:\n",
    "        raise ValueError(f'imgs type error: {type(imgs)}')\n",
    "\n",
    "    # y = _imgs.astype(np.float32) / 255.\n",
    "    y = _imgs\n",
    "    y = torch.from_numpy(y).permute(0, 3, 1, 2)  # NxHxWx3 -> Nx3xHxW\n",
    "    y = y.to(device)\n",
    "    y = y / 255.\n",
    "    y = torch.clamp(y, 0, 1)\n",
    "\n",
    "    return y\n",
    "\n",
    "def get_kp_info(x: torch.Tensor, **kwargs) -> dict:\n",
    "    \"\"\" get the implicit keypoint information\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    flag_refine_info: whether to trandform the pose to degrees and the dimention of the reshape\n",
    "    return: A dict contains keys: 'pitch', 'yaw', 'roll', 't', 'exp', 'scale', 'kp'\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        kp_info = motion_extractor(x)\n",
    "\n",
    "        if inference_cfg.flag_use_half_precision:\n",
    "            # float the dict\n",
    "            for k, v in kp_info.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    kp_info[k] = v.float()\n",
    "\n",
    "    flag_refine_info: bool = kwargs.get('flag_refine_info', True)\n",
    "    if flag_refine_info:\n",
    "        bs = kp_info['kp'].shape[0]\n",
    "        kp_info['pitch'] = headpose_pred_to_degree(kp_info['pitch'])[:, None]  # Bx1\n",
    "        kp_info['yaw'] = headpose_pred_to_degree(kp_info['yaw'])[:, None]  # Bx1\n",
    "        kp_info['roll'] = headpose_pred_to_degree(kp_info['roll'])[:, None]  # Bx1\n",
    "        kp_info['kp'] = kp_info['kp'].reshape(bs, -1, 3)  # BxNx3\n",
    "        kp_info['exp'] = kp_info['exp'].reshape(bs, -1, 3)  # BxNx3\n",
    "\n",
    "    return kp_info\n",
    "\n",
    "def read_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (256, 256))  # Resize to 256x256\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return video_path, frames\n",
    "\n",
    "def read_multiple_videos(video_paths, num_threads=4):\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        results = list(executor.map(read_video_frames, video_paths))\n",
    "    return results\n",
    "\n",
    "def euler_to_quaternion(pitch, yaw, roll):\n",
    "    cy = torch.cos(yaw * 0.5)\n",
    "    sy = torch.sin(yaw * 0.5)\n",
    "    cp = torch.cos(pitch * 0.5)\n",
    "    sp = torch.sin(pitch * 0.5)\n",
    "    cr = torch.cos(roll * 0.5)\n",
    "    sr = torch.sin(roll * 0.5)\n",
    "\n",
    "    w = cr * cp * cy + sr * sp * sy\n",
    "    x = sr * cp * cy - cr * sp * sy\n",
    "    y = cr * sp * cy + sr * cp * sy\n",
    "    z = cr * cp * sy - sr * sp * cy\n",
    "\n",
    "    return torch.stack([w, x, y, z], dim=-1)\n",
    "\n",
    "def quaternion_to_euler(q):\n",
    "    \"\"\"\n",
    "    Convert quaternion to Euler angles (pitch, yaw, roll) in radians.\n",
    "    q: torch.Tensor of shape (..., 4) representing quaternions (w, x, y, z)\n",
    "    Returns: tuple of (pitch, yaw, roll) as torch.Tensor\n",
    "    \"\"\"\n",
    "    # Extract the values from q\n",
    "    w, x, y, z = q[..., 0], q[..., 1], q[..., 2], q[..., 3]\n",
    "\n",
    "    # Roll (x-axis rotation)\n",
    "    sinr_cosp = 2 * (w * x + y * z)\n",
    "    cosr_cosp = 1 - 2 * (x * x + y * y)\n",
    "    roll = torch.atan2(sinr_cosp, cosr_cosp)\n",
    "\n",
    "    # Pitch (y-axis rotation)\n",
    "    sinp = 2 * (w * y - z * x)\n",
    "    pitch = torch.where(\n",
    "        torch.abs(sinp) >= 1,\n",
    "        torch.sign(sinp) * torch.pi / 2,\n",
    "        torch.asin(sinp)\n",
    "    )\n",
    "\n",
    "    # Yaw (z-axis rotation)\n",
    "    siny_cosp = 2 * (w * z + x * y)\n",
    "    cosy_cosp = 1 - 2 * (y * y + z * z)\n",
    "    yaw = torch.atan2(siny_cosp, cosy_cosp)\n",
    "\n",
    "    return pitch, yaw, roll\n",
    "\n",
    "def quaternion_to_euler_degrees(q):\n",
    "    \"\"\"\n",
    "    Convert quaternion to Euler angles (pitch, yaw, roll) in degrees.\n",
    "    q: torch.Tensor of shape (..., 4) representing quaternions (w, x, y, z)\n",
    "    Returns: tuple of (pitch, yaw, roll) as torch.Tensor in degrees\n",
    "    \"\"\"\n",
    "    pitch, yaw, roll = quaternion_to_euler(q)\n",
    "    return torch.rad2deg(pitch), torch.rad2deg(yaw), torch.rad2deg(roll)\n",
    "\n",
    "\n",
    "'''\n",
    "Loading source related modules\n",
    "'''\n",
    "def prepare_source(img: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    img: HxWx3, uint8, 256x256\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    x = img.copy()\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        x = x[np.newaxis].astype(np.float32) / 255.  # HxWx3 -> 1xHxWx3, normalized to 0~1\n",
    "    elif x.ndim == 4:\n",
    "        x = x.astype(np.float32) / 255.  # BxHxWx3, normalized to 0~1\n",
    "    else:\n",
    "        raise ValueError(f'img ndim should be 3 or 4: {x.ndim}')\n",
    "    x = np.clip(x, 0, 1)  # clip to 0~1\n",
    "    x = torch.from_numpy(x).permute(0, 3, 1, 2)  # 1xHxWx3 -> 1x3xHxW\n",
    "    x = x.to(device)\n",
    "    return x\n",
    "\n",
    "def warp_decode(feature_3d: torch.Tensor, kp_source: torch.Tensor, kp_driving: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the image after the warping of the implicit keypoints\n",
    "    feature_3d: Bx32x16x64x64, feature volume\n",
    "    kp_source: BxNx3\n",
    "    kp_driving: BxNx3\n",
    "    \"\"\"\n",
    "    # The line 18 in Algorithm 1: D(W(f_s; x_s, x′_d,i)）\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        # get decoder input\n",
    "        ret_dct = warping_module(feature_3d, kp_source=kp_source, kp_driving=kp_driving)\n",
    "        # decode\n",
    "        ret_dct['out'] = spade_generator(feature=ret_dct['out'])\n",
    "\n",
    "    return ret_dct\n",
    "\n",
    "def extract_feature_3d( x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the appearance feature of the image by F\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        feature_3d = appearance_feature_extractor(x)\n",
    "\n",
    "    return feature_3d.float()\n",
    "\n",
    "def transform_keypoint(kp_info: dict):\n",
    "    \"\"\"\n",
    "    transform the implicit keypoints with the pose, shift, and expression deformation\n",
    "    kp: BxNx3\n",
    "    \"\"\"\n",
    "    kp = kp_info['kp']    # (bs, k, 3)\n",
    "    pitch, yaw, roll = kp_info['pitch'], kp_info['yaw'], kp_info['roll']\n",
    "\n",
    "    t, exp = kp_info['t'], kp_info['exp']\n",
    "    scale = kp_info['scale']\n",
    "\n",
    "    pitch = headpose_pred_to_degree(pitch)\n",
    "    yaw = headpose_pred_to_degree(yaw)\n",
    "    roll = headpose_pred_to_degree(roll)\n",
    "\n",
    "    bs = kp.shape[0]\n",
    "    if kp.ndim == 2:\n",
    "        num_kp = kp.shape[1] // 3  # Bx(num_kpx3)\n",
    "    else:\n",
    "        num_kp = kp.shape[1]  # Bxnum_kpx3\n",
    "\n",
    "    rot_mat = get_rotation_matrix(pitch, yaw, roll)    # (bs, 3, 3)\n",
    "\n",
    "    # Eqn.2: s * (R * x_c,s + exp) + t\n",
    "    kp_transformed = kp.view(bs, num_kp, 3) @ rot_mat + exp.view(bs, num_kp, 3)\n",
    "    kp_transformed *= scale[..., None]  # (bs, k, 3) * (bs, 1, 1) = (bs, k, 3)\n",
    "    kp_transformed[:, :, 0:2] += t[:, None, 0:2]  # remove z, only apply tx ty\n",
    "\n",
    "    return kp_transformed\n",
    "\n",
    "def parse_output(out: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\" construct the output as standard\n",
    "    return: 1xHxWx3, uint8\n",
    "    \"\"\"\n",
    "    out = np.transpose(out.data.cpu().numpy(), [0, 2, 3, 1])  # 1x3xHxW -> 1xHxWx3\n",
    "    out = np.clip(out, 0, 1)  # clip to 0~1\n",
    "    out = np.clip(out * 255, 0, 255).astype(np.uint8)  # 0~1 -> 0~255\n",
    "\n",
    "    return out\n",
    "'''\n",
    "Main module for inference\n",
    "'''\n",
    "model_config = yaml.load(open(inference_cfg.models_config, 'r'), Loader=yaml.SafeLoader)\n",
    "# init F\n",
    "appearance_feature_extractor = load_model(inference_cfg.checkpoint_F, model_config, device, 'appearance_feature_extractor')\n",
    "# init M\n",
    "motion_extractor = load_model(inference_cfg.checkpoint_M, model_config, device, 'motion_extractor')\n",
    "# init W\n",
    "warping_module = load_model(inference_cfg.checkpoint_W, model_config, device, 'warping_module')\n",
    "# init G\n",
    "spade_generator = load_model(inference_cfg.checkpoint_G, model_config, device, 'spade_generator')\n",
    "# init S and R\n",
    "if inference_cfg.checkpoint_S is not None and os.path.exists(inference_cfg.checkpoint_S):\n",
    "    stitching_retargeting_module = load_model(inference_cfg.checkpoint_S, model_config, device, 'stitching_retargeting_module')\n",
    "else:\n",
    "    stitching_retargeting_module = None\n",
    "\n",
    "cropper = Cropper(crop_cfg=crop_cfg, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Move model and processor to global scope\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(device)\n",
    "wav2vec_processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def read_multiple_audios(paths, num_threads=12):\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        results = list(executor.map(load_and_process_audio, paths))\n",
    "    return results\n",
    "\n",
    "\n",
    "def read_json_and_form_paths(data,id_key):\n",
    "    filenames=[]\n",
    "    file_paths = []\n",
    "\n",
    "    # Iterate through the nested structure\n",
    "    for id_key, id_value in data.items():\n",
    "        os.makedirs(os.path.join(DB_ROOT,OUTPUT_DIR,id_key), exist_ok=True)\n",
    "        for url_key, url_value in id_value.items():\n",
    "            for clip_id in url_value.keys():\n",
    "                # Form the file path\n",
    "                file_path = os.path.join(DB_ROOT,AUDIO,id_key, url_key, clip_id.replace('.txt', '.wav'))\n",
    "                file_name = os.path.join(DB_ROOT,OUTPUT_DIR,id_key, url_key+'+'+clip_id.replace('.txt', ''))\n",
    "                filenames.append(file_name)\n",
    "                file_paths.append(file_path)\n",
    "\n",
    "    return file_paths, filenames\n",
    "\n",
    "def load_and_process_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    original_sample_rate = sample_rate\n",
    "\n",
    "    if sample_rate != TARGET_SAMPLE_RATE:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, TARGET_SAMPLE_RATE)\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    print(file_path,\" waveform.shape \",waveform.shape)\n",
    "\n",
    "    # Calculate section length and overlap in samples\n",
    "    section_samples = SECTION_LENGTH * 16027\n",
    "    overlap_samples = int(OVERLAP / FRAME_RATE * TARGET_SAMPLE_RATE)\n",
    "    print('section_samples',section_samples,'overlap_samples',overlap_samples)\n",
    "\n",
    "    # Pad if shorter than 3 seconds\n",
    "    if waveform.shape[1] < section_samples:\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, section_samples - waveform.shape[1]))\n",
    "        return [waveform.squeeze(0)], original_sample_rate\n",
    "\n",
    "    # Split into sections with overlap\n",
    "    sections = []\n",
    "    start = 0\n",
    "\n",
    "    print('starting to segment', file_path)\n",
    "    while start < waveform.shape[1]:\n",
    "        end = start + section_samples\n",
    "        if end >= waveform.shape[1]:\n",
    "            tmp=waveform[:, start:min(end, waveform.shape[1])]\n",
    "            tmp = torch.nn.functional.pad(tmp, (0, section_samples - tmp.shape[1]))\n",
    "            sections.append(tmp.squeeze(0))\n",
    "            print(tmp.shape)\n",
    "            break\n",
    "        else:\n",
    "            sections.append(waveform[:, start:min(end, waveform.shape[1])].squeeze(0))\n",
    "\n",
    "        start = int(end - overlap_samples)\n",
    "\n",
    "\n",
    "    return file_path, sections\n",
    "\n",
    "def inference_process_wav_file(path):\n",
    "    audio_path, segments = load_and_process_audio(path)\n",
    "    print(audio_path,segments)\n",
    "    segments = np.array(segments)\n",
    "\n",
    "    inputs = wav2vec_processor(segments, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\", padding=True).input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = wav2vec_model(inputs)\n",
    "        latent = outputs.last_hidden_state\n",
    "\n",
    "        seq_length = latent.shape[1]\n",
    "        new_seq_length = int(seq_length * (FRAME_RATE / 50))\n",
    "\n",
    "        latent_features_interpolated = F.interpolate(latent.transpose(1,2),\n",
    "                                                     size=new_seq_length,\n",
    "                                                     mode='linear',\n",
    "                                                     align_corners=True).transpose(1,2)\n",
    "    return latent_features_interpolated\n",
    "\n",
    "\n",
    "def process_wav_file(paths, output_paths, uid):\n",
    "    device = torch.device(f\"cuda\")\n",
    "\n",
    "    model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(device)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    read_2_gpu_batch_size = 2048\n",
    "    gpu_batch_size = BATCH_SIZE\n",
    "    process_queue = torch.Tensor().to(device)\n",
    "\n",
    "    audio_segments = read_multiple_audios(paths, num_threads=4)\n",
    "    all_segments = []\n",
    "    total_segments = 0\n",
    "\n",
    "    audio_lengths = []\n",
    "    output_fns = []\n",
    "\n",
    "    for (audio_path, segments), output_fn in zip(audio_segments,output_paths):\n",
    "        all_segments.extend(segments)\n",
    "        segment_count = len(segments)\n",
    "        total_segments += segment_count\n",
    "        audio_lengths.append(segment_count)\n",
    "\n",
    "        output_fns.append(output_fn)\n",
    "\n",
    "    all_segments = np.array(all_segments)\n",
    "    print(all_segments.size)\n",
    "\n",
    "    read_data_2_gpu_pointer = 0\n",
    "    pbar = tqdm(total=total_segments, desc=f\"Processing {uid}\")\n",
    "\n",
    "    while read_data_2_gpu_pointer < total_segments:\n",
    "        current_batch_size = min(read_2_gpu_batch_size, total_segments - read_data_2_gpu_pointer)\n",
    "\n",
    "        batch_input = all_segments[read_data_2_gpu_pointer:read_data_2_gpu_pointer + current_batch_size]\n",
    "\n",
    "        mini_batch_start = 0\n",
    "        all_info = []\n",
    "        while mini_batch_start < batch_input.shape[0]:\n",
    "            mini_batch_end = min(mini_batch_start + gpu_batch_size, batch_input.shape[0])\n",
    "            mini_batch = batch_input[mini_batch_start:mini_batch_end]\n",
    "\n",
    "            inputs = processor(mini_batch, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\", padding=True).input_values.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            latent = outputs.last_hidden_state\n",
    "            print('latent',latent.shape)\n",
    "            seq_length = latent.shape[1]\n",
    "            new_seq_length = int(seq_length * (FRAME_RATE / 50))  # Assuming Wav2Vec2 outputs at ~50Hz\n",
    "\n",
    "            latent_features_interpolated = F.interpolate(latent.transpose(1,2),\n",
    "                                                            size=new_seq_length,\n",
    "                                                            mode='linear',\n",
    "                                                            align_corners=True).transpose(1,2)\n",
    "            print('latent_features_interpolated',latent_features_interpolated.shape)\n",
    "            all_info.append(latent_features_interpolated)\n",
    "\n",
    "            mini_batch_start = mini_batch_end\n",
    "        all_info_tensor = torch.cat(all_info, dim=0)\n",
    "\n",
    "        process_queue = torch.cat((process_queue, all_info_tensor), dim=0)\n",
    "\n",
    "        print(audio_lengths)\n",
    "        while len(output_fns) > 0 and len(process_queue) >= audio_lengths[0]:\n",
    "            current_output_fn = output_fns[0]\n",
    "            current_segment_count = audio_lengths[0]\n",
    "\n",
    "            audio_tensor = process_queue[:current_segment_count]\n",
    "            np.save(current_output_fn, audio_tensor.cpu().numpy())\n",
    "            print('save',current_output_fn)\n",
    "            process_queue = process_queue[current_segment_count:]\n",
    "            output_fns.pop(0)\n",
    "            audio_lengths.pop(0)\n",
    "\n",
    "\n",
    "        read_data_2_gpu_pointer += current_batch_size\n",
    "        pbar.update(current_batch_size)\n",
    "\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DiT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_path exists: True\n",
      "Model checkpoint loaded from D:/Projects/Upenn_CIS_5650/final-project/config/model.pth\n"
     ]
    }
   ],
   "source": [
    "print(\"config_path exists:\", os.path.exists(config_path))\n",
    "audio_model_config = json.load(open(config_path))\n",
    "inference_manager = get_model(config_path, weight_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_s shape torch.Size([1, 3, 256, 256])\n",
      "x_c_s shape torch.Size([1, 21, 3])\n",
      "x_s shape torch.Size([1, 21, 3])\n",
      "f_s shape torch.Size([1, 32, 16, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "img_rgb = load_image_rgb(input_image_path)\n",
    "source_rgb_lst = [img_rgb]\n",
    "\n",
    "source_lmk = cropper.calc_lmk_from_cropped_image(source_rgb_lst[0])\n",
    "img_crop_256x256 = cv2.resize(source_rgb_lst[0], (256, 256))  # force to resize to 256x256\n",
    "\n",
    "I_s = prepare_source(img_crop_256x256)\n",
    "x_s_info = get_kp_info(I_s)\n",
    "x_c_s = x_s_info['kp']\n",
    "x_s = transform_keypoint(x_s_info)\n",
    "f_s = extract_feature_3d(I_s)\n",
    "\n",
    "print(f\"I_s shape {I_s.shape}\")\n",
    "print(f\"x_c_s shape {x_c_s.shape}\")\n",
    "print(f\"x_s shape {x_s.shape}\")\n",
    "print(f\"f_s shape {f_s.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Input Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Projects/Upenn_CIS_5650/final-project/data/audio/test1.wav  waveform.shape  torch.Size([1, 524556])\n",
      "section_samples 48081 overlap_samples 6400\n",
      "starting to segment D:/Projects/Upenn_CIS_5650/final-project/data/audio/test1.wav\n",
      "torch.Size([1, 48081])\n",
      "D:/Projects/Upenn_CIS_5650/final-project/data/audio/test1.wav [tensor([-0.0260, -0.0352, -0.0366,  ..., -0.0090, -0.0072, -0.0050]), tensor([-0.0675, -0.0870, -0.0981,  ...,  0.0172,  0.0209,  0.0255]), tensor([ 0.0080,  0.0071,  0.0077,  ..., -0.0710, -0.0679, -0.0633]), tensor([-0.0154, -0.0187, -0.0255,  ...,  0.0086,  0.0088,  0.0093]), tensor([-0.0096, -0.0090, -0.0085,  ...,  0.0095,  0.0095,  0.0094]), tensor([ 0.0003,  0.0003,  0.0006,  ..., -0.0013, -0.0015, -0.0014]), tensor([-0.0039, -0.0037, -0.0037,  ...,  0.0109,  0.0137,  0.0162]), tensor([-7.6839e-02, -6.5072e-02, -5.5529e-02,  ..., -3.6640e-03,\n",
      "        -1.8851e-03,  2.8846e-06]), tensor([-0.0037, -0.0040, -0.0062,  ..., -0.0164, -0.0237, -0.0263]), tensor([-0.0010, -0.0001,  0.0004,  ..., -0.0705, -0.0654, -0.0543]), tensor([ 0.0045,  0.0042,  0.0038,  ..., -0.0010, -0.0009, -0.0006]), tensor([ 0.0134,  0.0132,  0.0130,  ..., -0.0124, -0.0113, -0.0111]), tensor([-0.0148, -0.0166, -0.0182,  ...,  0.0000,  0.0000,  0.0000])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JackeyTY\\AppData\\Local\\Temp\\ipykernel_60444\\2557965910.py:1: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "d:\\Projects\\Upenn_CIS_5650\\final-project\\LivePortrait\\fp\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:863: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 75, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "custom_audio_latent = inference_process_wav_file(input_audio_path)\n",
    "\n",
    "custom_audio_latent.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Motion based on audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 7, 22, 33, 34, 40, 43, 45, 46, 48, 51, 52, 53, 57, 58, 59, 60, 61, 62] [-0.05029296875, 0.0857086181640625, -0.07587742805480957, 0.058624267578125, -0.0004341602325439453, 0.00019466876983642578, -0.038482666015625, 0.0345458984375, -0.030120849609375, 0.038360595703125, -3.0279159545898438e-05, 1.3887882232666016e-05, -0.0364990234375, 0.036102294921875, -0.043212890625, 0.046844482421875, -4.3332576751708984e-05, 1.8775463104248047e-05, -0.03326416015625, 0.057373046875, -0.03460693359375, 0.031707763671875, -0.0001958608627319336, 0.0005192756652832031, -0.0728759765625, 0.0587158203125, -0.04840087890625, 0.039642333984375, -0.00025916099548339844, 0.00048089027404785156, -0.09722900390625, 0.12469482421875, -0.1556396484375, 0.09326171875, -0.00018024444580078125, 0.00037860870361328125, -0.0279384758323431, 0.010650634765625, -0.039306640625, 0.03802490234375, -1.049041748046875e-05, 3.6954879760742188e-06, -0.032989501953125, 0.044281005859375, -0.037261962890625, 0.0433349609375, -0.00022792529489379376, 0.0003247261047363281, -0.0288234855979681, 0.006015777587890625, -0.0108795166015625, 0.0134124755859375, -7.784366607666016e-05, 5.2034854888916016e-05, -0.01531982421875, 0.027801513671875, -0.036041259765625, 0.0242156982421875, -8.83340835571289e-05, 2.6464462280273438e-05, -0.06463623046875, 0.0303802490234375, -0.0446159653365612, 0.03619384765625, -0.02947998046875, 0.030792236328125, -0.0159145500510931, 0.018890380859375, -0.01898193359375, 0.0264739990234375, -6.103515625e-05, 3.266334533691406e-05, -0.0094450069591403, 0.00604248046875, -0.005710510071367025, 0.00557708740234375, -2.866983413696289e-05, 1.4543533325195312e-05, -0.0265350341796875, 0.01186370849609375, -0.0227047111839056, 0.01386260986328125, -0.000133514404296875, 6.687641143798828e-05, -0.01129150390625, 0.01331329345703125, -0.0251922607421875, 0.0195465087890625, -8.285045623779297e-06, 6.079673767089844e-06, -0.0141599727794528, 0.018341064453125, -0.0189971923828125, 0.029296875, -6.049728108337149e-05, 3.057718276977539e-05, -0.01216888427734375, 0.02069091796875, -0.016754150390625, 0.017974853515625, -0.00014078617095947266, 6.842613220214844e-05, -0.01910400390625, 0.016204833984375, -0.025634765625, 0.04150390625, -0.0100250244140625, 0.00991058349609375, -0.005596160888671875, 0.01132965087890625, -0.0269775390625, 0.02166748046875, -0.000362396240234375, 9.059906005859375e-05, -0.0325927734375, 0.038818359375, -0.05877685546875, 0.076416015625, -0.02215576171875, 0.019775390625, -0.0219573974609375, 0.0247344970703125, -0.039764404296875, 0.045, -0.01512908935546875, 0.017730712890625]\n",
      "Audio input shape: torch.Size([13, 65, 768])\n",
      "Audio previous shape: torch.Size([13, 10, 768])\n",
      "Audio input shape: torch.Size([13, 65, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([13, 65, 20]), torch.Size([1, 10, 20]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_audio_example = input_audio_path\n",
    "audio_latent = custom_audio_latent\n",
    "\n",
    "audio_latent_input = audio_latent\n",
    "latent_mask_used=audio_model_config['latent_mask_1']\n",
    "latent_bound = audio_model_config['latent_bound']\n",
    "print(latent_mask_used, latent_bound)\n",
    "\n",
    "audio_seq = audio_latent_input[:, 10:, :]\n",
    "audio_prev = audio_latent_input[:, :10, :]\n",
    "\n",
    "print(\"Audio input shape:\", audio_seq.shape)\n",
    "print(\"Audio previous shape:\", audio_prev.shape)\n",
    "\n",
    "motion_prev = torch.zeros(audio_latent.shape[0], 10, 6, device=device)\n",
    "\n",
    "mouth_open_ratio_val = 0.25\n",
    "mouth_open_ratio_input = torch.tensor([mouth_open_ratio_val], device=device).unsqueeze(0)\n",
    "out_motion = torch.tensor([], device=device)\n",
    "B, T, audio_dim = audio_seq.shape\n",
    "motion_dim = audio_model_config['x_dim']\n",
    "shape_in = x_c_s.reshape(1, -1).to(device)\n",
    "this_audio_prev = torch.zeros(1, 10, audio_dim, device=device)\n",
    "this_motion_prev = torch.zeros(1, 10, motion_dim , device=device)\n",
    "motion_prev = torch.zeros(1, 10, motion_dim , device=device)\n",
    "print(\"Audio input shape:\", audio_seq.shape)\n",
    "for batch_index in range(0, audio_seq.shape[0]):\n",
    "    generated_motion = inference_manager.inference(audio_seq[batch_index:batch_index+1],\n",
    "                                                shape_in, this_motion_prev, this_audio_prev,\n",
    "                                                cfg_scale=0.25,\n",
    "                                                mouth_open_ratio = mouth_open_ratio_input,\n",
    "                                                denoising_steps=5)\n",
    "    this_motion_prev = generated_motion[:, -10:, :]\n",
    "    this_audio_prev = audio_seq[batch_index:batch_index+1, -10:, :]\n",
    "\n",
    "    generated_motion = generated_motion - torch.mean(generated_motion, dim=-1, keepdim=True)\n",
    "    out_motion = torch.cat((out_motion, generated_motion), dim=0)\n",
    "\n",
    "generated_motion = out_motion\n",
    "generated_motion.shape, motion_prev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Frames Using Generated Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose torch.Size([855, 5]) exp torch.Size([855, 20])\n",
      "0 4\n",
      "1 6\n",
      "2 7\n",
      "3 22\n",
      "4 33\n",
      "5 34\n",
      "6 40\n",
      "7 43\n",
      "8 45\n",
      "9 46\n",
      "10 48\n",
      "11 51\n",
      "12 52\n",
      "13 53\n",
      "14 57\n",
      "15 58\n",
      "16 59\n",
      "17 60\n",
      "18 61\n",
      "19 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating x_d:   0%|          | 0/855 [00:00<?, ?it/s]C:\\Users\\JackeyTY\\AppData\\Local\\Temp\\ipykernel_60444\\1505282800.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t, device=device)\n",
      "Generating x_d: 100%|██████████| 855/855 [00:00<00:00, 2107.37it/s]\n",
      "Processing batches: 100%|██████████| 214/214 [00:21<00:00, 10.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Assuming generated_motion is your output from dit_inference\n",
    "# generated_motion shape: [2, 65, 63]\n",
    "# motion_prev shape: [2, 10, 63]\n",
    "\n",
    "def process_motion_batch(gen_motion_batch, motion_prev, f_s, x_s, warp_decode_func):\n",
    "    frames = []\n",
    "    B, T, feat_count = gen_motion_batch.shape\n",
    "    full_motion = gen_motion_batch.reshape(B*T, feat_count)\n",
    "    full_motion = torch.cat([motion_prev[0], full_motion], dim=0)\n",
    "\n",
    "    pose = full_motion[:, -5:]\n",
    "    exp = full_motion[:, :]\n",
    "    print(\"pose\", pose.shape, \"exp\", exp.shape)\n",
    "    # exp_mask = torch.zeros_like(full_motion[0][0])\n",
    "    # pos_mouth = [14, 17, 19, 20]\n",
    "    # eye and mouth [15, 16, 18]\n",
    "    # pos_eye & forehead = [1, 2, 11, 12, 13] # 1.z is mouth, 12 small eye\n",
    "    # shape [0, 3, 4, 7, 8, 9, 10] # may include shape dependent pose\n",
    "    # pos_cloth = [5, 6]\n",
    "\n",
    "    # for p in pos_mouth:\n",
    "    #     exp_mask[p * 3:(p + 1) * 3] = 1\n",
    "\n",
    "    # # exp_mask = exp_mask.reshape(21,3)\n",
    "    # print(full_motion[0, 10:15, :] * exp_mask)\n",
    "\n",
    "    t_identity = torch.zeros((1, 3), dtype=torch.float32, device=device)\n",
    "    pitch_identity = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "    yaw_identity = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "    roll_identity = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "    scale_identity = torch.ones((1), dtype=torch.float32, device=device) * 1.5\n",
    "\n",
    "    use_identity_pose = True\n",
    "    if use_identity_pose:\n",
    "        t_s = t_identity\n",
    "        pitch_s = pitch_identity\n",
    "        yaw_s = yaw_identity\n",
    "        roll_s = roll_identity\n",
    "        scale_s = scale_identity\n",
    "    else:\n",
    "        t_s = x_s_info['t']\n",
    "        pitch_s = x_s_info['pitch']\n",
    "        yaw_s = x_s_info['yaw']\n",
    "        roll_s = x_s_info['roll']\n",
    "        scale_s = x_s_info['scale']\n",
    "    t = t_s\n",
    "    pitch = pitch_s\n",
    "    yaw = yaw_s\n",
    "    roll = roll_s\n",
    "    scale = scale_s\n",
    "\n",
    "    full_63_exp = torch.zeros(full_motion.shape[0], 63, device=device)\n",
    "    for i, dim in enumerate(audio_model_config['latent_mask_1']):\n",
    "        print(i, dim)\n",
    "        full_63_exp[:, dim] = exp[:, i]\n",
    "    full_motion = full_63_exp.reshape(-1, 63)\n",
    "\n",
    "    x_d_list = []\n",
    "\n",
    "    for i in tqdm(range(full_motion.shape[0]), desc=\"Generating x_d\"):\n",
    "        motion = full_motion[i].reshape(21, 3)\n",
    "\n",
    "        # Initialize empty tensors\n",
    "\n",
    "        # # Extract values from motion\n",
    "        exp = motion #* exp_mask\n",
    "        # pitch = pose[i, 0:1]\n",
    "        # yaw = pose[i, 1:2]\n",
    "        # roll = pose[i, 2:3]\n",
    "        # t_x = pose[i, 3:4]\n",
    "        # t_y = pose[i, 4:5]\n",
    "        t = torch.tensor(t, device=device)\n",
    "\n",
    "        x_d_i = scale * (x_c_s @ get_rotation_matrix(pitch, yaw, roll) + exp) + t\n",
    "        x_d_list.append(x_d_i.squeeze(0))\n",
    "\n",
    "    x_d_batch = torch.stack(x_d_list, dim=0)\n",
    "    f_s_batch = f_s.expand(x_d_batch.shape[0], -1, -1, -1, -1)\n",
    "    x_s_batch = x_s.expand(x_d_batch.shape[0], -1, -1)\n",
    "\n",
    "    inference_batch_size = 4\n",
    "    num_batches = (x_d_batch.shape[0] + inference_batch_size - 1) // inference_batch_size\n",
    "\n",
    "    frames = []\n",
    "    for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        start_idx = i * inference_batch_size\n",
    "        end_idx = min((i + 1) * inference_batch_size, x_d_batch.shape[0])\n",
    "\n",
    "        batch_f_s = f_s_batch[start_idx:end_idx]\n",
    "        batch_x_s = x_s_batch[start_idx:end_idx]\n",
    "        batch_x_d = x_d_batch[start_idx:end_idx]\n",
    "\n",
    "        out = warp_decode_func(batch_f_s, batch_x_s, batch_x_d)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        batch_frames = (out['out'].permute(0, 2, 3, 1).cpu().numpy() * 255).astype(np.uint8)\n",
    "        frames.extend(list(batch_frames))\n",
    "\n",
    "    return frames\n",
    "\n",
    "# Process the motion\n",
    "all_frames = process_motion_batch(generated_motion, motion_prev, f_s, x_s, warp_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Output as MP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video with audio saved to D:/Projects/Upenn_CIS_5650/final-project/LivePortrait/inference/animations/test1_with_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "# Remove the files if they exist\n",
    "if os.path.exists(output_no_audio_path):\n",
    "    os.remove(output_no_audio_path)\n",
    "if os.path.exists(output_video):\n",
    "    os.remove(output_video)\n",
    "fps = 25  # Adjust as needed\n",
    "\n",
    "height, width, layers = all_frames[0].shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video = cv2.VideoWriter(output_no_audio_path, fourcc, fps, (width, height))\n",
    "\n",
    "for frame in all_frames:\n",
    "    video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "video.release()\n",
    "\n",
    "# Add audio to the video using ffmpeg\n",
    "input_video = output_no_audio_path\n",
    "input_audio = used_audio_example  # Use the path to your audio file\n",
    "\n",
    "ffmpeg_cmd = [\n",
    "    'ffmpeg',\n",
    "    '-i', input_video,\n",
    "    '-i', input_audio,\n",
    "    '-c:v', 'copy',\n",
    "    '-c:a', 'aac',\n",
    "    '-shortest',\n",
    "    output_video\n",
    "]\n",
    "\n",
    "try:\n",
    "    subprocess.run(ffmpeg_cmd, check=True)\n",
    "    os.remove(output_no_audio_path)\n",
    "    print(f\"Video with audio saved to {output_video}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error adding audio to video: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
