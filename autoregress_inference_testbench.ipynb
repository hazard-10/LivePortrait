{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Decide input combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_syncnet_only = False\n",
    "# doesn't run motion generation\n",
    "sync_only_vid_root_dir = '/mnt/e/wsl_projects/LivePortrait/sync_output/2024-12-01-20-37_null_weight_1_ep_55_len_5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_syncnet_with_motion_gen = False\n",
    "plot_motion = False\n",
    "write_vid = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_context_len = 67\n",
    "gen_len_per_window = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "portrait_imgs = [\n",
    "    '/mnt/c/Users/mjh/Downloads/live_in/t4.jpg'\n",
    "]\n",
    "audio_paths = [\n",
    "    # '/mnt/c/Users/mjh/Downloads/live_in/i3.wav',\n",
    "    # '/mnt/c/Users/mjh/Downloads/live_in/i5.wav',\n",
    "    # '/mnt/c/Users/mjh/Downloads/live_in/i7.wav',\n",
    "    '/mnt/c/Users/mjh/Downloads/live_in/i8.wav'\n",
    "    # '/mnt/c/Users/mjh/Downloads/live_in/speech.wav'\n",
    "]\n",
    "model_weights_pairs = [\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/model_1023.pth'),\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/model_sterotype_0_125.pth'),\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/model_sterotype_1_140.pth'),\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/model_1111.pth'),\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/model_0.2_2.pth'),\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/model_5_5.pth'),\n",
    "    # ('audio_dit/output/config.json', 'audio_dit/output/redeem_5_vel_5_acc_ep_90.pth'),\n",
    "    ('audio_dit/output/config.json', 'audio_dit/output/norm_no_vel_ep_60.pth'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_scale_opts = [\n",
    "    # 0,\n",
    "    # 0.25,\n",
    "    # 0.5,\n",
    "    0.65,\n",
    "    # 1,\n",
    "    # 1.5,\n",
    "    # 2\n",
    "]\n",
    "mouth_open_ratio_opts = [\n",
    "    # 0.1,\n",
    "    # 0.15,\n",
    "    # 0.2,\n",
    "    # 0.225,\n",
    "    0.25,\n",
    "    # 0.275,\n",
    "    # 0.3\n",
    "]\n",
    "subtract_avg_motion_opts = [\n",
    "    False,\n",
    "    # True\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate Videos & Eval Syncnet Each outpout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import liveportrait modules\n",
    "import time\n",
    "import os\n",
    "import contextlib\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "import tyro\n",
    "import subprocess\n",
    "from rich.progress import track\n",
    "import torchvision\n",
    "import cv2\n",
    "import threading\n",
    "import queue\n",
    "import torchvision.transforms as transforms\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import imageio\n",
    "\n",
    "from src.config.argument_config import ArgumentConfig\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.config.crop_config import CropConfig\n",
    "\n",
    "def partial_fields(target_class, kwargs):\n",
    "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
    "\n",
    "args = ArgumentConfig()\n",
    "inference_cfg = partial_fields(InferenceConfig, args.__dict__)\n",
    "crop_cfg = partial_fields(CropConfig, args.__dict__)\n",
    "# print(\"inference_cfg: \", inference_cfg)\n",
    "# print(\"crop_cfg: \", crop_cfg)\n",
    "device = 'cuda'\n",
    "print(\"Compile complete\")\n",
    "\n",
    "'''\n",
    "Common modules\n",
    "'''\n",
    "from src.utils.helper import load_model\n",
    "from src.utils.camera import headpose_pred_to_degree, get_rotation_matrix\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.utils.cropper import Cropper\n",
    "from src.utils.camera import get_rotation_matrix\n",
    "from src.utils.io import load_image_rgb\n",
    "\n",
    "'''\n",
    "Main module for inference\n",
    "'''\n",
    "model_config = yaml.load(open(inference_cfg.models_config, 'r'), Loader=yaml.SafeLoader)\n",
    "# init F\n",
    "appearance_feature_extractor = load_model(inference_cfg.checkpoint_F, model_config, device, 'appearance_feature_extractor')\n",
    "# init M\n",
    "motion_extractor = load_model(inference_cfg.checkpoint_M, model_config, device, 'motion_extractor')\n",
    "# init W\n",
    "warping_module = load_model(inference_cfg.checkpoint_W, model_config, device, 'warping_module')\n",
    "# init G\n",
    "spade_generator = load_model(inference_cfg.checkpoint_G, model_config, device, 'spade_generator')\n",
    "# init S and R\n",
    "if inference_cfg.checkpoint_S is not None and os.path.exists(inference_cfg.checkpoint_S):\n",
    "    stitching_retargeting_module = load_model(inference_cfg.checkpoint_S, model_config, device, 'stitching_retargeting_module')\n",
    "else:\n",
    "    stitching_retargeting_module = None\n",
    "\n",
    "cropper = Cropper(crop_cfg=crop_cfg, device=device)\n",
    "\n",
    "'''\n",
    "Main function for inference\n",
    "'''\n",
    "\n",
    "def get_kp_info(x: torch.Tensor, **kwargs) -> dict:\n",
    "    \"\"\" get the implicit keypoint information\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    flag_refine_info: whether to trandform the pose to degrees and the dimention of the reshape\n",
    "    return: A dict contains keys: 'pitch', 'yaw', 'roll', 't', 'exp', 'scale', 'kp'\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        kp_info = motion_extractor(x)\n",
    "\n",
    "        if inference_cfg.flag_use_half_precision:\n",
    "            # float the dict\n",
    "            for k, v in kp_info.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    kp_info[k] = v.float()\n",
    "\n",
    "    flag_refine_info: bool = kwargs.get('flag_refine_info', True)\n",
    "    if flag_refine_info:\n",
    "        bs = kp_info['kp'].shape[0]\n",
    "        kp_info['pitch'] = headpose_pred_to_degree(kp_info['pitch'])[:, None]  # Bx1\n",
    "        kp_info['yaw'] = headpose_pred_to_degree(kp_info['yaw'])[:, None]  # Bx1\n",
    "        kp_info['roll'] = headpose_pred_to_degree(kp_info['roll'])[:, None]  # Bx1\n",
    "        kp_info['kp'] = kp_info['kp'].reshape(bs, -1, 3)  # BxNx3\n",
    "        kp_info['exp'] = kp_info['exp'].reshape(bs, -1, 3)  # BxNx3\n",
    "\n",
    "    return kp_info\n",
    "\n",
    "def prepare_source(img: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\" construct the input as standard\n",
    "    img: HxWx3, uint8, 256x256\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    x = img.copy()\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        x = x[np.newaxis].astype(np.float32) / 255.  # HxWx3 -> 1xHxWx3, normalized to 0~1\n",
    "    elif x.ndim == 4:\n",
    "        x = x.astype(np.float32) / 255.  # BxHxWx3, normalized to 0~1\n",
    "    else:\n",
    "        raise ValueError(f'img ndim should be 3 or 4: {x.ndim}')\n",
    "    x = np.clip(x, 0, 1)  # clip to 0~1\n",
    "    x = torch.from_numpy(x).permute(0, 3, 1, 2)  # 1xHxWx3 -> 1x3xHxW\n",
    "    x = x.to(device)\n",
    "    return x\n",
    "\n",
    "def warp_decode(feature_3d: torch.Tensor, kp_source: torch.Tensor, kp_driving: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the image after the warping of the implicit keypoints\n",
    "    feature_3d: Bx32x16x64x64, feature volume\n",
    "    kp_source: BxNx3\n",
    "    kp_driving: BxNx3\n",
    "    \"\"\"\n",
    "    # The line 18 in Algorithm 1: D(W(f_s; x_s, x′_d,i)）\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        # get decoder input\n",
    "        ret_dct = warping_module(feature_3d, kp_source=kp_source, kp_driving=kp_driving)\n",
    "        # decode\n",
    "        ret_dct['out'] = spade_generator(feature=ret_dct['out'])\n",
    "\n",
    "    return ret_dct\n",
    "\n",
    "def extract_feature_3d( x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" get the appearance feature of the image by F\n",
    "    x: Bx3xHxW, normalized to 0~1\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16,\n",
    "                                 enabled=inference_cfg.flag_use_half_precision):\n",
    "        feature_3d = appearance_feature_extractor(x)\n",
    "\n",
    "    return feature_3d.float()\n",
    "\n",
    "def transform_keypoint(kp_info: dict):\n",
    "    \"\"\"\n",
    "    transform the implicit keypoints with the pose, shift, and expression deformation\n",
    "    kp: BxNx3\n",
    "    \"\"\"\n",
    "    kp = kp_info['kp']    # (bs, k, 3)\n",
    "    pitch, yaw, roll = kp_info['pitch'], kp_info['yaw'], kp_info['roll']\n",
    "\n",
    "    t, exp = kp_info['t'], kp_info['exp']\n",
    "    scale = kp_info['scale']\n",
    "\n",
    "    pitch = headpose_pred_to_degree(pitch)\n",
    "    yaw = headpose_pred_to_degree(yaw)\n",
    "    roll = headpose_pred_to_degree(roll)\n",
    "\n",
    "    bs = kp.shape[0]\n",
    "    if kp.ndim == 2:\n",
    "        num_kp = kp.shape[1] // 3  # Bx(num_kpx3)\n",
    "    else:\n",
    "        num_kp = kp.shape[1]  # Bxnum_kpx3\n",
    "\n",
    "    rot_mat = get_rotation_matrix(pitch, yaw, roll)    # (bs, 3, 3)\n",
    "\n",
    "    # Eqn.2: s * (R * x_c,s + exp) + t\n",
    "    kp_transformed = kp.view(bs, num_kp, 3) @ rot_mat + exp.view(bs, num_kp, 3)\n",
    "    kp_transformed *= scale[..., None]  # (bs, k, 3) * (bs, 1, 1) = (bs, k, 3)\n",
    "    kp_transformed[:, :, 0:2] += t[:, None, 0:2]  # remove z, only apply tx ty\n",
    "\n",
    "    return kp_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wav2vec modules\n",
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\"\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "FRAME_RATE = 25\n",
    "SECTION_LENGTH = 3\n",
    "OVERLAP = 10\n",
    "\n",
    "DB_ROOT = 'vox2-audio-tx'\n",
    "LOG = 'log'\n",
    "AUDIO = 'audio/audio'\n",
    "OUTPUT_DIR = 'audio_encoder_output'\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "# Move model and processor to global scope\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained(MODEL_NAME).to(device)\n",
    "wav2vec_processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def load_and_process_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    original_sample_rate = sample_rate\n",
    "\n",
    "    if sample_rate != TARGET_SAMPLE_RATE:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, TARGET_SAMPLE_RATE)\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    # print(file_path,\" waveform.shape \",waveform.shape)\n",
    "\n",
    "    # Calculate section length and overlap in samples\n",
    "    section_samples = SECTION_LENGTH * 16027\n",
    "    overlap_samples = int(OVERLAP / FRAME_RATE * TARGET_SAMPLE_RATE)\n",
    "    # print('section_samples',section_samples,'overlap_samples',overlap_samples)\n",
    "\n",
    "    # pad 10 overlap at the beginning\n",
    "    waveform = torch.nn.functional.pad(waveform, (overlap_samples, 0))\n",
    "    # Pad if shorter than 3 seconds\n",
    "    if waveform.shape[1] < section_samples:\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, section_samples - waveform.shape[1]))\n",
    "        return [waveform.squeeze(0)], original_sample_rate\n",
    "\n",
    "    # Split into sections with overlap\n",
    "    sections = []\n",
    "    start = 0\n",
    "\n",
    "    # print('starting to segment', file_path)\n",
    "    while start < waveform.shape[1]:\n",
    "        end = start + section_samples\n",
    "        if end >= waveform.shape[1]:\n",
    "            tmp=waveform[:, start:min(end, waveform.shape[1])]\n",
    "            tmp = torch.nn.functional.pad(tmp, (0, section_samples - tmp.shape[1]))\n",
    "            sections.append(tmp.squeeze(0))\n",
    "            # print(tmp.shape)\n",
    "            break\n",
    "        else:\n",
    "            sections.append(waveform[:, start:min(end, waveform.shape[1])].squeeze(0))\n",
    "\n",
    "        start = int(end - overlap_samples)\n",
    "\n",
    "\n",
    "    return file_path, sections\n",
    "\n",
    "def inference_process_wav_file(path):\n",
    "    audio_path, segments = load_and_process_audio(path)\n",
    "    # print(audio_path,segments)\n",
    "    segments = np.array(segments)\n",
    "\n",
    "    inputs = wav2vec_processor(segments, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\", padding=True).input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = wav2vec_model(inputs)\n",
    "        latent = outputs.last_hidden_state\n",
    "\n",
    "        seq_length = latent.shape[1]\n",
    "        new_seq_length = int(seq_length * (FRAME_RATE / 50))\n",
    "\n",
    "        latent_features_interpolated = F.interpolate(latent.transpose(1,2),\n",
    "                                                     size=new_seq_length,\n",
    "                                                     mode='linear',\n",
    "                                                     align_corners=True).transpose(1,2)\n",
    "    return latent_features_interpolated\n",
    "\n",
    "def autoregress_load_and_process_audio(file_path):\n",
    "    first_segment_prev_length = 10\n",
    "    first_segment_main_length = 65\n",
    "    remaining_segment_prev_length = prev_context_len\n",
    "    remaining_segment_main_length = gen_len_per_window\n",
    "\n",
    "    # below is the same as load_and_process_audio\n",
    "    waveform, og_sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    if og_sample_rate != TARGET_SAMPLE_RATE:\n",
    "        waveform = torchaudio.functional.resample(waveform, og_sample_rate, TARGET_SAMPLE_RATE)\n",
    "\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "    # define sample count\n",
    "    per_window_samples = SECTION_LENGTH * 16027\n",
    "    first_prev_samples = int(first_segment_prev_length * 16027 / FRAME_RATE)\n",
    "    remaining_overlap_samples = int(remaining_segment_prev_length / FRAME_RATE * TARGET_SAMPLE_RATE)\n",
    "    # pad 10 overlap at the beginning\n",
    "    waveform = torch.nn.functional.pad(waveform, (first_prev_samples, 0))\n",
    "\n",
    "    # split into windows with overlap\n",
    "    windows = []\n",
    "    start = 0\n",
    "\n",
    "    total_sample_count = waveform.shape[1]\n",
    "    while start < total_sample_count:\n",
    "        end = start + per_window_samples\n",
    "        if end >= total_sample_count: # need to pad since last exceeds total sample count\n",
    "            tmp = waveform[:, start:min(end, total_sample_count)]\n",
    "            tmp = torch.nn.functional.pad(tmp, (0, per_window_samples - tmp.shape[1]))\n",
    "            windows.append(tmp.squeeze(0))\n",
    "            break\n",
    "        else:\n",
    "            windows.append(waveform[:, start:min(end, total_sample_count)].squeeze(0))\n",
    "        start = int(end - remaining_overlap_samples)\n",
    "\n",
    "    return windows\n",
    "\n",
    "def autoregress_inference_process_wav_file(path):\n",
    "    windows = autoregress_load_and_process_audio(path)\n",
    "    windows = np.array(windows)\n",
    "\n",
    "    inputs = wav2vec_processor(windows, sampling_rate=TARGET_SAMPLE_RATE, return_tensors=\"pt\", padding=True).input_values.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = wav2vec_model(inputs)\n",
    "        latent = outputs.last_hidden_state\n",
    "\n",
    "        seq_length = latent.shape[1]\n",
    "        new_seq_length = int(seq_length * (FRAME_RATE / 50))\n",
    "\n",
    "        latent_features_interpolated = F.interpolate(latent.transpose(1,2),\n",
    "                                                     size=new_seq_length,\n",
    "                                                     mode='linear',\n",
    "                                                     align_corners=True).transpose(1,2)\n",
    "    return latent_features_interpolated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dit modules\n",
    "from audio_dit.inference import InferenceManager, get_model\n",
    "from audio_dit.dataset import load_and_process_pair\n",
    "\n",
    "def process_motion_batch(gen_motion_batch, f_s, x_s, x_c_s, x_s_info, audio_model_config, warp_decode_func):\n",
    "    frames = []\n",
    "    B, feat_count = gen_motion_batch.shape\n",
    "    full_motion = gen_motion_batch.reshape(B, feat_count)\n",
    "    # full_motion = torch.cat([motion_prev[0], full_motion], dim=0)\n",
    "    if audio_model_config['use_headpose']:\n",
    "        pose = full_motion[:, -5:]\n",
    "        exp = full_motion[:, :-5]\n",
    "    # print(\"pose\", pose.shape, \"exp\", exp.shape)\n",
    "    # exp_mask = torch.zeros_like(full_motion[0][0])\n",
    "    # pos_mouth = [14, 17, 19, 20]\n",
    "    # eye and mouth [15, 16, 18]\n",
    "    # pos_eye & forehead = [1, 2, 11, 12, 13] # 1.z is mouth, 12 small eye\n",
    "    # shape [0, 3, 4, 7, 8, 9, 10] # may include shape dependent pose\n",
    "    # pos_cloth = [5, 6]\n",
    "\n",
    "    # for p in pos_mouth:\n",
    "    #     exp_mask[p * 3:(p + 1) * 3] = 1\n",
    "\n",
    "    # # exp_mask = exp_mask.reshape(21,3)\n",
    "    # print(full_motion[0, 10:15, :] * exp_mask)\n",
    "\n",
    "    t_identity = torch.zeros((1, 3), dtype=torch.float32, device=device)\n",
    "    pitch_identity = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "    yaw_identity = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "    roll_identity = torch.zeros((1), dtype=torch.float32, device=device)\n",
    "    scale_identity = torch.ones((1), dtype=torch.float32, device=device) * 1.5\n",
    "\n",
    "    if not audio_model_config['use_headpose']:\n",
    "        t_s = x_s_info['t']\n",
    "        pitch_s = x_s_info['pitch'] - 10\n",
    "        yaw_s = yaw_identity\n",
    "        roll_s = roll_identity\n",
    "        scale_s = x_s_info['scale']\n",
    "\n",
    "    full_63_exp = torch.zeros(full_motion.shape[0], 63, device=device)\n",
    "    for i, dim in enumerate(audio_model_config['latent_mask_1']):\n",
    "        # print(i, dim)\n",
    "        full_63_exp[:, dim] = exp[:, i]\n",
    "    full_motion = full_63_exp.reshape(-1, 63)\n",
    "\n",
    "    x_d_list = []\n",
    "\n",
    "    for i in tqdm(range(full_motion.shape[0]), desc=\"Generating x_d\"):\n",
    "        motion = full_motion[i].reshape(21, 3)\n",
    "        pose_i = pose[i].unsqueeze(0)\n",
    "\n",
    "        # Initialize empty tensors\n",
    "\n",
    "        # # Extract values from motion\n",
    "        exp = motion\n",
    "        if audio_model_config['use_headpose']:\n",
    "            pitch, yaw, roll, t_x, t_y = pose_i.unbind(-1)  # or pose_i.tolist()\n",
    "            t = torch.tensor([t_x, t_y, 0], device=device)\n",
    "            scale = x_s_info['scale']\n",
    "            # print(\"pitch, yaw, roll shape\", pitch.shape, yaw.shape, roll.shape, pose_i.shape)\n",
    "        else:\n",
    "            t = t_s\n",
    "            pitch = pitch_s\n",
    "            yaw = yaw_s\n",
    "            roll = roll_s\n",
    "            scale = scale_s\n",
    "            t = torch.tensor(t, device=device)\n",
    "\n",
    "        x_d_i = scale * (x_c_s @ get_rotation_matrix(pitch, yaw, roll) + exp) + t\n",
    "        x_d_list.append(x_d_i.squeeze(0))\n",
    "\n",
    "    x_d_batch = torch.stack(x_d_list, dim=0)\n",
    "    f_s_batch = f_s.expand(x_d_batch.shape[0], -1, -1, -1, -1)\n",
    "    x_s_batch = x_s.expand(x_d_batch.shape[0], -1, -1)\n",
    "\n",
    "    inference_batch_size = 4\n",
    "    num_batches = (x_d_batch.shape[0] + inference_batch_size - 1) // inference_batch_size\n",
    "\n",
    "    frames = []\n",
    "    for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        start_idx = i * inference_batch_size\n",
    "        end_idx = min((i + 1) * inference_batch_size, x_d_batch.shape[0])\n",
    "\n",
    "        batch_f_s = f_s_batch[start_idx:end_idx]\n",
    "        batch_x_s = x_s_batch[start_idx:end_idx]\n",
    "        batch_x_d = x_d_batch[start_idx:end_idx]\n",
    "\n",
    "        out = warp_decode_func(batch_f_s, batch_x_s, batch_x_d)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        batch_frames = (out['out'].permute(0, 2, 3, 1).cpu().numpy() * 255).astype(np.uint8)\n",
    "        frames.extend(list(batch_frames))\n",
    "\n",
    "    return frames\n",
    "\n",
    "def write_video(all_frames, audio_path, output_path):\n",
    "    output_no_audio_path = 'output/audio_driven_video_no_audio.mp4'\n",
    "    output_video = output_path\n",
    "\n",
    "    # Remove the files if they exist\n",
    "    if os.path.exists(output_no_audio_path):\n",
    "        os.remove(output_no_audio_path)\n",
    "    if os.path.exists(output_video):\n",
    "        os.remove(output_video)\n",
    "    fps = 25  # Adjust as needed\n",
    "\n",
    "    height, width, layers = all_frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_no_audio_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in all_frames:\n",
    "        video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    video.release()\n",
    "\n",
    "    # Add audio to the video using ffmpeg\n",
    "    input_video = output_no_audio_path\n",
    "    input_audio = audio_path  # Use the path to your audio file\n",
    "\n",
    "    ffmpeg_cmd = [\n",
    "        'ffmpeg',\n",
    "        '-i', input_video,\n",
    "        '-i', input_audio,\n",
    "        '-c:v', 'copy',\n",
    "        '-c:a', 'aac',\n",
    "        '-shortest',\n",
    "        output_video\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(ffmpeg_cmd, check=True)\n",
    "        os.remove(output_no_audio_path)\n",
    "        # print(f\"Video with audio saved to {output_video}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error adding audio to video: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blink noise\n",
    "def fixed_blink_noise(motion_tensor, motion_gt):\n",
    "    eye_indices = torch.tensor([5, 9], device=motion_tensor.device)\n",
    "    rest_latent = torch.tensor([-0.005], device=motion_tensor.device)\n",
    "    spikes = torch.tensor([0.0015, 0.0085, 0.011, 0.0075, 0.002], device=motion_tensor.device)\n",
    "    freeze_index = [0, 1, 2, 4, 6, 8, 10]\n",
    "    freeze_index = torch.tensor(freeze_index, device=motion_tensor.device)\n",
    "    period = 12\n",
    "    period_counter = 0\n",
    "    reset_flag = False\n",
    "    for i in range(motion_tensor.shape[0]):\n",
    "        in_period_index = i % period\n",
    "        if in_period_index < 5 and period_counter >= period:\n",
    "            for eye_index in eye_indices:\n",
    "                motion_tensor[i, eye_index] = spikes[in_period_index]\n",
    "                motion_tensor[i, eye_index] += torch.randn_like(motion_tensor[i, eye_index]) * 0.002\n",
    "            if in_period_index == 4:\n",
    "                reset_flag = True\n",
    "                period_counter = 0\n",
    "        else:\n",
    "            for eye_index in eye_indices:\n",
    "                motion_tensor[i, eye_index] = rest_latent\n",
    "                motion_tensor[i, eye_index] += torch.randn_like(motion_tensor[i, eye_index]) * 0.0002\n",
    "            period_counter += 1\n",
    "        if reset_flag:\n",
    "            reset_flag = False\n",
    "            period = torch.randint(15, 18, (1,)).item()\n",
    "        for f in freeze_index:\n",
    "            # motion_tensor[i, f] = motion_gt[i, f]\n",
    "            motion_tensor[i, f] = 0\n",
    "    return motion_tensor\n",
    "\n",
    "# pose\n",
    "headpose_bound_list = [\n",
    "    -21,                   25,                   -30,                   30,                  -23,                     23,\n",
    "    -0.3,                  0.3,                  -0.3,                 0.28,                ]\n",
    "headpose_loss_weight = [0.01, 0.01, 0.01, 0.01, 0.1, 0.1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize headpose\n",
    "def normalize_pose(full_motion, headpose_bound):\n",
    "    assert headpose_bound is not None and len(headpose_bound) % 2 == 0\n",
    "    headpose_bound = torch.tensor(headpose_bound)\n",
    "    headpose_bound = headpose_bound.reshape(headpose_bound.shape[0] // 2, 2)\n",
    "\n",
    "    # Assuming full_motion is a tensor of shape (batch_size, sequence_length, num_features)\n",
    "    # and the last 5 features are the ones to be normalized\n",
    "    last_5_features = full_motion[:, :, -5:]\n",
    "\n",
    "    # Normalize each of the last 5 features\n",
    "    for i in range(5):\n",
    "        lower_bound = headpose_bound[i][0]\n",
    "        upper_bound = headpose_bound[i][1]\n",
    "\n",
    "        # Clamp the values within the specified bounds\n",
    "        clamped = torch.clamp(last_5_features[:, :, i], min=lower_bound, max=upper_bound)\n",
    "\n",
    "        # Normalize to the range [-0.05, 0.05]\n",
    "        normalized = (clamped - lower_bound) / (upper_bound - lower_bound) * 0.1 - 0.05\n",
    "\n",
    "        # Update the last 5 features with the normalized values\n",
    "        last_5_features[:, :, i] = normalized\n",
    "\n",
    "    # Update the full_motion tensor with the normalized last 5 features\n",
    "    full_motion[:, :, -5:] = last_5_features\n",
    "\n",
    "    return full_motion\n",
    "\n",
    "def reverse_normalize_pose(normalized_motion, headpose_bound):\n",
    "    assert headpose_bound is not None and len(headpose_bound) % 2 == 0\n",
    "    headpose_bound = torch.tensor(headpose_bound)\n",
    "    headpose_bound = headpose_bound.reshape(headpose_bound.shape[0] // 2, 2)\n",
    "\n",
    "    # Assuming normalized_motion is a tensor of shape (batch_size, sequence_length, num_features)\n",
    "    # and the last 5 features are the ones to be reversed\n",
    "    last_5_features = normalized_motion[:, :, -5:]\n",
    "\n",
    "    # Reverse normalization for each of the last 5 features\n",
    "    for i in range(5):\n",
    "        lower_bound = headpose_bound[i][0]\n",
    "        upper_bound = headpose_bound[i][1]\n",
    "\n",
    "        # Reverse the normalization from [-0.05, 0.05] to the original range\n",
    "        original = (last_5_features[:, :, i] + 0.05) / 0.1 * (upper_bound - lower_bound) + lower_bound\n",
    "\n",
    "        # Update the last 5 features with the original values\n",
    "        last_5_features[:, :, i] = original\n",
    "\n",
    "    # Update the normalized_motion tensor with the original last 5 features\n",
    "    normalized_motion[:, :, -5:] = last_5_features\n",
    "\n",
    "    return normalized_motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio-driven inference function\n",
    "\n",
    "def inference_one_input(audio_path, portrait_path, output_vid_path, inference_manager, audio_model_config, cfg_s, mouth_ratio, subtract_avg_motion):\n",
    "\n",
    "\n",
    "    from audio_dit.dataset import process_motion_tensor\n",
    "    import numpy as np\n",
    "    motion_mean_exp_path = '/mnt/e/data/diffposetalk_data/TFHP_raw/train_split/live_latent/TH_00192/000.npy'\n",
    "    motion_mean_exp_tensor = torch.from_numpy(np.load(motion_mean_exp_path)).to(device='cuda')\n",
    "    motion_mean_exp_tensor = motion_mean_exp_tensor.unsqueeze(0).to(device='cuda')\n",
    "    motion_tensor, _, _, _ = process_motion_tensor(motion_mean_exp_tensor, None, \\\n",
    "                                latent_type=audio_model_config['motion_latent_type'],\n",
    "                                latent_mask_1=audio_model_config['latent_mask_1'],\n",
    "                                latent_bound=torch.tensor(audio_model_config['latent_bound'], device='cuda'),\n",
    "                                use_headpose=True, headpose_bound=torch.tensor(headpose_bound_list, device='cuda'))\n",
    "    mean_exp = torch.mean(motion_tensor.reshape(-1, motion_tensor.shape[-1]), dim=0)\n",
    "    # mean_exp = torch.zeros_like(mean_exp)\n",
    "    # mean_exp *= -1\n",
    "\n",
    "    # load audio\n",
    "    # audio_latent = inference_process_wav_file(audio_path)\n",
    "    audio_latent = autoregress_inference_process_wav_file(audio_path)\n",
    "    # audio_latent = torch.zeros_like(audio_latent)\n",
    "    window_count = audio_latent.shape[0]\n",
    "    # audio_seq = audio_latent[:, 10:, :]\n",
    "    # audio_prev = audio_latent[:, :10, :]\n",
    "    # load portrait\n",
    "    img_rgb = load_image_rgb(portrait_path)\n",
    "    source_rgb_lst = [img_rgb]\n",
    "    source_lmk = cropper.calc_lmk_from_cropped_image(source_rgb_lst[0])\n",
    "    img_crop_256x256 = cv2.resize(source_rgb_lst[0], (256, 256))  # force to resize to 256x256\n",
    "    I_s = prepare_source(img_crop_256x256)\n",
    "    x_s_info = get_kp_info(I_s)\n",
    "    x_c_s = x_s_info['kp']\n",
    "    x_s = transform_keypoint(x_s_info)\n",
    "    f_s = extract_feature_3d(I_s)\n",
    "\n",
    "    # inference\n",
    "    mouth_open_ratio_val = mouth_ratio\n",
    "    mouth_open_ratio_input = torch.tensor([mouth_open_ratio_val], device=device).unsqueeze(0)\n",
    "    out_motion = torch.tensor([], device=device)\n",
    "    out_null_motion = torch.tensor([], device=device)\n",
    "    motion_dim = audio_model_config['x_dim']\n",
    "    shape_in = x_c_s.reshape(1, -1).to(device)\n",
    "    for batch_index in range(0, window_count):\n",
    "        # print(f'batch_index: {batch_index}')\n",
    "        if batch_index == 0:\n",
    "            this_audio_prev = audio_latent[0:1, 0:10, :]\n",
    "            audio_seq = audio_latent[0:1, 10:, :]\n",
    "            this_motion_prev = torch.zeros(1, 10, motion_dim , device=device)\n",
    "            gen_length = 65\n",
    "        else:\n",
    "            this_audio_prev = audio_latent[batch_index:batch_index+1, 0:prev_context_len, :]\n",
    "            audio_seq = audio_latent[batch_index:batch_index+1, prev_context_len:, :]\n",
    "            gen_length = gen_len_per_window\n",
    "            # this_motion_prev = torch.cat((torch.zeros(1, 25, motion_dim , device=device), this_motion_prev[:, -40:, :]), dim=1)\n",
    "        # print(f'gen_length: {gen_length}')\n",
    "        # print(f'audio_seq shape: {audio_seq.shape}')\n",
    "        # print(f'this_motion_prev shape: {this_motion_prev.shape}')\n",
    "        # print(f'this_audio_prev shape: {this_audio_prev.shape}')\n",
    "        # this_motion_prev = torch.zeros_like(this_motion_prev)\n",
    "        mean_exp_expanded = mean_exp.expand(1, -1)\n",
    "        generated_motion, null_motion = inference_manager.inference(audio_seq,\n",
    "                                                    shape_in, this_motion_prev, this_audio_prev, #seq_mask=seq_mask,\n",
    "                                                    cfg_scale=cfg_s,\n",
    "                                                    mouth_open_ratio = mouth_open_ratio_input,\n",
    "                                                    denoising_steps=10,\n",
    "                                                    gen_length=gen_length,\n",
    "                                                    mean_exp=mean_exp_expanded)\n",
    "        full_window_motion = torch.cat((this_motion_prev, generated_motion), dim=1)\n",
    "        this_motion_prev = full_window_motion[:, -prev_context_len:, :]\n",
    "        # generated_motion = generated_motion.reshape(-1, 6)\n",
    "\n",
    "        if subtract_avg_motion:\n",
    "            generated_motion = generated_motion - torch.mean(generated_motion, dim=-1, keepdim=True)\n",
    "        generated_motion = generated_motion - torch.mean(null_motion, dim=-2, keepdim=True)\n",
    "        out_motion = torch.cat((out_motion, generated_motion.reshape(-1, motion_dim)), dim=0)\n",
    "        out_null_motion = torch.cat((out_null_motion, null_motion.reshape(-1, motion_dim)), dim=0)\n",
    "\n",
    "    # out_motion = out_motion - out_null_motion\n",
    "    from scipy.signal import savgol_filter\n",
    "\n",
    "    # Then modify the filtering line to:\n",
    "    out_motion_filtered = savgol_filter(out_motion.cpu().numpy(), window_length=5, polyorder=2, axis=0)\n",
    "    out_motion = torch.tensor(out_motion_filtered, device=device)\n",
    "    # out_null_motion = out_null_motion - out_null_motion\n",
    "\n",
    "    motion_gt = motion_tensor[:, :out_motion.shape[0], :].squeeze(0)\n",
    "    print(f'motion_gt shape: {motion_gt.shape}, out_motion shape: {out_motion.shape}')\n",
    "    out_motion = fixed_blink_noise(out_motion, motion_gt)\n",
    "    out_pose_motion = out_motion[:, -5:]\n",
    "    out_pose_smoothed = savgol_filter(out_pose_motion.cpu().numpy(), window_length=15, polyorder=2, axis=0)\n",
    "    out_motion[:, -5:-2] *= 2\n",
    "    out_motion[:, -2:] *= 0.5\n",
    "    out_motion[:, -5:] = torch.tensor(out_pose_smoothed, device=device)\n",
    "    plot_gt = False\n",
    "    if plot_motion:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "\n",
    "        # Get total number of dimensions\n",
    "        n_dims = out_motion.shape[1]\n",
    "        vel = out_motion[1:] - out_motion[:-1]\n",
    "        acc = vel[1:] - vel[:-1]\n",
    "\n",
    "        null_vel = out_null_motion[1:] - out_null_motion[:-1]\n",
    "        null_acc = null_vel[1:] - null_vel[:-1]\n",
    "\n",
    "        gt_vel = motion_gt[1:] - motion_gt[:-1]\n",
    "        gt_acc = gt_vel[1:] - gt_vel[:-1]\n",
    "\n",
    "        # Create figure with subplots - three rows per dimension\n",
    "        fig, axs = plt.subplots(n_dims*3, 1, figsize=(24, 5*n_dims*3))\n",
    "\n",
    "        # Plot each dimension\n",
    "        for i in tqdm(range(n_dims)):\n",
    "            # Plot position\n",
    "            position = out_motion[:, i].cpu().detach().numpy()\n",
    "            null_position = out_null_motion[:, i].cpu().detach().numpy()\n",
    "            gt_position = motion_gt[:, i].cpu().detach().numpy()\n",
    "            axs[i*3].plot(position, label='Motion')\n",
    "            if plot_gt:\n",
    "                axs[i*3].plot(gt_position, label='Ground Truth', alpha=0.5)\n",
    "            # else:\n",
    "            #     axs[i*3].plot(null_position, label='Null Motion', alpha=0.5)\n",
    "            axs[i*3].set_ylabel('Position')\n",
    "            axs[i*3].set_xlabel('Time step')\n",
    "            axs[i*3].legend()\n",
    "\n",
    "            # Plot velocity\n",
    "            velocity = vel[:, i].cpu().detach().numpy()\n",
    "            null_velocity = null_vel[:, i].cpu().detach().numpy()\n",
    "            gt_velocity = gt_vel[:, i].cpu().detach().numpy()\n",
    "            axs[i*3 + 1].plot(velocity, label='Motion')\n",
    "            # axs[i*3 + 1].plot(null_velocity, label='Null Motion', alpha=0.5)\n",
    "            if plot_gt:\n",
    "                axs[i*3 + 1].plot(gt_velocity, label='Ground Truth', alpha=0.5)\n",
    "            axs[i*3 + 1].set_title(f'Dimension {i} Velocity')\n",
    "            axs[i*3 + 1].set_ylabel('Velocity')\n",
    "            axs[i*3 + 1].set_xlabel('Time step')\n",
    "            axs[i*3 + 1].legend()\n",
    "\n",
    "            # Plot acceleration\n",
    "            acceleration = acc[:, i].cpu().detach().numpy()\n",
    "            null_acceleration = null_acc[:, i].cpu().detach().numpy()\n",
    "            gt_acceleration = gt_acc[:, i].cpu().detach().numpy()\n",
    "            axs[i*3 + 2].plot(acceleration, label='Motion')\n",
    "            # axs[i*3 + 2].plot(null_acceleration, label='Null Motion', alpha=0.5)\n",
    "            if plot_gt:\n",
    "                axs[i*3 + 2].plot(gt_acceleration, label='Ground Truth', alpha=0.5)\n",
    "            axs[i*3 + 2].set_title(f'Dimension {i} Acceleration')\n",
    "            axs[i*3 + 2].set_ylabel('Acceleration')\n",
    "            axs[i*3 + 2].set_xlabel('Time step')\n",
    "            axs[i*3 + 2].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    generated_motion = out_motion\n",
    "    generated_motion = reverse_normalize_pose(generated_motion.unsqueeze(0), headpose_bound=torch.tensor(headpose_bound_list, device='cuda'))\n",
    "    generated_motion = generated_motion.squeeze(0)\n",
    "    if not write_vid:\n",
    "        return generated_motion\n",
    "    all_frames = process_motion_batch(generated_motion, f_s, x_s, x_c_s, x_s_info, audio_model_config, warp_decode)\n",
    "    # write to video\n",
    "    write_video(all_frames, audio_path, output_vid_path)\n",
    "    return generated_motion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syncnet inference function\n",
    "from syncnet.syncnet import syncnet_inference\n",
    "\n",
    "def call_syncnet(output_vid_path, tmp_dir):\n",
    "\n",
    "    # Extract the reference from the video filename\n",
    "    video_basename = os.path.basename(output_vid_path)\n",
    "    reference = os.path.splitext(video_basename)[0]\n",
    "\n",
    "    results, activesd = syncnet_inference(output_vid_path, reference, tmp_dir, keep_output=False)\n",
    "    if results:\n",
    "        # print(\"\\nSyncNet Results:\")\n",
    "        # print(f\"AV Offset: {results['av_offset']}\")\n",
    "        # print(f\"Confidence: {results['confidence']}\")\n",
    "        # print(f\"Min Dist: {results['min_dist']}\")\n",
    "        # print(f\"Framewise Conf Shape: {np.array(results['framewise_conf']).shape}\")\n",
    "        # print(f\"ActiveSD Shape: {np.array(activesd).shape}\")\n",
    "        return results['confidence'], results['min_dist']\n",
    "\n",
    "def change_working_dir_to_script_location():\n",
    "    os.chdir('/mnt/e/wsl_projects/LivePortrait/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_syncnet_only:\n",
    "    change_working_dir_to_script_location()\n",
    "    sync_tmp_dir = './sync_output/tmp'\n",
    "    sync_tmp_dir_abs = os.path.abspath(sync_tmp_dir)\n",
    "    os.makedirs(sync_tmp_dir_abs, exist_ok=True)\n",
    "    print(f'sync_tmp_dir_abs: {sync_tmp_dir_abs}')\n",
    "    # Run syncnet on all videos in directory\n",
    "    result_dict = {}\n",
    "    json_output_path = os.path.join(sync_only_vid_root_dir, 'syncnet_results.json')\n",
    "\n",
    "    # Walk through all files in syncnet_root_dir\n",
    "    for root, dirs, files in tqdm(os.walk(sync_only_vid_root_dir), desc=\"Processing directories\"):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp4'):\n",
    "                vid_path = os.path.join(root, file)\n",
    "                vid_path_abs = os.path.abspath(vid_path)\n",
    "                print(f'Processing {vid_path_abs}')\n",
    "                change_working_dir_to_script_location()\n",
    "\n",
    "                # Get relative path from syncnet_root_dir as key\n",
    "                rel_path = os.path.relpath(vid_path, sync_only_vid_root_dir)\n",
    "\n",
    "                # Run syncnet\n",
    "                s_c, s_d = call_syncnet(vid_path_abs, sync_tmp_dir_abs)\n",
    "                result_dict[rel_path] = [s_c, s_d]\n",
    "                print(f'    {rel_path} done, confidence: {s_c}, min_dist: {s_d}')\n",
    "\n",
    "                # Save results\n",
    "                json.dump(result_dict, open(json_output_path, 'w'), indent=4)\n",
    "                print(f'Results saved to {json_output_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_syncnet_only:\n",
    "    adv = kmp # break execution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main motion generation function\n",
    "change_working_dir_to_script_location()\n",
    "sync_tmp_dir = './sync_output/tmp'\n",
    "sync_tmp_dir_abs = os.path.abspath(sync_tmp_dir)\n",
    "os.makedirs(sync_tmp_dir_abs, exist_ok=True)\n",
    "print(f'sync_tmp_dir_abs: {sync_tmp_dir_abs}')\n",
    "for model_weights_pair in model_weights_pairs:\n",
    "    change_working_dir_to_script_location()\n",
    "    config_path, weight_path = model_weights_pair\n",
    "    weight_basename = os.path.basename(weight_path).split('.pth')[0]\n",
    "    output_root = f'./sync_output/{time.strftime(\"%Y-%m-%d-%H-%M\", time.localtime(time.time()))}_{weight_basename}'\n",
    "    output_root_abs = os.path.abspath(output_root)\n",
    "    os.makedirs(output_root_abs, exist_ok=True)\n",
    "    print(f'output_root_abs: {output_root_abs}')\n",
    "\n",
    "    print(\"config_path exists:\", os.path.exists(config_path))\n",
    "    audio_model_config = json.load(open(config_path))\n",
    "    inference_manager = get_model(config_path, weight_path, device)\n",
    "    result_dict = {}\n",
    "    for cfg_s in cfg_scale_opts:\n",
    "        for mouth_ratio in mouth_open_ratio_opts:\n",
    "            for subtract_avg_motion in subtract_avg_motion_opts:\n",
    "                change_working_dir_to_script_location()\n",
    "                config_name = f'cfg_{cfg_s}_mouth_{mouth_ratio}_subtract_{subtract_avg_motion}'\n",
    "                output_parent = os.path.join(output_root_abs, config_name)\n",
    "                os.makedirs(output_parent, exist_ok=True)\n",
    "                print(f'processing {config_name}')\n",
    "                result_dict[config_name] = {}\n",
    "                for audio_path in audio_paths:\n",
    "                    audio_basename = os.path.basename(audio_path)\n",
    "                    for portrait_path in portrait_imgs:\n",
    "                        change_working_dir_to_script_location()\n",
    "                        portrait_basename = os.path.basename(portrait_path)\n",
    "                        vid_name = f'audio_{audio_basename}_img_{portrait_basename}.mp4'\n",
    "                        output_vid_path = os.path.join(output_parent, vid_name)\n",
    "                        print(f'    processing {output_vid_path}')\n",
    "\n",
    "                        generated_motion = inference_one_input(audio_path, portrait_path, output_vid_path, inference_manager, audio_model_config, cfg_s, mouth_ratio, subtract_avg_motion)\n",
    "                        output_vid_path_abs = os.path.abspath(output_vid_path)\n",
    "                        if run_syncnet_with_motion_gen:\n",
    "                            s_c, s_d = call_syncnet(output_vid_path_abs, sync_tmp_dir_abs)\n",
    "                            result_dict[config_name][vid_name] = [s_c, s_d]\n",
    "                            print(f'    {vid_name} done, confidence: {s_c}, min_dist: {s_d}')\n",
    "                            json_output_path = os.path.join(output_root_abs, 'result_dict.json')\n",
    "                            json.dump(result_dict, open(json_output_path, 'w'), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = kv\n",
    "config_path, weight_path = model_weights_pairs[0]\n",
    "print(f'config_path: {config_path}')\n",
    "audio_model_config = json.load(open(config_path))\n",
    "inference_manager = get_model(config_path, weight_path, device)\n",
    "inference_one_input(audio_paths[0],\n",
    "                    portrait_imgs[0],\n",
    "                    output_root_abs,\n",
    "                    inference_manager,\n",
    "                    audio_model_config,\n",
    "                    cfg_scale_opts[0],\n",
    "                    mouth_open_ratio_opts[0],\n",
    "                    subtract_avg_motion_opts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_c, s_d = call_syncnet(output_vid_path_abs, sync_tmp_dir_abs)\n",
    "# print(f'    {vid_name} done, confidence: {s_c}, min_dist: {s_d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "motion_mean_exp_path = '/mnt/e/data/diffposetalk_data/TFHP_raw/train_split/live_latent/TH_00192/000.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_dit.dataset import process_motion_tensor\n",
    "import numpy as np\n",
    "from  scipy.signal import savgol_filter\n",
    "\n",
    "plot_with_generated_motion = True\n",
    "plot_velocity = True\n",
    "plot_acceleration = True\n",
    "normalize_pose_flag = True\n",
    "config_path, weight_path = model_weights_pairs[0]\n",
    "print(\"config_path exists:\", os.path.exists(config_path))\n",
    "audio_model_config = json.load(open(config_path))\n",
    "\n",
    "motion_mean_exp_tensor = torch.from_numpy(np.load(motion_mean_exp_path)).to(device='cuda')\n",
    "motion_mean_exp_tensor = motion_mean_exp_tensor.unsqueeze(0).to(device='cuda')\n",
    "motion_tensor, _, _, _ = process_motion_tensor(motion_mean_exp_tensor, None, \\\n",
    "                            latent_type=audio_model_config['motion_latent_type'],\n",
    "                            latent_mask_1=audio_model_config['latent_mask_1'],\n",
    "                            latent_bound=torch.tensor(audio_model_config['latent_bound'], device='cuda'),\n",
    "                            use_headpose=True, headpose_bound=torch.tensor(headpose_bound_list, device='cuda'))\n",
    "if normalize_pose_flag:\n",
    "    # motion_tensor = normalize_pose(motion_tensor, headpose_bound=torch.tensor(headpose_bound_list, device='cuda'))\n",
    "    motion_tensor = reverse_normalize_pose(motion_tensor, headpose_bound=torch.tensor(headpose_bound_list, device='cuda'))\n",
    "if True:\n",
    "    # generated_motion_to_plot = savgol_filter(generated_motion.cpu().numpy(), window_length=15, polyorder=3)\n",
    "    # generated_motion_to_plot = torch.tensor(generated_motion_to_plot, device='cuda')\n",
    "    generated_motion_to_plot = generated_motion.unsqueeze(0)\n",
    "    min_len = min(motion_tensor.shape[1], generated_motion_to_plot.shape[1])\n",
    "    print(f'min_len: {min_len}, motion_tensor shape: {motion_tensor.shape}, generated_motion shape: {generated_motion_to_plot.shape}')\n",
    "    motion_tensor = motion_tensor[:, :min_len, :]\n",
    "    generated_motion_to_plot = generated_motion_to_plot[:, :min_len, :]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get last 5 dimensions\n",
    "last_5_dims = motion_tensor[0, :, -5:].cpu().numpy()\n",
    "if plot_with_generated_motion:\n",
    "    last_5_dims_gen = generated_motion_to_plot[0, :, -5:].cpu().numpy()\n",
    "\n",
    "# Create time axis\n",
    "time_steps = np.arange(last_5_dims.shape[0])\n",
    "\n",
    "# Create subplot for each dimension\n",
    "fig, axes = plt.subplots(5, 1, figsize=(12, 10), sharex=True)\n",
    "fig.suptitle('Last 5 Dimensions Over Time')\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].plot(time_steps, last_5_dims[:, i])\n",
    "    if plot_with_generated_motion:\n",
    "        axes[i].plot(time_steps, last_5_dims_gen[:, i], alpha=0.5)\n",
    "    axes[i].set_ylabel(f'Dim {motion_tensor.shape[2]-5+i}')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "last_10_dims = motion_tensor[0, :, -10:].cpu().numpy()\n",
    "if plot_with_generated_motion:\n",
    "    last_10_dims_gen = generated_motion_to_plot[0, :, -10:].cpu().numpy()\n",
    "\n",
    "# Create time axis\n",
    "time_steps = np.arange(last_10_dims.shape[0])\n",
    "\n",
    "# Create subplot for each dimension\n",
    "fig, axes = plt.subplots(5, 1, figsize=(12, 10), sharex=True)\n",
    "fig.suptitle('Last 5 Dimensions Over Time')\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].plot(time_steps, last_10_dims[:, i])\n",
    "    if plot_with_generated_motion:\n",
    "        axes[i].plot(time_steps, last_10_dims_gen[:, i], alpha=0.5)\n",
    "    axes[i].set_ylabel(f'Dim {motion_tensor.shape[2]-10+i}')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# ... existing code ...\n",
    "\n",
    "if plot_velocity:\n",
    "    # Create velocity plots\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(12, 10), sharex=True)\n",
    "    fig.suptitle('Velocity of Last 5 Dimensions Over Time')\n",
    "\n",
    "    last_5_dims_vel = motion_tensor[0, 1:, -5:] - motion_tensor[0, :-1, -5:]\n",
    "    last_5_dims_vel = last_5_dims_vel.cpu().numpy()\n",
    "    time_steps_vel = np.arange(last_5_dims_vel.shape[0])\n",
    "\n",
    "    if plot_with_generated_motion:\n",
    "        last_5_dims_vel_gen = generated_motion_to_plot[0, 1:, -5:] - generated_motion_to_plot[0, :-1, -5:]\n",
    "        last_5_dims_vel_gen = last_5_dims_vel_gen.cpu().numpy()\n",
    "    for i in range(5):\n",
    "        axes[i].plot(time_steps_vel, last_5_dims_vel[:, i], label='Ground Truth')\n",
    "        if plot_with_generated_motion:\n",
    "            axes[i].plot(time_steps_vel, last_5_dims_vel_gen[:, i], alpha=0.5, label='Generated')\n",
    "        axes[i].set_ylabel(f'Vel Dim {motion_tensor.shape[2]-5+i}')\n",
    "        axes[i].grid(True)\n",
    "        axes[i].legend()\n",
    "\n",
    "    axes[-1].set_xlabel('Time Steps')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if plot_acceleration:\n",
    "    # Create acceleration plots\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(12, 10), sharex=True)\n",
    "    fig.suptitle('Acceleration of Last 5 Dimensions Over Time')\n",
    "\n",
    "    last_5_dims_vel = motion_tensor[0, 1:, -5:] - motion_tensor[0, :-1, -5:]\n",
    "    last_5_dims_acc = last_5_dims_vel[1:, :] - last_5_dims_vel[:-1, :]\n",
    "    last_5_dims_acc = last_5_dims_acc.cpu().numpy()\n",
    "    time_steps_acc = np.arange(last_5_dims_acc.shape[0])\n",
    "\n",
    "    if plot_with_generated_motion:\n",
    "        last_5_dims_vel_gen = generated_motion_to_plot[0, 1:, -5:] - generated_motion_to_plot[0, :-1, -5:]\n",
    "        last_5_dims_acc_gen = last_5_dims_vel_gen[1:, :] - last_5_dims_vel_gen[:-1, :]\n",
    "        last_5_dims_acc_gen = last_5_dims_acc_gen.cpu().numpy()\n",
    "    for i in range(5):\n",
    "        axes[i].plot(time_steps_acc, last_5_dims_acc[:, i], label='Ground Truth')\n",
    "        if plot_with_generated_motion:\n",
    "            axes[i].plot(time_steps_acc, last_5_dims_acc_gen[:, i], alpha=0.5, label='Generated')\n",
    "        axes[i].set_ylabel(f'Acc Dim {motion_tensor.shape[2]-5+i}')\n",
    "        axes[i].grid(True)\n",
    "        axes[i].legend()\n",
    "\n",
    "    axes[-1].set_xlabel('Time Steps')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "axes[-1].set_xlabel('Time Steps')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vasa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
