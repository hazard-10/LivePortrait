{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mismatch_batch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "How data loading works in this project:\n",
    "* We assume the npy will be sufficient to load into memory\n",
    "* latent composition:\n",
    "    concat_tensor = torch.cat([\n",
    "        x_info['kp'], # 63\n",
    "        x_info['exp'], # 63,\n",
    "        x_info['t'], # 3\n",
    "        x_info['pitch'], # 1\n",
    "        x_info['yaw'], # 1\n",
    "        x_info['roll'], # 1\n",
    "        x_info['scale'], # 1\n",
    "    ], dim=1)\n",
    "    \n",
    "    for person_feat\n",
    "     - kp single batch avg used for v1. exp keep the same, use global avg scale. t and rot will be subtracted in subsequent frames, \n",
    "     - so we can ignore t and rot for now\n",
    "    for training data\n",
    "     - use individual kp, exp, subtract dominant t and rot, use global scale\n",
    "    \n",
    "    \n",
    "\n",
    "1. Load both audio and motion latent from the npy files with load_npy_files\n",
    "    2.  Motion latent is processed to have a window of 65 frames with 10 frames overlap\n",
    "    2.1 The rotation and translation are frontalized \n",
    "3.  The processed data is then loaded into a custom dataset class for random sampling\n",
    "'''\n",
    "\n",
    "'''\n",
    "Part 1: Load and process the npy files\n",
    "'''\n",
    "def load_and_process_pair(audio_file, motion_file, latent_type='exp', latent_mask_1=None, latent_bound=None):\n",
    "    # Load audio file\n",
    "    # Check the type of audio_file\n",
    "    if isinstance(audio_file, (str, os.PathLike)):\n",
    "        # If it's a string (file path), load the audio data\n",
    "        audio_data = np.load(audio_file)\n",
    "    elif isinstance(audio_file, (np.ndarray, torch.Tensor)):\n",
    "        # If it's already a numpy array or torch tensor, use it as is\n",
    "        audio_data = audio_file\n",
    "    else:\n",
    "        # If it's neither a string nor a numpy array/torch tensor, raise an error\n",
    "        raise ValueError(\"audio_file must be a file path string, numpy array, or torch tensor\")\n",
    "\n",
    "    # Load and process motion file\n",
    "    motion_data = np.load(motion_file)\n",
    "    pad_length = (65 - (motion_data.shape[0] - 10) % 65) % 65\n",
    "    padded_data = np.pad(motion_data, ((0, pad_length), (0, 0)), mode='constant')\n",
    "    \n",
    "    data_without_first_10 = padded_data[10:]\n",
    "    N = data_without_first_10.shape[0] // 65\n",
    "    reshaped_data = data_without_first_10[:N*65].reshape(N, 65, 136)\n",
    "    last_10 = reshaped_data[:, -10:, :]\n",
    "    prev_10 = np.concatenate([padded_data[:10][None, :, :], last_10[:-1]], axis=0)\n",
    "    motion_data = np.concatenate([prev_10, reshaped_data], axis=1)\n",
    "    \n",
    "    end_indices = torch.ones(N, dtype=torch.int32) * (motion_data.shape[1] - 1)\n",
    "    end_indices[-1] = motion_data.shape[1] - 1 - pad_length\n",
    "    \n",
    "    # Ensure audio and motion data have the same number of frames. \n",
    "    # Prev lookup show 1 frame mismatch is common. In this case we only fix batch size mismatch\n",
    "    min_frames = min(audio_data.shape[0], motion_data.shape[0])\n",
    "    audio_data = audio_data[:min_frames]\n",
    "    motion_data = motion_data[:min_frames]\n",
    "    end_indices = end_indices[:min_frames]\n",
    "    \n",
    "    motion_tensor = torch.from_numpy(motion_data)\n",
    "    if isinstance(audio_data, np.ndarray):\n",
    "        audio_tensor = torch.from_numpy(audio_data)\n",
    "    elif isinstance(audio_data, torch.Tensor):\n",
    "        audio_tensor = audio_data\n",
    "\n",
    "    motion_tensor, audio_tensor, shape_tensor, mouth_tensor = process_motion_tensor(motion_tensor, audio_tensor, latent_type, latent_mask_1, latent_bound)\n",
    "    \n",
    "    return motion_tensor, audio_tensor, shape_tensor, mouth_tensor, end_indices\n",
    "\n",
    "def process_directory(uid, audio_root, motion_root, latent_type='exp', latent_mask_1=None, latent_bound=None):\n",
    "    audio_dir = os.path.join(audio_root, uid)\n",
    "    motion_dir = os.path.join(motion_root, uid)\n",
    "    \n",
    "    if not (os.path.isdir(audio_dir) and os.path.isdir(motion_dir)):\n",
    "        print(f\"Directory not found for \", audio_dir, motion_dir)\n",
    "        return None, None\n",
    "    \n",
    "    audio_files = sorted([f for f in os.listdir(audio_dir) if f.endswith('.npy')])\n",
    "    motion_files = sorted([f for f in os.listdir(motion_dir) if f.endswith('.npy')])\n",
    "    \n",
    "    motion_tensor_list = []\n",
    "    audio_tensor_list = []\n",
    "    shape_tensor_list = []\n",
    "    mouth_tensor_list = []\n",
    "    end_indices_list = []\n",
    "    for audio_file, motion_file in zip(audio_files, motion_files):\n",
    "        if audio_file != motion_file and audio_file.split('+')[0] != motion_file.split('+')[0]: \n",
    "            print(f\"Mismatch in {uid}: {audio_file} and {motion_file}\")\n",
    "            continue\n",
    "        \n",
    "        audio_path = os.path.join(audio_dir, audio_file)\n",
    "        motion_path = os.path.join(motion_dir, motion_file)\n",
    "        \n",
    "        motion_tensor, audio_tensor, shape_tensor, mouth_tensor, end_indices = load_and_process_pair(audio_path, motion_path, latent_type, latent_mask_1, latent_bound)\n",
    "        motion_tensor_list.append(motion_tensor)\n",
    "        audio_tensor_list.append(audio_tensor)\n",
    "        shape_tensor_list.append(shape_tensor)\n",
    "        mouth_tensor_list.append(mouth_tensor)\n",
    "        end_indices_list.append(end_indices)\n",
    "    \n",
    "    return torch.cat(motion_tensor_list, dim=0), torch.cat(audio_tensor_list, dim=0), torch.cat(shape_tensor_list, dim=0), \\\n",
    "            torch.cat(mouth_tensor_list, dim=0), torch.cat(end_indices_list, dim=0)\n",
    "\n",
    "def load_npy_files(audio_root, motion_root, start_idx=None, end_idx=None, latent_type='exp', latent_mask_1=None, latent_bound=None):\n",
    "    all_dir_list = sorted(os.listdir(audio_root))\n",
    "    if start_idx is not None and end_idx is not None:\n",
    "        dir_list = all_dir_list[start_idx:end_idx]\n",
    "    else:\n",
    "        dir_list = all_dir_list\n",
    "    \n",
    "    all_motion_data = []\n",
    "    all_audio_data = []\n",
    "    all_shape_data = []\n",
    "    all_mouth_data = []\n",
    "    all_end_indices = []\n",
    "                \n",
    "                \n",
    "    for uid in tqdm(dir_list, desc=\"Processing directories\"):      \n",
    "        try:\n",
    "            motion_data, audio_data, shape_data, mouth_data, end_indices = process_directory(uid, audio_root, motion_root, latent_type, latent_mask_1, latent_bound)\n",
    "            if audio_data is not None and motion_data is not None:\n",
    "                    all_motion_data.append(motion_data)\n",
    "                    all_audio_data.append(audio_data)\n",
    "                    all_shape_data.append(shape_data)\n",
    "                    all_mouth_data.append(mouth_data)\n",
    "                    all_end_indices.append(end_indices)\n",
    "        except Exception as exc:\n",
    "            print(f'{uid} generated an exception: {exc}')\n",
    "    \n",
    "    motion_tensor = torch.concat(all_motion_data, dim=0)\n",
    "    audio_tensor = torch.concat(all_audio_data, dim=0)\n",
    "    shape_tensor = torch.concat(all_shape_data, dim=0)\n",
    "    mouth_tensor = torch.concat(all_mouth_data, dim=0)\n",
    "    end_indices = torch.concat(all_end_indices, dim=0)  \n",
    "    \n",
    "    # print(f\"audio loaded from disk. tensor shape: {audio_tensor.shape}\")\n",
    "    # print(f\"motion loaded from disk. tensor shape: {motion_tensor.shape}\")\n",
    "    \n",
    "    # motion_tensor = normalize_motion_tensor(motion_tensor, latent_bound, latent_mask_1)\n",
    "    return motion_tensor, audio_tensor, shape_tensor, mouth_tensor, end_indices\n",
    "\n",
    "def normalize_motion_tensor(motion_tensor, latent_bound=None, latent_mask_1=None):\n",
    "    motion_tensor = motion_tensor[:, :, :-1] # Remove the last feature which is always 0\n",
    "    print(f\"motion_tensor shape: {motion_tensor.shape}\")\n",
    "    if latent_bound is not None:\n",
    "        assert len(latent_bound) % 2 == 0\n",
    "        assert latent_mask_1 is not None\n",
    "        \n",
    "        latent_bound = torch.tensor(latent_bound).reshape(-1, 2)\n",
    "        min_vals = torch.zeros(len(latent_mask_1))\n",
    "        max_vals = torch.zeros(len(latent_mask_1))\n",
    "        \n",
    "        for i, mask_index in enumerate(latent_mask_1):\n",
    "            min_vals[i] = latent_bound[mask_index][0]\n",
    "            max_vals[i] = latent_bound[mask_index][1]\n",
    "    else:\n",
    "        min_vals, _ = torch.min(motion_tensor.reshape(-1, motion_tensor.shape[-1]), dim=0)\n",
    "        max_vals, _ = torch.max(motion_tensor.reshape(-1, motion_tensor.shape[-1]), dim=0)\n",
    "\n",
    "    denominator = max_vals - min_vals\n",
    "    denominator[denominator == 0] = 1.0  # Set to 1 where max and min are the same\n",
    "\n",
    "    normalized_tensor = motion_tensor.clone()\n",
    "    normalized_tensor = 2 * (normalized_tensor - min_vals) / denominator - 1\n",
    "    normalized_tensor = torch.clamp(normalized_tensor, -1, 1)\n",
    "    # Calculate quantiles for each feature dimension\n",
    "    quantiles = torch.tensor([0.0, 0.005, 0.1, 0.25, 0.5, 0.75, 0.9, 0.995, 1.0])\n",
    "    num_features = normalized_tensor.shape[-1]\n",
    "    \n",
    "    print(\"Quantiles for each feature dimension:\")\n",
    "    for i in range(num_features):\n",
    "        feat_quantiles = torch.quantile(normalized_tensor[:, i], quantiles)\n",
    "        print(f\"Feature {i}: {feat_quantiles.tolist()}\")\n",
    "\n",
    "    return normalized_tensor\n",
    "\n",
    "def reverse_normalize_motion_tensorr(motion_tensor, latent_bound, latent_mask_1):\n",
    "    assert latent_bound is not None and latent_mask_1 is not None\n",
    "    latent_bound = torch.tensor(latent_bound).reshape(-1, 2)\n",
    "    full_63_exp = torch.zeros(motion_tensor.shape[0], 68)\n",
    "    for i, dim in enumerate(latent_mask_1):\n",
    "        min_bound = latent_bound[dim][0]\n",
    "        max_bound = latent_bound[dim][1]\n",
    "        full_63_exp[:, dim] = (motion_tensor[:, i] + 1) * (max_bound - min_bound) / 2 + min_bound\n",
    "    return full_63_exp.reshape(-1, 68)\n",
    "\n",
    "'''\n",
    "Part 2: Frontalize the motion data, swtich euler angles to quaternions\n",
    "'''\n",
    "\n",
    "def euler_to_quaternion(pitch, yaw, roll):\n",
    "    cy = torch.cos(yaw * 0.5)\n",
    "    sy = torch.sin(yaw * 0.5)\n",
    "    cp = torch.cos(pitch * 0.5)\n",
    "    sp = torch.sin(pitch * 0.5)\n",
    "    cr = torch.cos(roll * 0.5)\n",
    "    sr = torch.sin(roll * 0.5)\n",
    "\n",
    "    w = cr * cp * cy + sr * sp * sy\n",
    "    x = sr * cp * cy - cr * sp * sy\n",
    "    y = cr * sp * cy + sr * cp * sy\n",
    "    z = cr * cp * sy - sr * sp * cy\n",
    "\n",
    "    return torch.stack([w, x, y, z], dim=-1)\n",
    "\n",
    "def angular_distance_vectorized(poses1, poses2):\n",
    "    diff = torch.abs(poses1.unsqueeze(1) - poses2.unsqueeze(0))\n",
    "    diff = torch.min(diff, 2*torch.pi - diff)\n",
    "    return torch.norm(diff, dim=2)\n",
    "\n",
    "def find_dominant_pose(poses):\n",
    "    distances = angular_distance_vectorized(poses, poses)\n",
    "    total_distances = torch.sum(distances, dim=1)\n",
    "    min_distance_index = torch.argmin(total_distances)\n",
    "    return poses[min_distance_index], min_distance_index\n",
    "\n",
    "def get_rotation_matrix(pitch_, yaw_, roll_):\n",
    "    \"\"\" the input is in degree\n",
    "    \"\"\"\n",
    "    # transform to radian\n",
    "    pitch = pitch_ / 180 * np.pi\n",
    "    yaw = yaw_ / 180 * np.pi\n",
    "    roll = roll_ / 180 * np.pi\n",
    "\n",
    "    device = pitch.device\n",
    "\n",
    "    if pitch.ndim == 1:\n",
    "        pitch = pitch.unsqueeze(1)\n",
    "    if yaw.ndim == 1:\n",
    "        yaw = yaw.unsqueeze(1)\n",
    "    if roll.ndim == 1:\n",
    "        roll = roll.unsqueeze(1)\n",
    "\n",
    "    # calculate the euler matrix\n",
    "    bs = pitch.shape[0]\n",
    "    ones = torch.ones([bs, 1]).to(device)\n",
    "    zeros = torch.zeros([bs, 1]).to(device)\n",
    "    x, y, z = pitch, yaw, roll\n",
    "\n",
    "    rot_x = torch.cat([\n",
    "        ones, zeros, zeros,\n",
    "        zeros, torch.cos(x), -torch.sin(x),\n",
    "        zeros, torch.sin(x), torch.cos(x)\n",
    "    ], dim=1).reshape([bs, 3, 3])\n",
    "\n",
    "    rot_y = torch.cat([\n",
    "        torch.cos(y), zeros, torch.sin(y),\n",
    "        zeros, ones, zeros,\n",
    "        -torch.sin(y), zeros, torch.cos(y)\n",
    "    ], dim=1).reshape([bs, 3, 3])\n",
    "\n",
    "    rot_z = torch.cat([\n",
    "        torch.cos(z), -torch.sin(z), zeros,\n",
    "        torch.sin(z), torch.cos(z), zeros,\n",
    "        zeros, zeros, ones\n",
    "    ], dim=1).reshape([bs, 3, 3])\n",
    "\n",
    "    rot = rot_z @ rot_y @ rot_x\n",
    "    return rot.permute(0, 2, 1)  # transpose\n",
    "\n",
    "@torch.no_grad()\n",
    "def process_motion_tensor(motion_tensor, audio_tensor, latent_type='exp', latent_mask_1=None, latent_bound=None):\n",
    "    device = motion_tensor.device\n",
    "    n_batches, seq_len, _ = motion_tensor.shape\n",
    "    all_in_bound = torch.ones(n_batches, seq_len, dtype=torch.bool)\n",
    "    \n",
    "    return_dominant_headpose = True\n",
    "    # Extract each component\n",
    "    kp = motion_tensor[:, :, :63]\n",
    "    exp = motion_tensor[:, :, 63:126]\n",
    "    translation = motion_tensor[:, :, 126:129]\n",
    "    orientation = motion_tensor[:, :, 129:132]\n",
    "    scale = motion_tensor[:, :, 132:133]\n",
    "    eye_open_ratio = motion_tensor[:, :, 133:135]\n",
    "    mouth_open_ratio = motion_tensor[:, :, 135:136]\n",
    "    \n",
    "    if return_dominant_headpose:\n",
    "        '''\n",
    "        x_d_i_new = scale_tensor * (x_c_s @ R_tensor + exp_tensor) + t_tensor\n",
    "        \n",
    "        In version one:\n",
    "        1. we compute the batch avg of kp used as shape feat\n",
    "        2. we keep the exp the same\n",
    "        3. we use the global avg of scale, so constant\n",
    "        4. we subtract the dominant t and rot from each frame\n",
    "        '''\n",
    "        # Find dominant orientation and translation for each frame\n",
    "        dominant_orientations = torch.zeros((n_batches, 3), device=device)\n",
    "        dominant_translations = torch.zeros((n_batches, 3), device=device)\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            dominant_orientations[i], _ = find_dominant_pose(orientation[i])\n",
    "            dominant_translations[i] = torch.median(translation[i], dim=0).values\n",
    "        \n",
    "        # Subtract dominant orientation and translation\n",
    "        orientation_adjusted = orientation - dominant_orientations.unsqueeze(1)\n",
    "        translation_adjusted = translation - dominant_translations.unsqueeze(1)\n",
    "        \n",
    "    if True:\n",
    "        '''\n",
    "        Use exp for motion representation\n",
    "        '''\n",
    "        \n",
    "        motion_tensor = torch.tensor([])\n",
    "        exp = exp.reshape(n_batches, seq_len, -1)\n",
    "        if latent_mask_1 is not None:\n",
    "            for i, d in enumerate(latent_mask_1):\n",
    "                if d >= 63:\n",
    "                    continue # skip headpose features\n",
    "                if i == 0:\n",
    "                    motion_tensor = exp[:, :, d:d+1]\n",
    "                else:\n",
    "                    motion_tensor = torch.cat([motion_tensor, exp[:, :, d:d+1]], dim=2)\n",
    "            motion_tensor = motion_tensor.reshape(n_batches, seq_len, -1)\n",
    "        # compute canonical shape kp, using the average of first 5 frames\n",
    "        first_frame_kp = torch.mean(kp[:, :5, :], dim=1)\n",
    "        \n",
    "        # Compute the median of mouth_open_ratio\n",
    "        median_mouth_open_ratio = torch.median(mouth_open_ratio)\n",
    "        mouth_open_ratio = median_mouth_open_ratio.expand(n_batches, 1)\n",
    "        \n",
    "        if return_dominant_headpose:\n",
    "            translation_adjusted = translation_adjusted[:, :, :2] # z is always 0\n",
    "            motion_tensor = torch.cat([motion_tensor, orientation_adjusted, translation_adjusted], dim=2)\n",
    "        print(\"motion_tensor.shape\", motion_tensor.shape)\n",
    "        return motion_tensor, audio_tensor, first_frame_kp, mouth_open_ratio\n",
    "\n",
    "\n",
    "'''\n",
    "Part 3: Create a custom dataset class for random sampling\n",
    "'''\n",
    "class MotionAudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        motion_latents, audio_latents, shape_latents, mouth_latents, end_indices = data\n",
    "        assert len(motion_latents) == len(audio_latents), \"Motion and audio latents must have the same length\"\n",
    "        assert len(motion_latents) == len(shape_latents), \"Motion and shape latents must have the same length\"\n",
    "        assert len(motion_latents) == len(mouth_latents), \"Motion and mouth latents must have the same length\"\n",
    "        assert len(motion_latents) == len(end_indices), \"Motion and end indices must have the same length\"\n",
    "        self.motion_latents = motion_latents\n",
    "        self.audio_latents = audio_latents\n",
    "        self.shape_latents = shape_latents\n",
    "        self.mouth_latents = mouth_latents\n",
    "        self.end_indices = end_indices\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.motion_latents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"motion_latent\": self.motion_latents[idx], \n",
    "            \"audio_latent\": self.audio_latents[idx],\n",
    "            \"shape_latent\": self.shape_latents[idx],\n",
    "            \"mouth_latent\": self.mouth_latents[idx],\n",
    "            \"end_indices\": self.end_indices[idx]\n",
    "        }\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_root = '/mnt/e/data/live_latent/audio_latent/'\n",
    "motion_root = '/mnt/e/data/live_latent/motion_temp/'\n",
    "hdtf_root = '/mnt/e/data/live_latent/hdtf/live_latent/'\n",
    "hdtf_audio_root = '/mnt/e/data/live_latent/hdtf/audio_latent/'\n",
    "start_idx = 0\n",
    "end_idx = 4\n",
    "hdtf_start_idx = 0\n",
    "hdtf_end_idx = 337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_mask_1 = [ 4, 6, 7, 22, 33, 34, 40, 43, 45, 46, 48, 51, 52, 53, 57, 58, 59, 60, 61, 62 ] # deleted 49,\n",
    "loss_weight   = [ 1, 1, 1, 1,  2,  3,  1,  1,  2,  3,  2,  1,  1,  1,  1,  2,  1,  1,  3,  1, ]\n",
    "latent_mask_1 += [i for i in range(62, 62 + 5)] # add 5 headpose features\n",
    "loss_weight +=  [1, 1, 1, 1, 1] # relative scale is [0.003, 0.002, 0.003, 0.2, 0.2]\n",
    "latent_bound_list =[\n",
    "    -0.05029296875,        0.0857086181640625,   -0.07587742805480957,  0.058624267578125,   -0.0004341602325439453,  0.00019466876983642578, \n",
    "    -0.038482666015625,    0.0345458984375,      -0.030120849609375,    0.038360595703125,   -3.0279159545898438e-05, 1.3887882232666016e-05,\n",
    "    -0.0364990234375,      0.036102294921875,    -0.043212890625,       0.046844482421875,   -4.3332576751708984e-05, 1.8775463104248047e-05, \n",
    "    -0.03326416015625,     0.057373046875,       -0.03460693359375,     0.031707763671875,   -0.0001958608627319336,  0.0005192756652832031,\n",
    "    -0.0728759765625,      0.0587158203125,      -0.04840087890625,     0.039642333984375,   -0.00025916099548339844, 0.00048089027404785156, \n",
    "    -0.09722900390625,     0.12469482421875,     -0.1556396484375,      0.09326171875,       -0.00018024444580078125, 0.00037860870361328125,\n",
    "    -0.0279384758323431,   0.010650634765625,    -0.039306640625,       0.03802490234375,    -1.049041748046875e-05,  3.6954879760742188e-06, \n",
    "    -0.032989501953125,    0.044281005859375,    -0.037261962890625,    0.0433349609375,     -0.00022792529489379376, 0.0003247261047363281,\n",
    "    -0.0288234855979681,   0.006015777587890625, -0.0108795166015625,   0.0134124755859375,  -7.784366607666016e-05,  5.2034854888916016e-05, \n",
    "    -0.01531982421875,     0.027801513671875,    -0.036041259765625,    0.0242156982421875,  -8.83340835571289e-05,   2.6464462280273438e-05,\n",
    "    -0.06463623046875,     0.0303802490234375,   -0.0446159653365612,   0.03619384765625,    -0.02947998046875,       0.030792236328125, \n",
    "    -0.0159145500510931,   0.018890380859375,    -0.01898193359375,     0.0264739990234375,  -6.103515625e-05,        3.266334533691406e-05,\n",
    "    -0.0094450069591403,   0.00604248046875,     -0.005710510071367025, 0.00557708740234375, -2.866983413696289e-05,  1.4543533325195312e-05, \n",
    "    -0.0265350341796875,   0.01186370849609375,  -0.0227047111839056,   0.01386260986328125, -0.000133514404296875,   6.687641143798828e-05, \n",
    "    -0.01129150390625,     0.01331329345703125,  -0.0251922607421875,   0.0195465087890625,  -8.285045623779297e-06,  6.079673767089844e-06, \n",
    "    -0.0141599727794528,   0.018341064453125,    -0.0189971923828125,   0.029296875,         -6.049728108337149e-05,  3.057718276977539e-05, \n",
    "    -0.01216888427734375,  0.02069091796875,     -0.016754150390625,    0.017974853515625,   -0.00014078617095947266, 6.842613220214844e-05, \n",
    "    -0.01910400390625,     0.016204833984375,    -0.025634765625,       0.04150390625,       -0.0100250244140625,     0.00991058349609375, \n",
    "    -0.005596160888671875, 0.01132965087890625,  -0.0269775390625,      0.02166748046875,    -0.000362396240234375,   9.059906005859375e-05,\n",
    "    -0.0325927734375,      0.038818359375,       -0.05877685546875,     0.076416015625,      -0.02215576171875,       0.019775390625, \n",
    "    -0.0219573974609375,   0.0247344970703125,   -0.039764404296875,    0.045,               -0.01512908935546875,    0.017730712890625,    \n",
    "    -21,                   25,                   -30,                   30,                  -23,                     23,\n",
    "    -0.3,                  0.3,                  -0.06,                 0.28,                \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing directories:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing directories:  25%|██▌       | 1/4 [00:00<00:00,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([7, 75, 26])\n",
      "motion_tensor.shape torch.Size([5, 75, 26])\n",
      "motion_tensor.shape torch.Size([4, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([4, 75, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing directories:  50%|█████     | 2/4 [00:00<00:00,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion_tensor.shape torch.Size([4, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([4, 75, 26])\n",
      "motion_tensor.shape torch.Size([6, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([5, 75, 26])\n",
      "motion_tensor.shape torch.Size([5, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([5, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([8, 75, 26])\n",
      "motion_tensor.shape torch.Size([4, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([5, 75, 26])\n",
      "motion_tensor.shape torch.Size([5, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([4, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing directories: 100%|██████████| 4/4 [00:00<00:00,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion_tensor.shape torch.Size([4, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([4, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([4, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([2, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n",
      "motion_tensor.shape torch.Size([3, 75, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vox_motion_latents, vox_audio_latents, _, _, end_indices = load_npy_files(audio_root, motion_root, start_idx, end_idx, 'exp', latent_mask_1, latent_bound_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([193, 75, 26])\n",
      "Total number of elements:  376350\n",
      "Number of zeros:  tensor(45590)\n"
     ]
    }
   ],
   "source": [
    "print(vox_motion_latents.shape)\n",
    "print(\"Total number of elements: \", vox_motion_latents.numel())\n",
    "print(\"Number of zeros: \", torch.sum(vox_motion_latents == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion_tensor shape: torch.Size([193, 75, 25])\n",
      "Quantiles for each feature dimension:\n",
      "Feature 0: [-1.0, -0.8701474666595459, -0.3793734908103943, -0.198322594165802, -0.014149308204650879, 0.1588902473449707, 0.3386431932449341, 0.9498491287231445, 1.0]\n",
      "Feature 1: [-1.0, -0.8525930047035217, -0.3816734552383423, -0.19672685861587524, -0.01488637924194336, 0.16243517398834229, 0.3457413613796234, 0.9934989213943481, 1.0]\n",
      "Feature 2: [-1.0, -0.8062981963157654, -0.38438737392425537, -0.20242154598236084, -0.01422119140625, 0.1599721908569336, 0.35126984119415283, 0.9962285757064819, 1.0]\n",
      "Feature 3: [-1.0, -0.837213933467865, -0.3835735321044922, -0.20214444398880005, -0.011363625526428223, 0.16139960289001465, 0.349974125623703, 0.9685590863227844, 1.0]\n",
      "Feature 4: [-1.0, -0.8253883123397827, -0.37673240900039673, -0.2001953125, -0.011161446571350098, 0.16015136241912842, 0.34138160943984985, 1.0, 1.0]\n",
      "Feature 5: [-1.0, -0.8158605098724365, -0.3779662549495697, -0.1990944743156433, -0.009886384010314941, 0.15894591808319092, 0.3453187048435211, 0.9914391040802002, 1.0]\n",
      "Feature 6: [-1.0, -0.8261224031448364, -0.38408195972442627, -0.1959574818611145, -0.009935915470123291, 0.16193783283233643, 0.3504095673561096, 1.0, 1.0]\n",
      "Feature 7: [-1.0, -0.8231714367866516, -0.3799296021461487, -0.19513678550720215, -0.009983718395233154, 0.16466999053955078, 0.34492266178131104, 0.9926602840423584, 1.0]\n",
      "Feature 8: [-1.0, -0.8435105681419373, -0.38416948914527893, -0.20050442218780518, -0.009326756000518799, 0.15638506412506104, 0.3452887237071991, 0.9328545928001404, 1.0]\n",
      "Feature 9: [-1.0, -0.8365795016288757, -0.3752528131008148, -0.20002323389053345, -0.011725127696990967, 0.1582392454147339, 0.3420149087905884, 0.93406081199646, 1.0]\n",
      "Feature 10: [-1.0, -0.8238050937652588, -0.373985230922699, -0.19900035858154297, -0.012610316276550293, 0.15730202198028564, 0.3400881290435791, 0.9396331310272217, 1.0]\n",
      "Feature 11: [-1.0, -0.8183094263076782, -0.3729305863380432, -0.1967976689338684, -0.012400627136230469, 0.15375113487243652, 0.3320554792881012, 0.92350834608078, 1.0]\n",
      "Feature 12: [-1.0, -0.8282368183135986, -0.37401920557022095, -0.1951189637184143, -0.009772717952728271, 0.1548532247543335, 0.3241347670555115, 0.9182726144790649, 1.0]\n",
      "Feature 13: [-1.0, -0.8485571146011353, -0.3725953698158264, -0.1938989758491516, -0.011698424816131592, 0.15353035926818848, 0.31924504041671753, 0.9985608458518982, 1.0]\n",
      "Feature 14: [-1.0, -0.8307426571846008, -0.37891024351119995, -0.19317197799682617, -0.013747692108154297, 0.1554335355758667, 0.32699713110923767, 0.9173438549041748, 1.0]\n",
      "Feature 15: [-1.0, -0.8156222105026245, -0.3706921339035034, -0.19360476732254028, -0.015482068061828613, 0.15231025218963623, 0.33308327198028564, 0.8732640147209167, 1.0]\n",
      "Feature 16: [-1.0, -0.8431835770606995, -0.36533910036087036, -0.19681447744369507, -0.013396739959716797, 0.15532851219177246, 0.33410727977752686, 0.8440545797348022, 1.0]\n",
      "Feature 17: [-1.0, -0.8236229419708252, -0.36174213886260986, -0.19685131311416626, -0.01492154598236084, 0.1469435691833496, 0.31982147693634033, 0.8278119564056396, 1.0]\n",
      "Feature 18: [-1.0, -0.8143540620803833, -0.3660295009613037, -0.19440245628356934, -0.013988971710205078, 0.14607131481170654, 0.3201013505458832, 0.7775577306747437, 1.0]\n",
      "Feature 19: [-1.0, -0.7970902323722839, -0.36550846695899963, -0.19272804260253906, -0.013427734375, 0.14734017848968506, 0.3164213001728058, 0.8086835145950317, 1.0]\n",
      "Feature 20: [-1.0, -0.7928312420845032, -0.3619956076145172, -0.19003945589065552, -0.01671963930130005, 0.14874660968780518, 0.31586483120918274, 0.835638165473938, 1.0]\n",
      "Feature 21: [-1.0, -0.8010885715484619, -0.3633532226085663, -0.1914529800415039, -0.01653498411178589, 0.14706885814666748, 0.3200945556163788, 0.8133418560028076, 1.0]\n",
      "Feature 22: [-1.0, -0.7892166376113892, -0.364529013633728, -0.1877785325050354, -0.016799569129943848, 0.14363837242126465, 0.31928834319114685, 0.7850011587142944, 1.0]\n",
      "Feature 23: [-1.0, -0.7829315066337585, -0.36195892095565796, -0.18392574787139893, -0.01962345838546753, 0.1434873342514038, 0.32103586196899414, 0.7701416015625, 1.0]\n",
      "Feature 24: [-1.0, -0.8073344230651855, -0.36859744787216187, -0.18989640474319458, -0.01637566089630127, 0.14310836791992188, 0.3261123597621918, 0.7956814765930176, 1.0]\n"
     ]
    }
   ],
   "source": [
    "normalized_motion = normalize_motion_tensor(vox_motion_latents, latent_bound_list, latent_mask_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_motion = normalized_motion.reshape(-1, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_motion = reverse_normalize_motion_tensorr(normalized_motion, latent_bound_list, latent_mask_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14475, 68])\n"
     ]
    }
   ],
   "source": [
    "print(unnormalized_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_motion = unnormalized_motion.reshape(-1, 75, 68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2863)\n",
      "First elements of vox_motion_latents:\n",
      "tensor([-2.0161e-03,  1.2903e-03, -1.3908e-02,  6.9904e-04,  6.1493e-03,\n",
      "         6.8321e-03, -5.5580e-03, -7.2403e-03, -3.5973e-03, -2.9221e-03,\n",
      "         3.5095e-03,  4.4441e-03,  5.5199e-03,  1.0109e-03,  5.4207e-03,\n",
      "        -2.0966e-02, -8.4915e-03,  8.1635e-04,  5.5962e-03,  3.4142e-03,\n",
      "         3.4142e-03,  5.0877e+00,  1.6544e+00, -6.7100e+00, -6.7963e-02,\n",
      "         1.3336e-01])\n",
      "\n",
      "First elements of unnormalized_motion:\n",
      "tensor([-2.0161e-03,  1.2903e-03, -1.3908e-02,  6.9904e-04,  6.1493e-03,\n",
      "         6.8321e-03, -5.5580e-03, -7.2403e-03, -3.5973e-03, -2.9221e-03,\n",
      "         3.5095e-03,  4.4441e-03,  5.5199e-03,  1.0109e-03,  5.4207e-03,\n",
      "        -2.0966e-02, -8.4915e-03,  8.1635e-04,  5.5962e-03,  3.4142e-03,\n",
      "         3.4142e-03,  5.0877e+00,  1.6544e+00, -6.7100e+00, -6.7963e-02])\n",
      "\n",
      "Difference between the first two elements:\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -9.3132e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  1.9073e-06,  0.0000e+00,  1.4901e-08],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  3.7253e-09,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "\n",
      "Mean absolute difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# compare the unnormalized motion and the original motion\n",
    "local_unnormalized_motion = unnormalized_motion.reshape(-1, 68)[:, latent_mask_1]\n",
    "local_vox_motion_latents = vox_motion_latents[:, :, :25].reshape(-1, 25)\n",
    "print(torch.norm(local_unnormalized_motion - local_vox_motion_latents, dim=1).mean())\n",
    "# Print the first two elements of vox_motion_latents and unnormalized_motion\n",
    "print(\"First elements of vox_motion_latents:\")\n",
    "print(vox_motion_latents[0, 0, :])\n",
    "\n",
    "print(\"\\nFirst elements of unnormalized_motion:\")\n",
    "print(unnormalized_motion[0, 0, :][latent_mask_1])\n",
    "\n",
    "# Calculate and print the difference\n",
    "difference = local_vox_motion_latents[:2] - local_unnormalized_motion[:2]\n",
    "print(\"\\nDifference between the first two elements:\")\n",
    "print(difference)\n",
    "\n",
    "# Calculate and print the mean absolute difference\n",
    "mean_abs_diff = torch.abs(difference).mean()\n",
    "print(f\"\\nMean absolute difference: {mean_abs_diff:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_length, _ = vox_motion_latents.shape\n",
    "mask = torch.arange(seq_length, device=vox_motion_latents.device).expand(batch_size, seq_length) < end_indices.unsqueeze(1)\n",
    "# Apply the mask to both x_pred and x_gt\n",
    "vox_motion_latents = vox_motion_latents[mask]\n",
    "vox_motion_latents = vox_motion_latents.unsqueeze(1)\n",
    "print(vox_motion_latents.shape)\n",
    "print(\"Total number of elements: \", vox_motion_latents.numel())\n",
    "print(\"Number of zeros: \", torch.sum(vox_motion_latents == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_vals, _ = torch.min(vox_motion_latents.reshape(-1, vox_motion_latents.shape[-1]), dim=(0))\n",
    "print(min_vals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_vals, _ = torch.min(vox_motion_latents.reshape(-1, vox_motion_latents.shape[-1]), dim=(0))\n",
    "max_vals, _ = torch.max(vox_motion_latents.reshape(-1, vox_motion_latents.shape[-1]), dim=(0))\n",
    "\n",
    "# Avoid division by zero\n",
    "denominator = (max_vals - min_vals)\n",
    "denominator[denominator == 0] = 1.0  # Set to 1 where max and min are the same\n",
    "\n",
    "vox_motion_latents = 2 * (vox_motion_latents - min_vals) / denominator - 1\n",
    "\n",
    "# Ensure the tensor is within [-1, 1] range\n",
    "vox_motion_latents = torch.clamp(vox_motion_latents, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_mask = [i for i in range(57, 63)]\n",
    "latent_mask = [i for i in range(15, 20)]\n",
    "\n",
    "# picked_vox_motion_latents = vox_motion_latents[:,:, latent_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdtf_audio_latents, hdtf_motion_latents = load_npy_files(hdtf_audio_root, hdtf_root, hdtf_start_idx, hdtf_end_idx)\n",
    "# hdtf_motion_latents, hdtf_audio_latents, _= process_motion_tensor(hdtf_motion_latents,hdtf_audio_latents, 'exp', latent_mask_1 = latent_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load into motion latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motion_latents = torch.cat([vox_motion_latents, hdtf_motion_latents], dim=0)\n",
    "motion_latents = torch.cat([vox_motion_latents], dim=0)\n",
    "print(motion_latents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Plot raw latents distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Show quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(latent_mask)):\n",
    "    quantiles = torch.quantile(motion_latents[:, :, i], torch.tensor([0.0, 0.005, 0.1, 0.25, 0.5, 0.75, 0.9, 0.995, 1.0]))\n",
    "    std = torch.std(motion_latents[:, :, i])\n",
    "    print(f\"Latent {i}: Quantiles:\\n {quantiles.numpy()},\\n Std: {std.item()}\")\n",
    "    \n",
    "    # Find the non-zero quantile\n",
    "    non_zero_data = motion_latents[:, :, i][motion_latents[:, :, i] != 0]\n",
    "    quantiles = torch.quantile(non_zero_data, torch.tensor([0.0, 0.005, 0.1, 0.25, 0.5, 0.75, 0.9, 0.995, 1.0]))\n",
    "    std = torch.std(non_zero_data)\n",
    "    print(f\"Non-zero Latent {i}: Quantiles:\\n {quantiles.numpy()},\\n Std: {std.item()}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "\n",
    "# Print quantiles for vox_orientation_latents\n",
    "# for i in range(vox_orientation_latents.shape[-1]):\n",
    "#     quantiles = torch.quantile(vox_orientation_latents[:, i], torch.tensor([0.0, 0.005, 0.1, 0.25, 0.5, 0.75, 0.9, 0.995, 1.0]))\n",
    "#     std = torch.std(vox_orientation_latents[:, i])\n",
    "#     print(f\"Orientation Latent {i}: Quantiles:\\n {quantiles.numpy()},\\n Std: {std.item()}\")\n",
    "\n",
    "\n",
    "# # Print quantiles for vox_translation_latents\n",
    "# for i in range(vox_translation_latents.shape[-1]):\n",
    "#     quantiles = torch.quantile(vox_translation_latents[:, i], torch.tensor([0.0, 0.005, 0.1, 0.25, 0.5, 0.75, 0.9, 0.995, 1.0]))\n",
    "#     std = torch.std(vox_translation_latents[:, i])\n",
    "#     print(f\"Translation Latent {i}: Quantiles:\\n {quantiles.numpy()},\\n Std: {std.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Plot in histgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print motion 1\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "latents_analysis = motion_latents\n",
    "# latents_analysis = aligned_latents\n",
    "latent_dim = len(latent_mask)\n",
    "\n",
    "# Flatten the batch and length dimensions\n",
    "flattened_motion = latents_analysis.view(-1, latents_analysis.shape[-1])\n",
    "\n",
    "# Plot histograms for each feature\n",
    "column_num = 6\n",
    "fig, axes = plt.subplots(latent_dim // column_num + int(latent_dim % column_num != 0), column_num, figsize=(20, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, dim in tqdm(enumerate(latent_mask), total=len(latent_mask), desc=\"Plotting histograms\"):\n",
    "    print(\"Plotting feature: \", dim, \", index: \", i)\n",
    "    data = flattened_motion[:, dim].numpy()\n",
    "    mean = np.mean(data)\n",
    "    \n",
    "    # Plot histogram\n",
    "    sns.histplot(data, ax=axes[i], kde=True)\n",
    "    \n",
    "    # Find the bar that contains the mean and color it red\n",
    "    for patch in axes[i].patches:\n",
    "        if patch.get_x() <= mean < patch.get_x() + patch.get_width():\n",
    "            patch.set_facecolor('red')\n",
    "            break\n",
    "    \n",
    "    print(\"Plotted feature: \", dim, \", index: \", i)\n",
    "    axes[i].set_title(f'Feature {dim}')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "    axes[i].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "    \n",
    "    # Add a vertical line at the mean\n",
    "    axes[i].axvline(mean, color='red', linestyle='dashed', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # Plot box plots for each feature\n",
    "# plt.figure(figsize=(20, 6))\n",
    "# sns.boxplot(data=flattened_motion.numpy())\n",
    "# plt.title('Box Plot of Motion Features')\n",
    "# plt.xlabel('Feature Index')\n",
    "# plt.ylabel('Value')\n",
    "# plt.gca().xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "# plt.gca().yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot vox_translation and orientation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Flatten the batch and length dimensions\n",
    "flattened_vox_translation = vox_translation_latents\n",
    "flattened_orientation = vox_orientation_latents\n",
    "\n",
    "# Plot vox_translation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(3):\n",
    "    sns.histplot(flattened_vox_translation[:, i].numpy(), ax=axes[i], kde=True)\n",
    "    axes[i].set_title(f'Vox Translation {[\"X\", \"Y\", \"Z\"][i]}')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "    axes[i].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot orientation\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(3):\n",
    "    sns.histplot(flattened_orientation[:, i].numpy(), ax=axes[i], kde=True)\n",
    "    axes[i].set_title(f'Orientation {i+1}')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "    axes[i].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clamp at will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
    "# 0-6 bound\n",
    "# -0.05029296875, 0.0857086181640625, -0.07587742805480957, 0.058624267578125, -0.0004341602325439453, 0.00019466876983642578, \n",
    "#  -0.038482666015625, 0.0345458984375, -0.030120849609375, 0.038360595703125, -3.0279159545898438e-05, 1.3887882232666016e-05,\n",
    "# 6-12 bound\n",
    "# -0.0364990234375, 0.036102294921875, -0.043212890625, 0.046844482421875, -4.3332576751708984e-05, 1.8775463104248047e-05, \n",
    "# -0.03326416015625, 0.057373046875, -0.03460693359375, 0.031707763671875, -0.0001958608627319336, 0.0005192756652832031,\n",
    "# 12-18 bound\n",
    "# -0.0728759765625, 0.0587158203125, -0.04840087890625, 0.039642333984375, -0.00025916099548339844, 0.00048089027404785156, \n",
    "#  -0.09722900390625, 0.12469482421875, -0.1556396484375, 0.09326171875, -0.00018024444580078125, 0.00037860870361328125,\n",
    "# 18-24 bound\n",
    "# -0.0279384758323431, 0.010650634765625, -0.039306640625, 0.03802490234375, -1.049041748046875e-05, 3.6954879760742188e-06, \n",
    "#  -0.032989501953125, 0.044281005859375, -0.037261962890625, 0.0433349609375, -0.00022792529489379376, 0.0003247261047363281,\n",
    "# 24-30 bound\n",
    "# -0.0288234855979681, 0.006015777587890625, -0.0108795166015625, 0.0134124755859375, -7.784366607666016e-05, 5.2034854888916016e-05, \n",
    "#  -0.01531982421875, 0.027801513671875, -0.036041259765625, 0.0242156982421875, -8.83340835571289e-05, 2.6464462280273438e-05,\n",
    "# 30-36 bound\n",
    "# -0.06463623046875, 0.0303802490234375, -0.0446159653365612, 0.03619384765625, -0.02947998046875, 0.030792236328125, -0.0159145500510931, \n",
    "#  0.018890380859375, -0.01898193359375, 0.0264739990234375, -6.103515625e-05, 3.266334533691406e-05\n",
    "# 36-42 bound\n",
    "# -0.0094450069591403, 0.00604248046875, -0.005710510071367025, 0.00557708740234375, -2.866983413696289e-05, 1.4543533325195312e-05, \n",
    "#  -0.0265350341796875, 0.01186370849609375, -0.0227047111839056, 0.01386260986328125, -0.000133514404296875, 6.687641143798828e-05, \n",
    "# 42-48 bound\n",
    "# -0.01129150390625, 0.01331329345703125, -0.0251922607421875, 0.0195465087890625, -8.285045623779297e-06, 6.079673767089844e-06, \n",
    "#  -0.0141599727794528, 0.018341064453125, -0.0189971923828125, 0.029296875, -6.049728108337149e-05, 3.057718276977539e-05, \n",
    "# 48-54 bound\n",
    "# -0.01216888427734375, 0.02069091796875, -0.016754150390625, 0.017974853515625, -0.00014078617095947266, 6.842613220214844e-05, \n",
    "#  -0.01910400390625, 0.016204833984375, -0.025634765625, 0.04150390625, -0.0100250244140625, 0.00991058349609375, \n",
    "#  -0.005596160888671875, 0.01132965087890625, -0.0269775390625, 0.02166748046875, -0.000362396240234375, 9.059906005859375e-05,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a target, generate all_bounds\n",
    "all_bounds = torch.zeros(len(latent_mask), 2)\n",
    "for i in range(len(latent_mask)):\n",
    "    quantiles = torch.quantile(motion_latents[:, :, latent_mask[i]], torch.tensor([target[i], 1-target[i]]))\n",
    "    all_bounds[i] = quantiles\n",
    "print(all_bounds)\n",
    "print([tensor.item() for tensor in all_bounds.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all items in bounds if neede to ( deprecated )\n",
    "\n",
    "\n",
    "# all_in_bound = torch.ones(motion_latents.shape[0], motion_latents.shape[1], dtype=torch.bool)\n",
    "# for i in range(len(latent_mask)):\n",
    "#     all_in_bound &= (motion_latents[:, :, i] >= all_bounds[i][0]) & (motion_latents[:, :, i] <= all_bounds[i][1])\n",
    "\n",
    "# print(all_in_bound.shape)\n",
    "# print(all_in_bound.sum())\n",
    "# print(motion_latents.shape[0] * motion_latents.shape[1])\n",
    "\n",
    "# # Count the number of entries in dim 0 its entries in dim 1 and 2 are all ones\n",
    "# entries_in_bound_dim0 = all_in_bound.all(dim=1).sum()\n",
    "# print(entries_in_bound_dim0)\n",
    "\n",
    "# Count how many of the last dim is full of zeros and drop them\n",
    "# clamped_motion_latents = clamped_motion_latents.reshape(-1, clamped_motion_latents.shape[-1])\n",
    "# non_zero_mask = ~(clamped_motion_latents == 0).all(dim=-1)\n",
    "# clamped_motion_latents = clamped_motion_latents[non_zero_mask]\n",
    "\n",
    "# # Count the number of dropped vectors\n",
    "# dropped_count = (~non_zero_mask).sum().item()\n",
    "# print(f\"Number of vectors dropped (all zeros in last dimension): {dropped_count}\")\n",
    "\n",
    "# # Display the original and new shapes for reference\n",
    "# print(f\"Original shape: {clamped_motion_latents.shape[0] + dropped_count, clamped_motion_latents.shape[1]}\")\n",
    "# print(f\"New shape after dropping zero vectors: {clamped_motion_latents.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clamp motion tensor to boundary\n",
    "clamped_motion_latents = torch.zeros_like(motion_latents)\n",
    "\n",
    "for i in range(len(latent_mask)):\n",
    "    lower_bound = all_bounds[i][0]\n",
    "    upper_bound = all_bounds[i][1]\n",
    "    clamped_motion_latents[:, :, latent_mask[i]] = torch.clamp(motion_latents[:, :, latent_mask[i]], min=lower_bound, max=upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print audio latents analysis ( deprecated )\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "latents_analysis = audio_latents[:, :, :]\n",
    "latent_dim = latents_analysis.shape[-1]\n",
    "\n",
    "# Flatten the batch and length dimensions\n",
    "flattened_motion = latents_analysis.view(-1, latent_dim)\n",
    "\n",
    "# Calculate basic statistics\n",
    "mean = flattened_motion.mean(dim=0)\n",
    "std = flattened_motion.std(dim=0)\n",
    "min_vals = flattened_motion.min(dim=0).values\n",
    "max_vals = flattened_motion.max(dim=0).values\n",
    "\n",
    "# Calculate absolute magnitude statistics\n",
    "abs_flattened_motion = flattened_motion.abs()\n",
    "abs_mean = abs_flattened_motion.mean(dim=0)\n",
    "abs_std = abs_flattened_motion.std(dim=0)\n",
    "abs_min_vals = abs_flattened_motion.min(dim=0).values\n",
    "abs_max_vals = abs_flattened_motion.max(dim=0).values\n",
    "\n",
    "# Create a summary dataframe\n",
    "summary = pd.DataFrame({\n",
    "    'Mean': mean.numpy(),\n",
    "    'Std': std.numpy(),\n",
    "    'Min': min_vals.numpy(),\n",
    "    'Max': max_vals.numpy(),\n",
    "    'Abs Mean': abs_mean.numpy(),\n",
    "    'Abs Std': abs_std.numpy(),\n",
    "    'Abs Min': abs_min_vals.numpy(),\n",
    "    'Abs Max': abs_max_vals.numpy()\n",
    "})\n",
    "\n",
    "# Set display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Print the full summary\n",
    "print(summary.to_string())\n",
    "\n",
    "# # Plot histograms for each feature\n",
    "# fig, axes = plt.subplots(latent_dim // 10 + int(latent_dim % 10 != 0), 10, figsize=(20, 14))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for i in tqdm(range(latent_dim), desc=\"Plotting histograms\"):\n",
    "#     sns.histplot(flattened_motion[:, i].numpy(), ax=axes[i], kde=True)\n",
    "#     axes[i].set_title(f'Feature {i}')\n",
    "#     axes[i].set_xlabel('')\n",
    "#     axes[i].set_ylabel('')\n",
    "#     axes[i].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "#     axes[i].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot box plots for each feature\n",
    "# plt.figure(figsize=(20, 6))\n",
    "# sns.boxplot(data=flattened_motion.numpy())\n",
    "# plt.title('Box Plot of Motion Features')\n",
    "# plt.xlabel('Feature Index')\n",
    "# plt.ylabel('Value')\n",
    "# plt.gca().xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "# plt.gca().yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "# plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clamped_motion_latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print motion 1\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "latents_analysis = clamped_motion_latents\n",
    "# latents_analysis = aligned_latents\n",
    "latent_dim = len(latent_mask)\n",
    "\n",
    "# Flatten the batch and length dimensions\n",
    "flattened_motion = latents_analysis.view(-1, latents_analysis.shape[-1])\n",
    "print(flattened_motion.shape)\n",
    "# # Calculate basic statistics\n",
    "# mean = flattened_motion.mean(dim=0)\n",
    "# std = flattened_motion.std(dim=0)\n",
    "# min_vals = flattened_motion.min(dim=0).values\n",
    "# max_vals = flattened_motion.max(dim=0).values\n",
    "\n",
    "# # Calculate absolute magnitude statistics\n",
    "# abs_flattened_motion = flattened_motion.abs()\n",
    "# abs_mean = abs_flattened_motion.mean(dim=0)\n",
    "# abs_std = abs_flattened_motion.std(dim=0)\n",
    "# abs_min_vals = abs_flattened_motion.min(dim=0).values\n",
    "# abs_max_vals = abs_flattened_motion.max(dim=0).values\n",
    "\n",
    "# # Create a summary dataframe\n",
    "# summary = pd.DataFrame({\n",
    "#     'Mean': mean.numpy(),\n",
    "#     'Std': std.numpy(),\n",
    "#     'Min': min_vals.numpy(),\n",
    "#     'Max': max_vals.numpy(),\n",
    "#     'Abs Mean': abs_mean.numpy(),\n",
    "#     'Abs Std': abs_std.numpy(),\n",
    "#     'Abs Min': abs_min_vals.numpy(),\n",
    "#     'Abs Max': abs_max_vals.numpy()\n",
    "# })\n",
    "\n",
    "# # Set display options to show all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# # Print the full summary\n",
    "# print(summary.to_string())\n",
    "\n",
    "# Plot histograms for each feature\n",
    "column_num = 6\n",
    "fig, axes = plt.subplots(latent_dim // column_num + int(latent_dim % column_num != 0), column_num, figsize=(20, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, dim in tqdm(enumerate(latent_mask), total=len(latent_mask), desc=\"Plotting histograms\"):\n",
    "    sns.histplot(flattened_motion[:, dim].numpy(), ax=axes[i], kde=True)\n",
    "    axes[i].set_title(f'Feature {dim}')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "    axes[i].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # Plot box plots for each feature\n",
    "# plt.figure(figsize=(20, 6))\n",
    "# sns.boxplot(data=flattened_motion.numpy())\n",
    "# plt.title('Box Plot of Motion Features')\n",
    "# plt.xlabel('Feature Index')\n",
    "# plt.ylabel('Value')\n",
    "# plt.gca().xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "# plt.gca().yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deprecated Hard scaling. Likely Won't need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break execution\n",
    "1 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming abs_std is already defined\n",
    "# abs_std = torch.randn(70)  # Example tensor, replace with your actual tensor\n",
    "\n",
    "# Define the bins in logarithmic scale from 1e-1 to 1e-8\n",
    "bins = np.logspace(-1, -8, num=8)\n",
    "\n",
    "# Digitize the abs_std values into the defined bins\n",
    "bin_indices = np.digitize(abs_std[:63].numpy(), bins)\n",
    "\n",
    "# Create a list to store indices for each bin\n",
    "bin_index_lists = [[] for _ in range(len(bins) + 1)]\n",
    "\n",
    "# Populate the bin_index_lists\n",
    "for idx, bin_idx in enumerate(bin_indices):\n",
    "    bin_index_lists[bin_idx].append(idx)\n",
    "\n",
    "# Print the bin counts and indices\n",
    "bin_counts = np.bincount(bin_indices, minlength=len(bins) + 1)\n",
    "for i, (count, indices) in enumerate(zip(bin_counts, bin_index_lists)):\n",
    "    if i == 0:\n",
    "        print(f\"< {bins[0]:.1e}: {count}\")\n",
    "    elif i == len(bins):\n",
    "        print(f\">= {bins[-1]:.1e}: {count}\")\n",
    "    else:\n",
    "        print(f\"{bins[i-1]:.1e} - {bins[i]:.1e}: {count}\")\n",
    "    print(f\"Indices: {indices}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_analysis = motion_latents\n",
    "latent_dim = latents_analysis.shape[-1]\n",
    "\n",
    "# Flatten the batch and length dimensions\n",
    "flattened_motion = latents_analysis.view(-1, latent_dim)\n",
    "\n",
    "flattened_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features):\n",
    "    # Step 1: Standardize (mean=0, std=1)\n",
    "    mean = features.mean(dim=0, keepdim=True)\n",
    "    std = features.std(dim=0, keepdim=True)\n",
    "    standardized = (features - mean) / (std + 1e-8)\n",
    "    \n",
    "    # Step 2: Min-max scaling to [0, 1]\n",
    "    # min_vals = standardized.min(dim=0, keepdim=True)[0]\n",
    "    # max_vals = standardized.max(dim=0, keepdim=True)[0]\n",
    "    # normalized = (standardized - min_vals) / (max_vals - min_vals + 1e-8)\n",
    "    \n",
    "    return standardized\n",
    "\n",
    "flattened_motion = normalize_features(flattened_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "latents_analysis = motion_latents\n",
    "\n",
    "# Flatten the batch and length dimensions\n",
    "flattened_motion = latents_analysis.view(-1, latent_dim)\n",
    "flattened_motion = flattened_motion[:, :63]\n",
    "latent_dim = flattened_motion.shape[-1]\n",
    "# Calculate basic statistics\n",
    "mean = flattened_motion.mean(dim=0)\n",
    "std = flattened_motion.std(dim=0)\n",
    "min_vals = flattened_motion.min(dim=0).values\n",
    "max_vals = flattened_motion.max(dim=0).values\n",
    "\n",
    "# Create a summary dataframe\n",
    "import pandas as pd\n",
    "summary = pd.DataFrame({\n",
    "    'Mean': mean.numpy(),\n",
    "    'Std': std.numpy(),\n",
    "    'Min': min_vals.numpy(),\n",
    "    'Max': max_vals.numpy()\n",
    "})\n",
    "\n",
    "# Set display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Print the full summary\n",
    "print(summary.to_string())\n",
    "\n",
    "# Plot histograms for each feature\n",
    "fig, axes = plt.subplots(latent_dim // 10 + int(latent_dim % 10!=0), 10, figsize=(20, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(latent_dim), desc=\"Plotting histograms\"):\n",
    "    sns.histplot(flattened_motion[:, i].numpy(), ax=axes[i], kde=True)\n",
    "    axes[i].set_title(f'Feature {i}')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot box plots for each feature\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(data=flattened_motion.numpy())\n",
    "plt.title('Box Plot of Motion Features')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=flattened_motion[:, 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def count_distribution(y, bins):\n",
    "    pos_bins = np.array(bins)\n",
    "    neg_bins = -pos_bins[::-1]\n",
    "    all_bins = np.concatenate([neg_bins, [0], pos_bins])\n",
    "    \n",
    "    counts, _ = np.histogram(y, bins=all_bins)\n",
    "    \n",
    "    bin_labels = [f\"-{b}\" for b in bins[::-1]] + [\"0\"] + [f\"+{b}\" for b in bins]\n",
    "    \n",
    "    for label, count in zip(bin_labels, counts):\n",
    "        print(f\"{label}: {count}\")\n",
    "\n",
    "    return counts, bin_labels\n",
    "\n",
    "# Define the bins\n",
    "bins = [0.1, 0.2, 0.5, 1, 2, 5, 10, 20]\n",
    "\n",
    "# Get the data for feature 24\n",
    "y = flattened_motion[:, 24].numpy()\n",
    "\n",
    "# Count the distribution\n",
    "counts, labels = count_distribution(y, bins)\n",
    "\n",
    "# Optionally, you can plot the distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(labels, counts)\n",
    "plt.title('Distribution of Feature 24')\n",
    "plt.xlabel('Bins')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature 24\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(flattened_motion[:, 24].numpy(), kde=True)\n",
    "plt.title('Distribution of Feature 24')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Box plot for feature 24\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(y=flattened_motion[:, 24].numpy())\n",
    "plt.title('Box Plot of Feature 24')\n",
    "plt.ylabel('Value')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics for feature 24\n",
    "feature_24_summary = summary.iloc[24]\n",
    "print(\"Summary statistics for Feature 24:\")\n",
    "print(feature_24_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features):\n",
    "    mean = features.mean(dim=0, keepdim=True)\n",
    "    std = features.std(dim=0, keepdim=True)\n",
    "    return (features - mean) / (std + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "\n",
    "normalized_features = normalize_features(all_coefs['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set display options to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Calculate statistics\n",
    "mean = flattened_motion.mean(dim=0)\n",
    "std = flattened_motion.std(dim=0)\n",
    "min_vals = flattened_motion.min(dim=0).values\n",
    "max_vals = flattened_motion.max(dim=0).values\n",
    "median = flattened_motion.median(dim=0).values  # Use .values to get the tensor\n",
    "percentile_25 = torch.quantile(flattened_motion, 0.25, dim=0)\n",
    "percentile_75 = torch.quantile(flattened_motion, 0.75, dim=0)\n",
    "\n",
    "# Create the summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'Mean': mean.numpy(),\n",
    "    'Std': std.numpy(),\n",
    "    'Min': min_vals.numpy(),\n",
    "    '25%': percentile_25.numpy(),\n",
    "    'Median': median.numpy(),\n",
    "    '75%': percentile_75.numpy(),\n",
    "    'Max': max_vals.numpy()\n",
    "})\n",
    "\n",
    "# Print the full summary\n",
    "print(summary)\n",
    "\n",
    "# Reset display options to default (optional)\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.width')\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Specify the directory containing the LMDB file\n",
    "lmdb_dir = '/mnt/e/data/diffposetalk_data/HDTF_TFHP-lmdb/'\n",
    "\n",
    "# Check if the directory exists and contains the necessary files\n",
    "if not os.path.isdir(lmdb_dir):\n",
    "    print(f\"The directory {lmdb_dir} does not exist.\")\n",
    "elif not os.path.isfile(os.path.join(lmdb_dir, 'data.mdb')):\n",
    "    print(f\"The data file 'data.mdb' is missing from {lmdb_dir}\")\n",
    "elif not os.path.isfile(os.path.join(lmdb_dir, 'lock.mdb')):\n",
    "    print(f\"The lock file 'lock.mdb' is missing from {lmdb_dir}\")\n",
    "else:\n",
    "    print(f\"Found LMDB environment in: {lmdb_dir}\")\n",
    "\n",
    "    # Try to open the LMDB environment\n",
    "    # try:\n",
    "    env = lmdb.open(lmdb_dir, readonly=True, lock=False)\n",
    "    \n",
    "    # Start a new read transaction\n",
    "    with env.begin(write=False) as txn:\n",
    "        # Get the number of keys in the database\n",
    "        num_keys = txn.stat()['entries']\n",
    "        print(f\"Number of entries in the database: {num_keys}\")\n",
    "\n",
    "        # Read all entries\n",
    "        all_data = []\n",
    "        cursor = txn.cursor()\n",
    "        for key, value in tqdm(cursor, total=num_keys, desc=\"Reading LMDB\"):\n",
    "            if key == b'metadata':\n",
    "                metadata = pickle.loads(value)\n",
    "                print(\"Metadata:\", metadata)\n",
    "            else:\n",
    "                entry = pickle.loads(value)\n",
    "                all_data.append(entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_count = {}\n",
    "for entry in all_data:\n",
    "    keys = tuple(entry.keys())  # Convert keys to tuple so it can be used as a dictionary key\n",
    "    if keys in key_count:\n",
    "        key_count[keys] += 1\n",
    "    else:\n",
    "        key_count[keys] = 1\n",
    "\n",
    "# Print the results\n",
    "for keys, count in key_count.items():\n",
    "    print(f\"Keys: {keys}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "coef_keys = ['shape', 'exp', 'pose']\n",
    "all_coefs = {k: [] for k in coef_keys}\n",
    "all_audio = []\n",
    "\n",
    "for entry in all_data:\n",
    "    if 'coef' in entry.keys():\n",
    "        for k in coef_keys:\n",
    "            all_coefs[k].append(entry['coef'][k])\n",
    "    # Uncomment the following line if you want to process audio data\n",
    "    # all_audio.append(torch.from_numpy(np.frombuffer(entry['audio'], dtype=np.float32)))\n",
    "\n",
    "# Convert to tensors and stack arrays with shape[0] == 100\n",
    "for k in coef_keys:\n",
    "    valid_coefs = [coef for coef in all_coefs[k] if coef.shape[0] == 100]\n",
    "    if valid_coefs:\n",
    "        all_coefs[k] = np.stack(valid_coefs, axis=0)\n",
    "        all_coefs[k] = torch.from_numpy(all_coefs[k])\n",
    "    else:\n",
    "        print(f\"Warning: No valid coefficients found for {k}\")\n",
    "        all_coefs[k] = None\n",
    "\n",
    "# Print shapes\n",
    "print(\"\\nData shapes:\")\n",
    "for k in coef_keys:\n",
    "    if all_coefs[k] is not None:\n",
    "        print(f\"{k} shape: {all_coefs[k].shape}\")\n",
    "    else:\n",
    "        print(f\"{k}: No valid data\")\n",
    "\n",
    "# Calculate and print statistics\n",
    "print(\"\\nCoefficient statistics:\")\n",
    "for k in coef_keys:\n",
    "    if all_coefs[k] is not None:\n",
    "        print(f\"\\n{k}:\")\n",
    "        print(f\"  Mean: {all_coefs[k].mean().item():.4f}\")\n",
    "        print(f\"  Std: {all_coefs[k].std().item():.4f}\")\n",
    "        print(f\"  Min: {all_coefs[k].min().item():.4f}\")\n",
    "        print(f\"  Max: {all_coefs[k].max().item():.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n{k}: No valid data for statistics\")\n",
    "\n",
    "# Uncomment the following lines if you want to process audio data\n",
    "# all_audio = torch.cat(all_audio, dim=0)\n",
    "# print(\"\\nAudio shape:\", all_audio.shape)\n",
    "# print(\"\\nAudio statistics:\")\n",
    "# print(f\"  Mean: {all_audio.mean().item():.4f}\")\n",
    "# print(f\"  Std: {all_audio.std().item():.4f}\")\n",
    "# print(f\"  Min: {all_audio.min().item():.4f}\")\n",
    "# print(f\"  Max: {all_audio.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up the plots for all three coefficients\n",
    "for coef_key in ['exp', 'shape', 'pose']:\n",
    "    if all_coefs[coef_key] is not None:\n",
    "        coef_data = all_coefs[coef_key]\n",
    "        num_features = coef_data.shape[-1]\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        grid_rows = int(np.ceil(np.sqrt(num_features)))\n",
    "        grid_cols = int(np.ceil(num_features / grid_rows))\n",
    "        \n",
    "        plt.figure(figsize=(20, 15))\n",
    "        \n",
    "        for feat_idx in range(num_features):\n",
    "            # Extract the current feature dimension\n",
    "            feature = coef_data[..., feat_idx].flatten()\n",
    "            \n",
    "            # Create a subplot for each feature\n",
    "            plt.subplot(grid_rows, grid_cols, feat_idx + 1)\n",
    "            \n",
    "            # Plot the histogram\n",
    "            plt.hist(feature.numpy(), bins=50, edgecolor='black')\n",
    "            \n",
    "            # Set title and labels\n",
    "            plt.title(f'{coef_key.capitalize()} Feature {feat_idx + 1}')\n",
    "            plt.xlabel('Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            \n",
    "            # Add some statistics to the plot\n",
    "            plt.text(0.05, 0.95, f'Mean: {feature.mean():.4f}\\nStd: {feature.std():.4f}', \n",
    "                     transform=plt.gca().transAxes, verticalalignment='top', fontsize=8)\n",
    "\n",
    "        # Adjust the layout and display the plot\n",
    "        plt.suptitle(f'Histogram of {coef_key.capitalize()} Coefficient Features', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No valid data for '{coef_key}' coefficient\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vasa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
