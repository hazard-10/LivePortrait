{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributed import reduce\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from diffusers import DDIMScheduler\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "from models.dit_model import TransformerBackbone\n",
    "from models.vanilla_transformer import VanillaTransformer\n",
    "from dataset import load_npy_files, process_motion_tensor, MotionAudioDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global config\n",
    "config = {\n",
    "    # Model config\n",
    "    \"x_dim\": 63,\n",
    "    \"a_dim\": 768,\n",
    "    \"max_seq_length\": 75,\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 8,\n",
    "    \"num_attention_heads\": 8,\n",
    "    # Training config\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_epochs\": -1,\n",
    "    \"save_interval\": 5,\n",
    "    \"num_iterations\": 8500000,\n",
    "    \"scheduler\": \"none\",\n",
    "    \"warmup_iters\": 5000,\n",
    "    \"lr_min_scale\": 0.2,\n",
    "    \"cos_iters\": 800000,\n",
    "    # Validation config\n",
    "    \"validate_only\": False,\n",
    "    \"valid_batch_size\": 256,\n",
    "    \"validate_interval\": 1000,\n",
    "}\n",
    "\n",
    "# Global data parameters\n",
    "audio_root = '/mnt/e/data/live_encoder_output/audio_latent/'\n",
    "motion_root = '/mnt/e/data/live_encoder_output/live_latent/'\n",
    "start_idx = 0\n",
    "end_idx = 400\n",
    "output_dir = \"output\"\n",
    "checkpoint_dir = None\n",
    "world_size = 1\n",
    "model_type = \"vanilla\"\n",
    "audio_latents, motion_latents = load_npy_files(audio_root, motion_root, start_idx, end_idx)\n",
    "motion_latents, _, _ = process_motion_tensor(motion_latents)\n",
    "\n",
    "# Global training variables\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = None\n",
    "noise_scheduler = None\n",
    "optimizer = None\n",
    "scaler = None\n",
    "lr_scheduler = None\n",
    "dataset = MotionAudioDataset(motion_latents, audio_latents)\n",
    "sampler = DistributedSampler(dataset, num_replicas=1, rank=0, shuffle=True)\n",
    "dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], sampler=sampler, pin_memory=True)\n",
    "\n",
    "\n",
    "epoch_losses = []\n",
    "iteration_losses = []\n",
    "prev_seq_length = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    global model, noise_scheduler, device, model_type, config\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize model based on model_type\n",
    "    if model_type == \"dit\":\n",
    "        model = TransformerBackbone(\n",
    "            x_dim=config[\"x_dim\"],\n",
    "            a_dim=config[\"a_dim\"],\n",
    "            max_seq_length=config[\"max_seq_length\"],\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            num_attention_heads=config[\"num_attention_heads\"],\n",
    "            norm_type=\"ada_norm_zero\",\n",
    "            device=device\n",
    "        ).to(device)\n",
    "    elif model_type == \"vanilla\":\n",
    "        model = VanillaTransformer(\n",
    "            x_dim=config[\"x_dim\"],\n",
    "            a_dim=config[\"a_dim\"],\n",
    "            max_seq_length=config[\"max_seq_length\"],\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            num_heads=config[\"num_attention_heads\"],\n",
    "        ).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    # model = model.bfloat16()\n",
    "\n",
    "    # Initialize noise scheduler (only for DiT model)\n",
    "    if model_type == \"dit\":\n",
    "        noise_scheduler = DDIMScheduler(\n",
    "            num_train_timesteps=1000,\n",
    "            beta_schedule=\"squaredcos_cap_v2\",\n",
    "            clip_sample=False,\n",
    "            set_alpha_to_one=False\n",
    "        )\n",
    "\n",
    "def load_checkpoint():\n",
    "    global model, optimizer, checkpoint_dir, device\n",
    "\n",
    "    model_path = os.path.join(checkpoint_dir, \"model.pth\")\n",
    "    optimizer_path = os.path.join(checkpoint_dir, \"optimizer.pth\")\n",
    "\n",
    "    if os.path.exists(model_path) or os.path.exists(optimizer_path):\n",
    "        if os.path.exists(model_path): \n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            print(f\"Model checkpoint loaded from {model_path}\")\n",
    "        if os.path.exists(optimizer_path) and optimizer is not None:\n",
    "            optimizer.load_state_dict(torch.load(optimizer_path, map_location=device))\n",
    "            print(f\"Checkpoint loaded from {optimizer_path}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found in {checkpoint_dir}\")\n",
    "        \n",
    "def get_scheduler():\n",
    "    global config, optimizer\n",
    "\n",
    "    # if config[\"scheduler\"] == \"none\":\n",
    "    #     return None\n",
    "    # elif config[\"scheduler\"] == \"cosine\":\n",
    "    #     return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    #         optimizer,\n",
    "    #         T_max=config[\"cos_iters\"],\n",
    "    #         eta_min=config[\"learning_rate\"] * config[\"lr_min_scale\"]\n",
    "    #     )\n",
    "    # else:\n",
    "    #     raise ValueError(f\"Unknown scheduler type: {config['scheduler']}\")\n",
    "\n",
    "def run(scheduler_type = \"cosine\", learning_rate = None, epochs = None):\n",
    "    global model, checkpoint_dir, motion_latents, audio_latents, config, device, dataloader, optimizer, lr_scheduler, scaler, epoch_losses, iteration_losses\n",
    "\n",
    "    model.train()\n",
    "    if checkpoint_dir:\n",
    "        load_checkpoint()\n",
    "    \n",
    "    # dataset = MotionAudioDataset(motion_latents, audio_latents)\n",
    "    # dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True, pin_memory=True)\n",
    "    # Initialize optimizer\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    # lr_scheduler = get_scheduler()\n",
    "    \n",
    "        # Choose scheduler based on config\n",
    "    if scheduler_type == \"cosine\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=epoch, eta_min=learning_rate * 0.1\n",
    "        )\n",
    "    elif scheduler_type == \"plateau\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "    elif scheduler_type == \"linear\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=1.0, end_factor=0.1, total_iters=epoch\n",
    "        )\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # iter_per_epoch = data_size // config[\"batch_size\"] + data_size % config[\"batch_size\"]\n",
    "    # config[\"num_epochs\"] = config[\"num_iterations\"] // iter_per_epoch\n",
    "    \n",
    "    epoch_pbar = tqdm(range(epochs), desc=\"Training Epochs\")\n",
    "    iteration = 0\n",
    "    for epoch in epoch_pbar:\n",
    "        batch_loss = torch.zeros(1).to(device)\n",
    "        mini_batch_pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "        for mini_batch in mini_batch_pbar:\n",
    "            loss = train_step(mini_batch)\n",
    "            batch_loss += loss\n",
    "            \n",
    "            iteration_losses.append(loss.item())\n",
    "            iteration += 1\n",
    "            if iteration % 10 == 0:\n",
    "                epoch_pbar.set_postfix({\"Loss\": f\"{loss.item():.2e}\"})\n",
    "            # mini_batch_pbar.set_postfix({\"Loss\": f\"{loss.item():.2e}\"})\n",
    "            \n",
    "        avg_loss = batch_loss.item() / len(dataloader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "\n",
    "        # if (epoch + 1) % config[\"save_interval\"] == 0:\n",
    "        print(f\"Loss at epoch {epoch+1}: {avg_loss:.2e}\")\n",
    "        # Step the scheduler\n",
    "        if lr_scheduler is not None:\n",
    "            if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                lr_scheduler.step(avg_loss)\n",
    "            else:\n",
    "                lr_scheduler.step()\n",
    "        \n",
    "        # Print current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current learning rate: {current_lr:.2e}\")\n",
    "        # epoch_pbar.set_postfix({\"Loss\": f\"{avg_loss:.2e}\"})\n",
    "            # plot_and_save_loss()\n",
    "        # if (epoch + 1) % (config[\"save_interval\"] * 5) == 0:\n",
    "        #     save_checkpoint(epoch + 1)\n",
    "\n",
    "def train_step(batch):\n",
    "    global optimizer, device, prev_seq_length, model_type, model, noise_scheduler, lr_scheduler\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x = batch['motion_latent'].to(device)\n",
    "    a = batch['audio_latent'].to(device)\n",
    "\n",
    "    x_prev, x_gt = x[:, :prev_seq_length], x[:, prev_seq_length:]\n",
    "    x_input = torch.zeros_like(x_gt)  # Zero out the motion data for now\n",
    "    a_prev, a_train = a[:, :prev_seq_length], a[:, prev_seq_length:]\n",
    "\n",
    "    # Convert inputs to BF16\n",
    "    # x_gt = x_gt.bfloat16()\n",
    "    # x_prev = x_prev.bfloat16()\n",
    "    # a_train = a_train.bfloat16()\n",
    "    # a_prev = a_prev.bfloat16()\n",
    "    if model_type == \"dit\":\n",
    "        noise = torch.randn_like(x_gt)\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (x_gt.shape[0],), device=device).long()\n",
    "        x_noisy = noise_scheduler.add_noise(x_gt, noise, timesteps)\n",
    "        x_pred = model(x_noisy, x_prev, a_train, a_prev, timesteps)[:, prev_seq_length:]\n",
    "    elif model_type == \"vanilla\":\n",
    "        # with autocast(dtype=torch.bfloat16):\n",
    "        x_pred = model(x_input, x_prev, a_train, a_prev)\n",
    "        loss = torch.nn.functional.mse_loss(x_pred, x_gt)\n",
    "            \n",
    "        loss.backward()\n",
    "    \n",
    "        # if lr_scheduler:\n",
    "        #     lr_scheduler.step()\n",
    "        # else:\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def save_checkpoint(epoch):\n",
    "    global output_dir, model, optimizer\n",
    "\n",
    "    checkpoint_dir = os.path.join(output_dir, f\"checkpoint_epoch_{epoch}\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoint_dir, \"model.pth\"))\n",
    "    torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, \"optimizer.pth\"))\n",
    "    print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "def plot_and_save_loss():\n",
    "    global iteration_losses, epoch_losses, output_dir\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "    # Filter losses for each plot\n",
    "    full_range_losses = [(i, loss) for i, loss in enumerate(iteration_losses) if loss <= 5e-4]\n",
    "    zoomed_losses = [(i, loss) for i, loss in enumerate(iteration_losses) if loss <= 1e-6]\n",
    "\n",
    "    # Full range plot\n",
    "    if full_range_losses:\n",
    "        iterations, losses = zip(*full_range_losses)\n",
    "        ax1.plot(iterations, losses)\n",
    "        ax1.set_title(\"Iteration Loss (Full Range)\")\n",
    "        ax1.set_xlabel(\"Iteration\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.set_ylim(0, 5e-4)  # Set the y-axis limit to 1e-4\n",
    "\n",
    "    # Zoomed-in plot\n",
    "    if zoomed_losses:\n",
    "        iterations, losses = zip(*zoomed_losses)\n",
    "        ax2.plot(iterations, losses)\n",
    "        ax2.set_title(\"Iteration Loss (Zoomed)\")\n",
    "        ax2.set_xlabel(\"Iteration\")\n",
    "        ax2.set_ylabel(\"Loss\")\n",
    "        ax2.set_ylim(0, 5e-5)  # Set the y-axis limit to 1e-6\n",
    "        ax2.set_yscale('log')  # Use log scale for better visualization\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"loss_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Save losses to files\n",
    "    with open(os.path.join(output_dir, \"epoch_losses.txt\"), \"w\") as f:\n",
    "        for loss in epoch_losses:\n",
    "            f.write(f\"{loss}\\n\")\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"iteration_losses.txt\"), \"w\") as f:\n",
    "        for loss in iteration_losses:\n",
    "            f.write(f\"{loss}\\n\")\n",
    "    print(\"Loss plot and data saved\")\n",
    "\n",
    "def validate():\n",
    "    global model, checkpoint_dir, motion_latents, audio_latents, config, device, prev_seq_length, model_type, noise_scheduler\n",
    "\n",
    "    model.eval()\n",
    "    if checkpoint_dir:\n",
    "        load_checkpoint()\n",
    "    else:\n",
    "        raise ValueError(\"No checkpoint provided for validation\")\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    # Create a validation dataset and dataloader\n",
    "    valid_dataset = MotionAudioDataset(motion_latents, audio_latents)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=config[\"valid_batch_size\"], shuffle=False, pin_memory=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            x = batch['motion_latent'].to(device)\n",
    "            a = batch['audio_latent'].to(device)\n",
    "\n",
    "            x_prev, x_gt = x[:, :prev_seq_length], x[:, prev_seq_length:]\n",
    "            a_prev, a_train = a[:, :prev_seq_length], a[:, prev_seq_length:]\n",
    "\n",
    "            if model_type == \"dit\":\n",
    "                # For DiT model, we need to add noise and timesteps\n",
    "                noise = torch.randn_like(x_gt)\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (x_gt.shape[0],), device=device).long()\n",
    "                x_noisy = noise_scheduler.add_noise(x_gt, noise, timesteps)\n",
    "                x_pred = model(x_noisy, x_prev, a_train, a_prev, timesteps)[:, prev_seq_length:]\n",
    "            elif model_type == \"vanilla\":\n",
    "                x_input = torch.zeros_like(x_gt)  # Zero out the motion data for prediction\n",
    "                x_pred = model(x_input, x_prev, a_train, a_prev)\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(x_pred, x_gt)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            print(f\"Batch validation Loss: {loss.item():.10f}\")\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    print(f\"Avg validation Loss: {avg_loss:.10f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(scheduler_type=\"none\", learning_rate = 2e-4, epochs = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dataloader=dataloader, learning_rate = 1e-3, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dataloader=dataloader, learning_rate = 4e-4, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dataloader=dataloader, learning_rate = 3e-4, epochs =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dataloader=dataloader, learning_rate = 2e-4, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dataloader=dataloader, learning_rate = 1e-4, epochs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dataloader=dataloader, learning_rate = 5e-5, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dataloader=dataloader, learning_rate = 5e-5, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dataloader=dataloader, learning_rate = 5e-5, epochs = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vasa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
